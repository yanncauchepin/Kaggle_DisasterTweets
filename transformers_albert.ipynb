{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **HuggingFace Login**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac7960124b546e9b2b7e356a835b24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cauchepy\\Git\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sentencepiece\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", index_col=0)\n",
    "\n",
    "df_train = pd.read_csv(\"/media/yanncauchepin/ExternalDisk/Datasets/NaturalLanguageProcessing/disaster_tweets/train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"/media/yanncauchepin/ExternalDisk/Datasets/NaturalLanguageProcessing/disaster_tweets/test.csv\", index_col=0)\n",
    "'''\n",
    "\n",
    "df_train = pd.read_csv(\"C:/Users/cauchepy/Datasets/NaturalLanguageProcessing/kaggle_disastertweets/train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"C:/Users/cauchepy/Datasets/NaturalLanguageProcessing/kaggle_disastertweets/test.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Short Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length - train 7613 - test 3263\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length - train {len(df_train)} - test {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocess Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Merge columns (full)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fatalities</th>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deluge</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>armageddon</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damage</th>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body%20bags</th>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             train  test\n",
       "keyword                 \n",
       "fatalities      45     5\n",
       "deluge          42     8\n",
       "armageddon      42     8\n",
       "damage          41     9\n",
       "body%20bags     41     9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = pd.concat([\n",
    "    pd.DataFrame(df_train[\"keyword\"].value_counts()).rename(columns={\"count\":\"train\"}),\n",
    "    pd.DataFrame(df_test[\"keyword\"].value_counts()).rename(columns={\"count\":\"test\"})\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>104.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>71.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>London</th>\n",
       "      <td>45.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train  test\n",
       "location                  \n",
       "USA            104.0  37.0\n",
       "New York        71.0  38.0\n",
       "United States   50.0  15.0\n",
       "London          45.0  13.0\n",
       "Canada          29.0  13.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = pd.concat([\n",
    "    pd.DataFrame(df_train[\"location\"].value_counts()).rename(columns={\"count\":\"train\"}),\n",
    "    pd.DataFrame(df_test[\"location\"].value_counts()).rename(columns={\"count\":\"test\"})\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = df_train.copy()\n",
    "df_train_full['text'] = df_train_full.apply(lambda row: f\"{row['location'] or ''} {row['keyword'] or ''} {row['text']}\".strip(), axis=1)\n",
    "df_test_full = df_test.copy()\n",
    "df_test_full['text'] = df_test_full.apply(lambda row: f\"{row['location'] or ''} {row['keyword'] or ''} {row['text']}\".strip(), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Value': [f1, precision, recall]\n",
    "    }, index=['F1 Score', 'Precision', 'Recall'])\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
    "    \n",
    "    return metrics_df, cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALBERT Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_tokenizer_full = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "albert_encodings_full = albert_tokenizer_full(list(df_train_full['text']), truncation=True, padding=True, max_length=256)\n",
    "albert_labels_full = torch.tensor(list(df_train_full['target']))\n",
    "\n",
    "albert_input_ids_train_full, albert_input_ids_valid_full, albert_token_type_ids_train_full, albert_token_type_ids_valid_full, \\\n",
    "albert_attention_mask_train_full, albert_attention_mask_valid_full, albert_y_train_full, albert_y_valid_full = train_test_split(\n",
    "    albert_encodings_full['input_ids'], \n",
    "    albert_encodings_full['token_type_ids'], \n",
    "    albert_encodings_full['attention_mask'], \n",
    "    albert_labels_full, \n",
    "    test_size=0.15, \n",
    "    stratify=albert_labels_full, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "albert_train_encodings_full = {\n",
    "    'input_ids': torch.tensor(albert_input_ids_train_full),\n",
    "    'token_type_ids': torch.tensor(albert_token_type_ids_train_full),\n",
    "    'attention_mask': torch.tensor(albert_attention_mask_train_full)\n",
    "}\n",
    "\n",
    "albert_valid_encodings_full = {\n",
    "    'input_ids': torch.tensor(albert_input_ids_valid_full),\n",
    "    'token_type_ids': torch.tensor(albert_token_type_ids_valid_full),\n",
    "    'attention_mask': torch.tensor(albert_attention_mask_valid_full)\n",
    "}\n",
    "\n",
    "albert_train_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": albert_train_encodings_full['input_ids'],\n",
    "    \"attention_mask\": albert_train_encodings_full['attention_mask'],\n",
    "    \"labels\": albert_y_train_full\n",
    "})\n",
    "\n",
    "albert_valid_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": albert_valid_encodings_full['input_ids'],\n",
    "    \"attention_mask\": albert_valid_encodings_full['attention_mask'],\n",
    "    \"labels\": albert_y_valid_full\n",
    "})\n",
    "\n",
    "albert_test_encodings_full = albert_tokenizer_full(list(df_test_full['text']), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "albert_test_encodings_full = {\n",
    "    key: torch.tensor(val) for key, val in albert_test_encodings_full.items()\n",
    "}\n",
    "\n",
    "albert_test_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": albert_test_encodings_full['input_ids'],\n",
    "    \"attention_mask\": albert_test_encodings_full['attention_mask']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Save HuggingFace ALBERT Tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219297dc08b14121aa31ebbd3fb6d472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cauchepy\\Git\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cauchepy\\.cache\\huggingface\\hub\\models--yanncauchepin--kaggle_disastertweets_albert_tokenizer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yanncauchepin/kaggle_disastertweets_albert_tokenizer/commit/7a7baa1173f3e18d03a3d765cd13fd98e9ee46d0', commit_message='Upload tokenizer', commit_description='', oid='7a7baa1173f3e18d03a3d765cd13fd98e9ee46d0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yanncauchepin/kaggle_disastertweets_albert_tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='yanncauchepin/kaggle_disastertweets_albert_tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_tokenizer_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_albert_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALBERT Training from scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _ALBERT Transformers + local save_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098aa9294bba4099819464ee3c723dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.793, 'grad_norm': 67.93590545654297, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 0.712, 'grad_norm': 44.848941802978516, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 0.6488, 'grad_norm': 60.00485610961914, 'learning_rate': 3e-06, 'epoch': 0.02}\n",
      "{'loss': 0.7002, 'grad_norm': 49.69922637939453, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6291, 'grad_norm': 20.778968811035156, 'learning_rate': 5e-06, 'epoch': 0.03}\n",
      "{'loss': 0.5648, 'grad_norm': 22.72116470336914, 'learning_rate': 6e-06, 'epoch': 0.04}\n",
      "{'loss': 0.6075, 'grad_norm': 113.43247985839844, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 0.6722, 'grad_norm': 49.39283752441406, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 0.6164, 'grad_norm': 44.678653717041016, 'learning_rate': 9e-06, 'epoch': 0.06}\n",
      "{'loss': 0.6168, 'grad_norm': 41.77793502807617, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 0.6478, 'grad_norm': 47.63259506225586, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6224, 'grad_norm': 34.36805725097656, 'learning_rate': 1.2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.626, 'grad_norm': 34.595848083496094, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.5081, 'grad_norm': 46.411399841308594, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5726, 'grad_norm': 50.076820373535156, 'learning_rate': 1.5e-05, 'epoch': 0.09}\n",
      "{'loss': 0.3411, 'grad_norm': 10.098567008972168, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.1}\n",
      "{'loss': 0.693, 'grad_norm': 38.46382141113281, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 0.6945, 'grad_norm': 77.7969741821289, 'learning_rate': 1.8e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5082, 'grad_norm': 15.37747573852539, 'learning_rate': 1.9e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5854, 'grad_norm': 5.576014995574951, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5707, 'grad_norm': 20.850162506103516, 'learning_rate': 2.1e-05, 'epoch': 0.13}\n",
      "{'loss': 0.7183, 'grad_norm': 1339.4932861328125, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.7771, 'grad_norm': 68.25312805175781, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.537, 'grad_norm': 23.621623992919922, 'learning_rate': 2.4e-05, 'epoch': 0.15}\n",
      "{'loss': 0.6073, 'grad_norm': 39.25248336791992, 'learning_rate': 2.5e-05, 'epoch': 0.15}\n",
      "{'loss': 0.7085, 'grad_norm': 181.27670288085938, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 0.5699, 'grad_norm': 17.882858276367188, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5632, 'grad_norm': 52.23277282714844, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5848, 'grad_norm': 35.740089416503906, 'learning_rate': 2.9e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4162, 'grad_norm': 11.17082691192627, 'learning_rate': 3e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7623, 'grad_norm': 116.46077728271484, 'learning_rate': 3.1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.531, 'grad_norm': 22.408828735351562, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5352, 'grad_norm': 10.571855545043945, 'learning_rate': 3.3e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5317, 'grad_norm': 6.41126012802124, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.21}\n",
      "{'loss': 0.693, 'grad_norm': 26.367006301879883, 'learning_rate': 3.5e-05, 'epoch': 0.22}\n",
      "{'loss': 1.0277, 'grad_norm': 20.0787410736084, 'learning_rate': 3.6e-05, 'epoch': 0.22}\n",
      "{'loss': 0.5972, 'grad_norm': 10.88129997253418, 'learning_rate': 3.7e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4904, 'grad_norm': 19.8061466217041, 'learning_rate': 3.8e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5674, 'grad_norm': 14.385337829589844, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.24}\n",
      "{'loss': 0.7776, 'grad_norm': 71.93865966796875, 'learning_rate': 4e-05, 'epoch': 0.25}\n",
      "{'loss': 0.496, 'grad_norm': 99.6511459350586, 'learning_rate': 4.1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9199, 'grad_norm': 20.43712615966797, 'learning_rate': 4.2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6941, 'grad_norm': 10.295512199401855, 'learning_rate': 4.3e-05, 'epoch': 0.27}\n",
      "{'loss': 0.6923, 'grad_norm': 2.0367038249969482, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.27}\n",
      "{'loss': 0.674, 'grad_norm': 9.827640533447266, 'learning_rate': 4.5e-05, 'epoch': 0.28}\n",
      "{'loss': 0.7416, 'grad_norm': 10.654196739196777, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.6885, 'grad_norm': 11.084253311157227, 'learning_rate': 4.7e-05, 'epoch': 0.29}\n",
      "{'loss': 0.6648, 'grad_norm': 3.7381680011749268, 'learning_rate': 4.8e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6903, 'grad_norm': 3.920239210128784, 'learning_rate': 4.9e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6928, 'grad_norm': 3.2307677268981934, 'learning_rate': 5e-05, 'epoch': 0.31}\n",
      "{'loss': 0.6332, 'grad_norm': 74.78749084472656, 'learning_rate': 4.993412384716733e-05, 'epoch': 0.32}\n",
      "{'loss': 0.622, 'grad_norm': 64.55307006835938, 'learning_rate': 4.986824769433465e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7473, 'grad_norm': 29.324214935302734, 'learning_rate': 4.980237154150198e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6548, 'grad_norm': 4.298563003540039, 'learning_rate': 4.97364953886693e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6565, 'grad_norm': 19.24983024597168, 'learning_rate': 4.967061923583663e-05, 'epoch': 0.34}\n",
      "{'loss': 0.5817, 'grad_norm': 9.634073257446289, 'learning_rate': 4.960474308300396e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5528, 'grad_norm': 5.8056182861328125, 'learning_rate': 4.953886693017128e-05, 'epoch': 0.35}\n",
      "{'loss': 0.7634, 'grad_norm': 30.39498519897461, 'learning_rate': 4.947299077733861e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7344, 'grad_norm': 8.793373107910156, 'learning_rate': 4.940711462450593e-05, 'epoch': 0.36}\n",
      "{'loss': 0.463, 'grad_norm': 52.276371002197266, 'learning_rate': 4.9341238471673256e-05, 'epoch': 0.37}\n",
      "{'loss': 0.6805, 'grad_norm': 4.240113258361816, 'learning_rate': 4.9275362318840584e-05, 'epoch': 0.38}\n",
      "{'loss': 0.961, 'grad_norm': 14.00168514251709, 'learning_rate': 4.9209486166007906e-05, 'epoch': 0.38}\n",
      "{'loss': 0.8329, 'grad_norm': 11.908285140991211, 'learning_rate': 4.9143610013175234e-05, 'epoch': 0.39}\n",
      "{'loss': 0.4279, 'grad_norm': 9.450584411621094, 'learning_rate': 4.9077733860342555e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7125, 'grad_norm': 34.5743293762207, 'learning_rate': 4.901185770750988e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7383, 'grad_norm': 30.884580612182617, 'learning_rate': 4.894598155467721e-05, 'epoch': 0.41}\n",
      "{'loss': 0.6765, 'grad_norm': 75.56670379638672, 'learning_rate': 4.888010540184453e-05, 'epoch': 0.41}\n",
      "{'loss': 0.7626, 'grad_norm': 199.6830291748047, 'learning_rate': 4.881422924901186e-05, 'epoch': 0.42}\n",
      "{'loss': 0.573, 'grad_norm': 48.31730270385742, 'learning_rate': 4.874835309617918e-05, 'epoch': 0.43}\n",
      "{'loss': 0.8082, 'grad_norm': 10.19729995727539, 'learning_rate': 4.868247694334651e-05, 'epoch': 0.43}\n",
      "{'loss': 0.6532, 'grad_norm': 9.690157890319824, 'learning_rate': 4.861660079051384e-05, 'epoch': 0.44}\n",
      "{'loss': 0.7381, 'grad_norm': 36.13299560546875, 'learning_rate': 4.855072463768116e-05, 'epoch': 0.44}\n",
      "{'loss': 0.7338, 'grad_norm': 9.522517204284668, 'learning_rate': 4.848484848484849e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6516, 'grad_norm': 11.19526195526123, 'learning_rate': 4.841897233201581e-05, 'epoch': 0.46}\n",
      "{'loss': 0.8354, 'grad_norm': 9.291580200195312, 'learning_rate': 4.835309617918314e-05, 'epoch': 0.46}\n",
      "{'loss': 0.7232, 'grad_norm': 15.088031768798828, 'learning_rate': 4.8287220026350465e-05, 'epoch': 0.47}\n",
      "{'loss': 0.6693, 'grad_norm': 20.519271850585938, 'learning_rate': 4.822134387351779e-05, 'epoch': 0.48}\n",
      "{'loss': 0.7394, 'grad_norm': 84.62044525146484, 'learning_rate': 4.8155467720685115e-05, 'epoch': 0.48}\n",
      "{'loss': 0.7436, 'grad_norm': 11.238081932067871, 'learning_rate': 4.8089591567852436e-05, 'epoch': 0.49}\n",
      "{'loss': 0.8187, 'grad_norm': 5.341056823730469, 'learning_rate': 4.8023715415019764e-05, 'epoch': 0.49}\n",
      "{'loss': 0.6665, 'grad_norm': 5.299999237060547, 'learning_rate': 4.795783926218709e-05, 'epoch': 0.5}\n",
      "{'loss': 0.7279, 'grad_norm': 4.259945869445801, 'learning_rate': 4.7891963109354414e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7245, 'grad_norm': 9.838202476501465, 'learning_rate': 4.782608695652174e-05, 'epoch': 0.51}\n",
      "{'loss': 0.706, 'grad_norm': 3.0840399265289307, 'learning_rate': 4.776021080368906e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7169, 'grad_norm': 6.144277572631836, 'learning_rate': 4.769433465085639e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6839, 'grad_norm': 13.032744407653809, 'learning_rate': 4.762845849802372e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8626, 'grad_norm': 24.541181564331055, 'learning_rate': 4.756258234519104e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7176, 'grad_norm': 18.39969253540039, 'learning_rate': 4.749670619235837e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6757, 'grad_norm': 11.67076587677002, 'learning_rate': 4.743083003952569e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7325, 'grad_norm': 3.6900720596313477, 'learning_rate': 4.736495388669302e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6812, 'grad_norm': 6.220282077789307, 'learning_rate': 4.7299077733860346e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6679, 'grad_norm': 9.625678062438965, 'learning_rate': 4.723320158102767e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6698, 'grad_norm': 3.753535509109497, 'learning_rate': 4.7167325428194996e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6969, 'grad_norm': 5.977163314819336, 'learning_rate': 4.710144927536232e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6969, 'grad_norm': 11.443184852600098, 'learning_rate': 4.7035573122529645e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6637, 'grad_norm': 11.385266304016113, 'learning_rate': 4.696969696969697e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7282, 'grad_norm': 26.539962768554688, 'learning_rate': 4.6903820816864295e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6743, 'grad_norm': 14.435344696044922, 'learning_rate': 4.683794466403162e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7018, 'grad_norm': 8.405808448791504, 'learning_rate': 4.6772068511198944e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7415, 'grad_norm': 1251.6082763671875, 'learning_rate': 4.670619235836627e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7147, 'grad_norm': 5.465060710906982, 'learning_rate': 4.66403162055336e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7602, 'grad_norm': 4.442361354827881, 'learning_rate': 4.657444005270092e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6899, 'grad_norm': 5.462803363800049, 'learning_rate': 4.650856389986825e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6905, 'grad_norm': 5.844081401824951, 'learning_rate': 4.644268774703557e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6769, 'grad_norm': 3.56345534324646, 'learning_rate': 4.63768115942029e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6407, 'grad_norm': 7.482110500335693, 'learning_rate': 4.631093544137022e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6933, 'grad_norm': 3.2425954341888428, 'learning_rate': 4.624505928853755e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6637, 'grad_norm': 3.458113193511963, 'learning_rate': 4.6179183135704877e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6977, 'grad_norm': 3.3564627170562744, 'learning_rate': 4.61133069828722e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7101, 'grad_norm': 5.783207893371582, 'learning_rate': 4.6047430830039526e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6888, 'grad_norm': 6.502656936645508, 'learning_rate': 4.598155467720685e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7098, 'grad_norm': 4.597142219543457, 'learning_rate': 4.5915678524374175e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6809, 'grad_norm': 3.712813138961792, 'learning_rate': 4.5849802371541504e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7205, 'grad_norm': 7.498641490936279, 'learning_rate': 4.5783926218708825e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6647, 'grad_norm': 3.6013717651367188, 'learning_rate': 4.571805006587615e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7598, 'grad_norm': 2.372309923171997, 'learning_rate': 4.565217391304348e-05, 'epoch': 0.72}\n",
      "{'loss': 0.714, 'grad_norm': 6.655721664428711, 'learning_rate': 4.55862977602108e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6822, 'grad_norm': 5.368288993835449, 'learning_rate': 4.552042160737813e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6637, 'grad_norm': 3.7919342517852783, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6984, 'grad_norm': 8.225584030151367, 'learning_rate': 4.538866930171278e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7291, 'grad_norm': 2.498122215270996, 'learning_rate': 4.532279314888011e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6955, 'grad_norm': 10.839498519897461, 'learning_rate': 4.525691699604743e-05, 'epoch': 0.75}\n",
      "{'loss': 0.71, 'grad_norm': 4.529693603515625, 'learning_rate': 4.519104084321476e-05, 'epoch': 0.76}\n",
      "{'loss': 0.679, 'grad_norm': 4.797577381134033, 'learning_rate': 4.5125164690382086e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6776, 'grad_norm': 2.3888511657714844, 'learning_rate': 4.505928853754941e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6911, 'grad_norm': 3.7916572093963623, 'learning_rate': 4.4993412384716735e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6886, 'grad_norm': 3.5471487045288086, 'learning_rate': 4.492753623188406e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6868, 'grad_norm': 7.788699150085449, 'learning_rate': 4.4861660079051384e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6711, 'grad_norm': 10.161330223083496, 'learning_rate': 4.479578392621871e-05, 'epoch': 0.8}\n",
      "{'loss': 0.709, 'grad_norm': 1.9331650733947754, 'learning_rate': 4.472990777338604e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7063, 'grad_norm': 5.547201633453369, 'learning_rate': 4.466403162055336e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6615, 'grad_norm': 2.3897714614868164, 'learning_rate': 4.459815546772069e-05, 'epoch': 0.82}\n",
      "{'loss': 0.729, 'grad_norm': 9.888081550598145, 'learning_rate': 4.453227931488801e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8315, 'grad_norm': 2.20757794380188, 'learning_rate': 4.446640316205534e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6984, 'grad_norm': 2.979118824005127, 'learning_rate': 4.440052700922267e-05, 'epoch': 0.83}\n",
      "{'loss': 0.692, 'grad_norm': 1.9483239650726318, 'learning_rate': 4.433465085638999e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6971, 'grad_norm': 4.729875087738037, 'learning_rate': 4.426877470355732e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6679, 'grad_norm': 2.086029529571533, 'learning_rate': 4.4202898550724645e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6748, 'grad_norm': 3.075988292694092, 'learning_rate': 4.4137022397891966e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6572, 'grad_norm': 2.8079986572265625, 'learning_rate': 4.4071146245059295e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6543, 'grad_norm': 2.8517262935638428, 'learning_rate': 4.4005270092226616e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7051, 'grad_norm': 8.09957218170166, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6596, 'grad_norm': 2.8873302936553955, 'learning_rate': 4.387351778656127e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7, 'grad_norm': 8.007946968078613, 'learning_rate': 4.3807641633728593e-05, 'epoch': 0.89}\n",
      "{'loss': 0.7087, 'grad_norm': 3.5426182746887207, 'learning_rate': 4.374176548089592e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6747, 'grad_norm': 3.162026882171631, 'learning_rate': 4.367588932806324e-05, 'epoch': 0.9}\n",
      "{'loss': 0.698, 'grad_norm': 7.346530437469482, 'learning_rate': 4.361001317523057e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7063, 'grad_norm': 2.1489672660827637, 'learning_rate': 4.35441370223979e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6608, 'grad_norm': 4.279540538787842, 'learning_rate': 4.347826086956522e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6594, 'grad_norm': 3.0388357639312744, 'learning_rate': 4.341238471673255e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7165, 'grad_norm': 2.990147590637207, 'learning_rate': 4.334650856389987e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6959, 'grad_norm': 3.2146430015563965, 'learning_rate': 4.32806324110672e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6641, 'grad_norm': 2.336421251296997, 'learning_rate': 4.3214756258234526e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7183, 'grad_norm': 2.0146563053131104, 'learning_rate': 4.314888010540185e-05, 'epoch': 0.95}\n",
      "{'loss': 0.6559, 'grad_norm': 1.9251761436462402, 'learning_rate': 4.3083003952569175e-05, 'epoch': 0.96}\n",
      "{'loss': 0.7064, 'grad_norm': 2.0081381797790527, 'learning_rate': 4.30171277997365e-05, 'epoch': 0.96}\n",
      "{'loss': 0.7, 'grad_norm': 1.59337317943573, 'learning_rate': 4.2951251646903825e-05, 'epoch': 0.97}\n",
      "{'loss': 0.698, 'grad_norm': 5.13562536239624, 'learning_rate': 4.288537549407115e-05, 'epoch': 0.98}\n",
      "{'loss': 0.6824, 'grad_norm': 1.7399024963378906, 'learning_rate': 4.2819499341238474e-05, 'epoch': 0.98}\n",
      "{'loss': 0.6857, 'grad_norm': 8.586432456970215, 'learning_rate': 4.27536231884058e-05, 'epoch': 0.99}\n",
      "{'loss': 0.6747, 'grad_norm': 5.430546283721924, 'learning_rate': 4.2687747035573124e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb51b8e20a949c7a61525307469d42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6843461990356445, 'eval_runtime': 6.8235, 'eval_samples_per_second': 167.362, 'eval_steps_per_second': 41.914, 'epoch': 1.0}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The `metric_for_best_model` training argument is set to 'eval_f1', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Consider changing the `metric_for_best_model` via the TrainingArguments.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cauchepy\\Git\\.venv\\lib\\site-packages\\transformers\\trainer.py:3151\u001b[0m, in \u001b[0;36mTrainer._determine_best_metric\u001b[1;34m(self, metrics, trial)\u001b[0m\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3151\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eval_f1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 27\u001b[0m\n\u001b[0;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      4\u001b[0m     hub_model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaggle_disastertweets_albert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./albert_results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m albert_trainer_full \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39malbert_model_full,\n\u001b[0;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     23\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39malbert_train_dataset_full,\n\u001b[0;32m     24\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39malbert_valid_dataset_full\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m \u001b[43malbert_trainer_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m albert_trainer_full\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Local\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cauchepy\\Git\\.venv\\lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cauchepy\\Git\\.venv\\lib\\site-packages\\transformers\\trainer.py:2618\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2618\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2622\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cauchepy\\Git\\.venv\\lib\\site-packages\\transformers\\trainer.py:3050\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[0;32m   3048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m   3049\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m-> 3050\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_determine_best_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n\u001b[0;32m   3053\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;241m=\u001b[39m is_new_best_metric\n",
      "File \u001b[1;32mc:\\Users\\cauchepy\\Git\\.venv\\lib\\site-packages\\transformers\\trainer.py:3153\u001b[0m, in \u001b[0;36mTrainer._determine_best_metric\u001b[1;34m(self, metrics, trial)\u001b[0m\n\u001b[0;32m   3151\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m metrics[metric_to_check]\n\u001b[0;32m   3152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m-> 3153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m   3154\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `metric_for_best_model` training argument is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, which is not found in the evaluation metrics. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe available evaluation metrics are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider changing the `metric_for_best_model` via the TrainingArguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3156\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m   3158\u001b[0m operator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgreater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgreater_is_better \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mless\n\u001b[0;32m   3160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The `metric_for_best_model` training argument is set to 'eval_f1', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Consider changing the `metric_for_best_model` via the TrainingArguments.\""
     ]
    }
   ],
   "source": [
    "albert_model_full = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    hub_model_id=\"kaggle_disastertweets_albert_model\",\n",
    "    output_dir=\"./albert_results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./albert_logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "albert_trainer_full = Trainer(\n",
    "    model=albert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=albert_train_dataset_full,\n",
    "    eval_dataset=albert_valid_dataset_full\n",
    ")\n",
    "\n",
    "albert_trainer_full.train()\n",
    "\n",
    "albert_trainer_full.save_model(\"albert_model\") # Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Save HugggingFace trained ALBERT Transformers_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1166dfc806ae463288c682ae7f2816ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e1eee3051a40d8a5fb9ca038ac0433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bea8a607d0948bbaf9c31e8e075a7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yanncauchepin/kaggle_disastertweets_albert_model/commit/3a2b0e060519d1a32fc5bb3a50b9be67fd20c73c', commit_message='End of training', commit_description='', oid='3a2b0e060519d1a32fc5bb3a50b9be67fd20c73c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yanncauchepin/kaggle_disastertweets_albert_model', endpoint='https://huggingface.co', repo_type='model', repo_id='yanncauchepin/kaggle_disastertweets_albert_model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_trainer_full.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALBERT Loading from pretrained**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_model_full = AlbertForSequenceClassification.from_pretrained(\"albert_model\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    hub_model_id=\"kaggle_disastertweets_albert_model\",\n",
    "    output_dir=\"./albert_results\",\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./albert_logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "albert_trainer_full = Trainer(\n",
    "    model=albert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=albert_train_dataset_full,\n",
    "    eval_dataset=albert_valid_dataset_full\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALBERT Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211c1410f90e46bfb5f0e13817a013a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.766772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.774081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "F1 Score   0.766772\n",
       "Precision  0.783000\n",
       "Recall     0.774081"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>589</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>196</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                 589                  62\n",
       "Actual Positive                 196                 295"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "albert_predictions_full = albert_trainer_full.predict(albert_valid_dataset_full)\n",
    "albert_logits_full = albert_predictions_full.predictions\n",
    "albert_y_pred_full = np.argmax(albert_logits_full, axis=1)\n",
    "\n",
    "albert_trainer_full_assessement = evaluate_classifier(albert_y_valid_full.numpy(), albert_y_pred_full)\n",
    "display(albert_trainer_full_assessement[0])\n",
    "display(albert_trainer_full_assessement[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALBERT Test Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _ALBERT Test Predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_test_predictions_full = albert_trainer_full.predict(albert_test_dataset_full)\n",
    "albert_test_logits_full = albert_test_predictions_full.predictions\n",
    "albert_test_y_pred_full = np.argmax(albert_test_logits_full, axis=1)\n",
    "\n",
    "albert_test_submission_full = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': albert_test_y_pred_full.flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Save HuggingFace ALBERT Test Predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_albert_test_submission_full = Dataset.from_pandas(albert_test_submission_full)\n",
    "hf_albert_test_submission_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_albert_submission_df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
