{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b4ec5a50594182a38089372881fb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", index_col=0)\\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", index_col=0)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", index_col=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/media/yanncauchepin/ExternalDisk/Datasets/NaturalLanguageProcessing/disaster_tweets/train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"/media/yanncauchepin/ExternalDisk/Datasets/NaturalLanguageProcessing/disaster_tweets/test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length - train 7613 - test 3263\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length - train {len(df_train)} - test {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fatalities</th>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deluge</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>armageddon</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sinking</th>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damage</th>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train  test\n",
       "keyword                \n",
       "fatalities     45     5\n",
       "deluge         42     8\n",
       "armageddon     42     8\n",
       "sinking        41     9\n",
       "damage         41     9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = pd.concat([\n",
    "    pd.DataFrame(df_train[\"keyword\"].value_counts()).rename(columns={\"count\":\"train\"}),\n",
    "    pd.DataFrame(df_test[\"keyword\"].value_counts()).rename(columns={\"count\":\"test\"})\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>104.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>71.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>London</th>\n",
       "      <td>45.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train  test\n",
       "location                  \n",
       "USA            104.0  37.0\n",
       "New York        71.0  38.0\n",
       "United States   50.0  15.0\n",
       "London          45.0  13.0\n",
       "Canada          29.0  13.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = pd.concat([\n",
    "    pd.DataFrame(df_train[\"location\"].value_counts()).rename(columns={\"count\":\"train\"}),\n",
    "    pd.DataFrame(df_test[\"location\"].value_counts()).rename(columns={\"count\":\"test\"})\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = df_train.copy()\n",
    "df_train_full['text'] = df_train_full.apply(lambda row: f\"{row['location'] or ''} {row['keyword'] or ''} {row['text']}\".strip(), axis=1)\n",
    "df_test_full = df_test.copy()\n",
    "df_test_full['text'] = df_test_full.apply(lambda row: f\"{row['location'] or ''} {row['keyword'] or ''} {row['text']}\".strip(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_texts_vec = vectorizer.fit_transform(df_train['text'])\n",
    "train_texts_vec.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_texts_tfidf = tfidf.fit_transform(train_texts_vec)\n",
    "train_texts_tfidf = train_texts_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_vec = vectorizer.transform(df_test['text'])\n",
    "test_texts_vec.todense()\n",
    "test_texts_tfdif = tfidf.transform(test_texts_vec)\n",
    "test_texts_tfdif = test_texts_tfdif.todense()\n",
    "X_test = np.asarray(test_texts_tfdif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_texts_tfidf, df_train['target'], test_size = 0.2, stratify=df_train['target'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_valid = np.asarray(X_valid)\n",
    "y_valid = np.asarray(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_full = CountVectorizer(stop_words='english')\n",
    "train_full_texts_vec = vectorizer_full.fit_transform(df_train_full['text'])\n",
    "train_full_texts_vec.todense()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_full = TfidfTransformer()\n",
    "train_full_texts_tfidf = tfidf_full.fit_transform(train_full_texts_vec)\n",
    "train_full_texts_tfidf = train_full_texts_tfidf.todense()\n",
    "\n",
    "X_full_train = np.asarray(train_full_texts_tfidf)\n",
    "y_full_train = np.asarray(df_train_full['target'])\n",
    "\n",
    "test_full_texts_vec = vectorizer_full.transform(df_test_full['text'])\n",
    "test_full_texts_vec.todense()\n",
    "test_full_texts_tfdif = tfidf_full.transform(test_full_texts_vec)\n",
    "test_full_texts_tfdif = test_full_texts_tfdif.todense()\n",
    "X_full_test = np.asarray(test_full_texts_tfdif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def evaluate_classifier(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "        'Value': [f1, precision, recall]\n",
    "    })\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
    "    \n",
    "    return metrics_df, cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.796308\n",
      "1  Precision  0.807306\n",
      "2     Recall  0.801051,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 790                  79\n",
      "Actual Positive                 224                 430)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_valid)\n",
    "\n",
    "naive_bayes_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(naive_bayes_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier_full = MultinomialNB()\n",
    "naive_bayes_classifier_full.fit(X_full_train, y_full_train)\n",
    "y_full_pred = naive_bayes_classifier_full.predict(X_full_test)\n",
    "\n",
    "naive_bayes_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.svm import SVC\\nsvc_classifier = SVC(kernel='linear')\\nsvc_classifier.fit(X_train, y_train)\\ny_pred = svc_classifier.predict(X_valid)\\n\\nsvc_classifier_assessement = evaluate_classifier(y_valid, y_pred)\\nprint(svc_classifier_assessement)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.svm import SVC\n",
    "svc_classifier = SVC(kernel='linear')\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "y_pred = svc_classifier.predict(X_valid)\n",
    "\n",
    "svc_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(svc_classifier_assessement)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.794483\n",
      "1  Precision  0.810064\n",
      "2     Recall  0.800394,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 801                  68\n",
      "Actual Positive                 236                 418)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression_classifier = LogisticRegression()\n",
    "logistic_regression_classifier.fit(X_train, y_train)\n",
    "y_pred = logistic_regression_classifier.predict(X_valid)\n",
    "\n",
    "logistic_regression_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(logistic_regression_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier_full = LogisticRegression()\n",
    "logistic_regression_classifier_full.fit(X_full_train, y_full_train)\n",
    "y_full_pred = logistic_regression_classifier_full.predict(X_full_test)\n",
    "\n",
    "logistic_regression_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.775101\n",
      "1  Precision  0.802528\n",
      "2     Recall  0.784636,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 814                  55\n",
      "Actual Positive                 273                 381)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "y_pred = random_forest_classifier.predict(X_valid)\n",
    "\n",
    "random_forest_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(random_forest_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier_full = RandomForestClassifier()\n",
    "random_forest_classifier_full.fit(X_full_train, y_full_train)\n",
    "y_full_pred = random_forest_classifier_full.predict(X_full_test)\n",
    "\n",
    "random_forest_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-10-31 20:49:37.365278: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2024-10-31 20:49:37.365795: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:135] retrieving CUDA diagnostic information for host: yanncauchepincomputing\n",
      "2024-10-31 20:49:37.365813: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:142] hostname: yanncauchepincomputing\n",
      "2024-10-31 20:49:37.366105: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:166] libcuda reported version is: 550.120.0\n",
      "2024-10-31 20:49:37.366144: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] kernel reported version is: 550.107.2\n",
      "2024-10-31 20:49:37.366157: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:252] kernel version 550.107.2 does not match DSO version 550.120.0 -- cannot find working devices in this configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 20:49:40.182417: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 520402680 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m762/762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.6765 - loss: 0.5968 - val_accuracy: 0.7984 - val_loss: 0.4524\n",
      "Epoch 2/10\n",
      "\u001b[1m762/762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8905 - loss: 0.2714 - val_accuracy: 0.7682 - val_loss: 0.5765\n",
      "Epoch 3/10\n",
      "\u001b[1m762/762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9630 - loss: 0.0922 - val_accuracy: 0.7630 - val_loss: 0.7429\n",
      "Epoch 4/10\n",
      "\u001b[1m762/762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9837 - loss: 0.0401 - val_accuracy: 0.7525 - val_loss: 0.8938\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "(      Metric     Value\n",
      "0   F1 Score  0.792614\n",
      "1  Precision  0.807421\n",
      "2     Recall  0.798424,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 798                  71\n",
      "Actual Positive                 236                 418)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "neural_network_classifier = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "neural_network_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = neural_network_classifier.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "y_pred = neural_network_classifier.predict(X_valid)\n",
    "\n",
    "neural_network_classifier_assessment = evaluate_classifier(y_valid, np.round(y_pred))\n",
    "print(neural_network_classifier_assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.6790 - loss: 0.5842 - val_accuracy: 0.8082 - val_loss: 0.4394\n",
      "Epoch 2/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8946 - loss: 0.2580 - val_accuracy: 0.7723 - val_loss: 0.5667\n",
      "Epoch 3/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9689 - loss: 0.0890 - val_accuracy: 0.7627 - val_loss: 0.7224\n",
      "Epoch 4/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9848 - loss: 0.0359 - val_accuracy: 0.7434 - val_loss: 0.9126\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "X_full_train_, X_full_valid, y_full_train_, y_full_valid = train_test_split(X_full_train, y_full_train, test_size = 0.15, stratify=y_full_train, random_state = 0)\n",
    "\n",
    "neural_network_classifier_full = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_full_train_.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "neural_network_classifier_full.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping_full = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history_full = neural_network_classifier_full.fit(\n",
    "    X_full_train_, y_full_train_,\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_full_valid, y_full_valid),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "y_full_pred = neural_network_classifier_full.predict(X_full_test)\n",
    "\n",
    "neural_network_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': np.round(y_full_pred).flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [20:58:09] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.785225\n",
      "1  Precision  0.794514\n",
      "2     Recall  0.789888,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 778                  91\n",
      "Actual Positive                 229                 425)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier(n_estimators=1000, max_depth=4, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_classifier.fit(X_train, y_train, verbose=1)\n",
    "\n",
    "y_pred = xgb_classifier.predict(X_valid)\n",
    "\n",
    "xgb_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(xgb_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:00:38] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier_full = XGBClassifier(n_estimators=1000, max_depth=4, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_classifier_full.fit(X_full_train, y_full_train, verbose=1)\n",
    "\n",
    "y_full_pred = xgb_classifier_full.predict(X_full_test)\n",
    "\n",
    "xgb_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6797045\ttotal: 74.8ms\tremaining: 1m 14s\n",
      "2:\tlearn: 0.6640753\ttotal: 127ms\tremaining: 42.2s\n",
      "4:\tlearn: 0.6527238\ttotal: 177ms\tremaining: 35.3s\n",
      "6:\tlearn: 0.6418404\ttotal: 225ms\tremaining: 31.9s\n",
      "8:\tlearn: 0.6339999\ttotal: 272ms\tremaining: 30s\n",
      "10:\tlearn: 0.6280628\ttotal: 324ms\tremaining: 29.1s\n",
      "12:\tlearn: 0.6209378\ttotal: 389ms\tremaining: 29.5s\n",
      "14:\tlearn: 0.6162400\ttotal: 437ms\tremaining: 28.7s\n",
      "16:\tlearn: 0.6115203\ttotal: 486ms\tremaining: 28.1s\n",
      "18:\tlearn: 0.6076645\ttotal: 538ms\tremaining: 27.8s\n",
      "20:\tlearn: 0.6030091\ttotal: 586ms\tremaining: 27.3s\n",
      "22:\tlearn: 0.5995534\ttotal: 636ms\tremaining: 27s\n",
      "24:\tlearn: 0.5964546\ttotal: 683ms\tremaining: 26.7s\n",
      "26:\tlearn: 0.5930054\ttotal: 732ms\tremaining: 26.4s\n",
      "28:\tlearn: 0.5902966\ttotal: 781ms\tremaining: 26.2s\n",
      "30:\tlearn: 0.5873470\ttotal: 828ms\tremaining: 25.9s\n",
      "32:\tlearn: 0.5849357\ttotal: 877ms\tremaining: 25.7s\n",
      "34:\tlearn: 0.5827788\ttotal: 926ms\tremaining: 25.5s\n",
      "36:\tlearn: 0.5798956\ttotal: 975ms\tremaining: 25.4s\n",
      "38:\tlearn: 0.5778346\ttotal: 1.02s\tremaining: 25.2s\n",
      "40:\tlearn: 0.5759892\ttotal: 1.07s\tremaining: 25.1s\n",
      "42:\tlearn: 0.5736391\ttotal: 1.12s\tremaining: 25s\n",
      "44:\tlearn: 0.5715019\ttotal: 1.17s\tremaining: 24.8s\n",
      "46:\tlearn: 0.5694379\ttotal: 1.22s\tremaining: 24.7s\n",
      "48:\tlearn: 0.5674322\ttotal: 1.26s\tremaining: 24.6s\n",
      "50:\tlearn: 0.5658421\ttotal: 1.31s\tremaining: 24.5s\n",
      "52:\tlearn: 0.5638582\ttotal: 1.36s\tremaining: 24.4s\n",
      "54:\tlearn: 0.5613693\ttotal: 1.41s\tremaining: 24.3s\n",
      "56:\tlearn: 0.5600778\ttotal: 1.46s\tremaining: 24.2s\n",
      "58:\tlearn: 0.5587724\ttotal: 1.51s\tremaining: 24.1s\n",
      "60:\tlearn: 0.5571517\ttotal: 1.56s\tremaining: 24s\n",
      "62:\tlearn: 0.5556529\ttotal: 1.6s\tremaining: 23.9s\n",
      "64:\tlearn: 0.5540510\ttotal: 1.65s\tremaining: 23.8s\n",
      "66:\tlearn: 0.5523827\ttotal: 1.7s\tremaining: 23.7s\n",
      "68:\tlearn: 0.5505863\ttotal: 1.75s\tremaining: 23.6s\n",
      "70:\tlearn: 0.5490264\ttotal: 1.8s\tremaining: 23.5s\n",
      "72:\tlearn: 0.5476734\ttotal: 1.84s\tremaining: 23.4s\n",
      "74:\tlearn: 0.5461642\ttotal: 1.89s\tremaining: 23.3s\n",
      "76:\tlearn: 0.5442920\ttotal: 1.94s\tremaining: 23.2s\n",
      "78:\tlearn: 0.5426018\ttotal: 1.98s\tremaining: 23.1s\n",
      "80:\tlearn: 0.5411093\ttotal: 2.03s\tremaining: 23s\n",
      "82:\tlearn: 0.5394318\ttotal: 2.08s\tremaining: 23s\n",
      "84:\tlearn: 0.5378561\ttotal: 2.13s\tremaining: 22.9s\n",
      "86:\tlearn: 0.5362890\ttotal: 2.17s\tremaining: 22.8s\n",
      "88:\tlearn: 0.5347047\ttotal: 2.22s\tremaining: 22.8s\n",
      "90:\tlearn: 0.5330016\ttotal: 2.27s\tremaining: 22.7s\n",
      "92:\tlearn: 0.5310676\ttotal: 2.32s\tremaining: 22.6s\n",
      "94:\tlearn: 0.5297311\ttotal: 2.37s\tremaining: 22.5s\n",
      "96:\tlearn: 0.5277773\ttotal: 2.42s\tremaining: 22.6s\n",
      "98:\tlearn: 0.5261703\ttotal: 2.47s\tremaining: 22.5s\n",
      "100:\tlearn: 0.5247077\ttotal: 2.52s\tremaining: 22.4s\n",
      "102:\tlearn: 0.5230015\ttotal: 2.57s\tremaining: 22.4s\n",
      "104:\tlearn: 0.5213563\ttotal: 2.62s\tremaining: 22.3s\n",
      "106:\tlearn: 0.5201779\ttotal: 2.67s\tremaining: 22.3s\n",
      "108:\tlearn: 0.5184250\ttotal: 2.72s\tremaining: 22.2s\n",
      "110:\tlearn: 0.5169502\ttotal: 2.77s\tremaining: 22.2s\n",
      "112:\tlearn: 0.5149358\ttotal: 2.81s\tremaining: 22.1s\n",
      "114:\tlearn: 0.5129840\ttotal: 2.86s\tremaining: 22s\n",
      "116:\tlearn: 0.5114098\ttotal: 2.91s\tremaining: 22s\n",
      "118:\tlearn: 0.5100370\ttotal: 2.96s\tremaining: 21.9s\n",
      "120:\tlearn: 0.5087392\ttotal: 3.01s\tremaining: 21.9s\n",
      "122:\tlearn: 0.5074549\ttotal: 3.06s\tremaining: 21.8s\n",
      "124:\tlearn: 0.5061063\ttotal: 3.11s\tremaining: 21.8s\n",
      "126:\tlearn: 0.5048079\ttotal: 3.16s\tremaining: 21.7s\n",
      "128:\tlearn: 0.5037329\ttotal: 3.21s\tremaining: 21.7s\n",
      "130:\tlearn: 0.5023032\ttotal: 3.26s\tremaining: 21.7s\n",
      "132:\tlearn: 0.5011450\ttotal: 3.31s\tremaining: 21.6s\n",
      "134:\tlearn: 0.4996258\ttotal: 3.36s\tremaining: 21.6s\n",
      "136:\tlearn: 0.4979305\ttotal: 3.41s\tremaining: 21.5s\n",
      "138:\tlearn: 0.4962446\ttotal: 3.46s\tremaining: 21.4s\n",
      "140:\tlearn: 0.4953408\ttotal: 3.51s\tremaining: 21.4s\n",
      "142:\tlearn: 0.4938285\ttotal: 3.57s\tremaining: 21.4s\n",
      "144:\tlearn: 0.4923820\ttotal: 3.62s\tremaining: 21.3s\n",
      "146:\tlearn: 0.4911069\ttotal: 3.67s\tremaining: 21.3s\n",
      "148:\tlearn: 0.4895391\ttotal: 3.73s\tremaining: 21.3s\n",
      "150:\tlearn: 0.4881495\ttotal: 3.78s\tremaining: 21.3s\n",
      "152:\tlearn: 0.4869094\ttotal: 3.84s\tremaining: 21.3s\n",
      "154:\tlearn: 0.4855590\ttotal: 3.89s\tremaining: 21.2s\n",
      "156:\tlearn: 0.4844663\ttotal: 3.95s\tremaining: 21.2s\n",
      "158:\tlearn: 0.4832558\ttotal: 4s\tremaining: 21.2s\n",
      "160:\tlearn: 0.4823963\ttotal: 4.06s\tremaining: 21.1s\n",
      "162:\tlearn: 0.4812555\ttotal: 4.11s\tremaining: 21.1s\n",
      "164:\tlearn: 0.4800440\ttotal: 4.16s\tremaining: 21.1s\n",
      "166:\tlearn: 0.4789399\ttotal: 4.21s\tremaining: 21s\n",
      "168:\tlearn: 0.4781308\ttotal: 4.26s\tremaining: 21s\n",
      "170:\tlearn: 0.4771756\ttotal: 4.31s\tremaining: 20.9s\n",
      "172:\tlearn: 0.4759461\ttotal: 4.36s\tremaining: 20.8s\n",
      "174:\tlearn: 0.4747981\ttotal: 4.41s\tremaining: 20.8s\n",
      "176:\tlearn: 0.4736382\ttotal: 4.47s\tremaining: 20.8s\n",
      "178:\tlearn: 0.4725821\ttotal: 4.52s\tremaining: 20.7s\n",
      "180:\tlearn: 0.4716533\ttotal: 4.57s\tremaining: 20.7s\n",
      "182:\tlearn: 0.4705809\ttotal: 4.62s\tremaining: 20.6s\n",
      "184:\tlearn: 0.4696018\ttotal: 4.67s\tremaining: 20.6s\n",
      "186:\tlearn: 0.4688866\ttotal: 4.71s\tremaining: 20.5s\n",
      "188:\tlearn: 0.4678496\ttotal: 4.76s\tremaining: 20.4s\n",
      "190:\tlearn: 0.4670510\ttotal: 4.81s\tremaining: 20.4s\n",
      "192:\tlearn: 0.4658619\ttotal: 4.86s\tremaining: 20.3s\n",
      "194:\tlearn: 0.4652432\ttotal: 4.91s\tremaining: 20.3s\n",
      "196:\tlearn: 0.4645378\ttotal: 4.96s\tremaining: 20.2s\n",
      "198:\tlearn: 0.4634856\ttotal: 5.01s\tremaining: 20.2s\n",
      "200:\tlearn: 0.4627012\ttotal: 5.06s\tremaining: 20.1s\n",
      "202:\tlearn: 0.4617999\ttotal: 5.12s\tremaining: 20.1s\n",
      "204:\tlearn: 0.4607078\ttotal: 5.17s\tremaining: 20s\n",
      "206:\tlearn: 0.4598261\ttotal: 5.22s\tremaining: 20s\n",
      "208:\tlearn: 0.4589594\ttotal: 5.27s\tremaining: 19.9s\n",
      "210:\tlearn: 0.4580456\ttotal: 5.32s\tremaining: 19.9s\n",
      "212:\tlearn: 0.4571112\ttotal: 5.37s\tremaining: 19.9s\n",
      "214:\tlearn: 0.4560788\ttotal: 5.43s\tremaining: 19.8s\n",
      "216:\tlearn: 0.4551481\ttotal: 5.47s\tremaining: 19.7s\n",
      "218:\tlearn: 0.4540412\ttotal: 5.52s\tremaining: 19.7s\n",
      "220:\tlearn: 0.4531933\ttotal: 5.57s\tremaining: 19.6s\n",
      "222:\tlearn: 0.4523048\ttotal: 5.62s\tremaining: 19.6s\n",
      "224:\tlearn: 0.4516181\ttotal: 5.67s\tremaining: 19.5s\n",
      "226:\tlearn: 0.4510290\ttotal: 5.72s\tremaining: 19.5s\n",
      "228:\tlearn: 0.4502631\ttotal: 5.76s\tremaining: 19.4s\n",
      "230:\tlearn: 0.4495500\ttotal: 5.81s\tremaining: 19.4s\n",
      "232:\tlearn: 0.4488495\ttotal: 5.86s\tremaining: 19.3s\n",
      "234:\tlearn: 0.4482463\ttotal: 5.91s\tremaining: 19.3s\n",
      "236:\tlearn: 0.4474210\ttotal: 5.96s\tremaining: 19.2s\n",
      "238:\tlearn: 0.4466364\ttotal: 6.01s\tremaining: 19.1s\n",
      "240:\tlearn: 0.4460738\ttotal: 6.06s\tremaining: 19.1s\n",
      "242:\tlearn: 0.4452302\ttotal: 6.11s\tremaining: 19s\n",
      "244:\tlearn: 0.4447196\ttotal: 6.18s\tremaining: 19s\n",
      "246:\tlearn: 0.4435851\ttotal: 6.23s\tremaining: 19s\n",
      "248:\tlearn: 0.4429541\ttotal: 6.28s\tremaining: 18.9s\n",
      "250:\tlearn: 0.4423128\ttotal: 6.33s\tremaining: 18.9s\n",
      "252:\tlearn: 0.4417453\ttotal: 6.39s\tremaining: 18.9s\n",
      "254:\tlearn: 0.4409863\ttotal: 6.44s\tremaining: 18.8s\n",
      "256:\tlearn: 0.4401960\ttotal: 6.5s\tremaining: 18.8s\n",
      "258:\tlearn: 0.4396794\ttotal: 6.55s\tremaining: 18.8s\n",
      "260:\tlearn: 0.4384640\ttotal: 6.61s\tremaining: 18.7s\n",
      "262:\tlearn: 0.4376213\ttotal: 6.65s\tremaining: 18.6s\n",
      "264:\tlearn: 0.4371905\ttotal: 6.7s\tremaining: 18.6s\n",
      "266:\tlearn: 0.4363084\ttotal: 6.75s\tremaining: 18.5s\n",
      "268:\tlearn: 0.4356339\ttotal: 6.8s\tremaining: 18.5s\n",
      "270:\tlearn: 0.4344576\ttotal: 6.86s\tremaining: 18.4s\n",
      "272:\tlearn: 0.4337178\ttotal: 6.93s\tremaining: 18.4s\n",
      "274:\tlearn: 0.4330944\ttotal: 6.99s\tremaining: 18.4s\n",
      "276:\tlearn: 0.4327276\ttotal: 7.06s\tremaining: 18.4s\n",
      "278:\tlearn: 0.4321933\ttotal: 7.12s\tremaining: 18.4s\n",
      "280:\tlearn: 0.4316327\ttotal: 7.17s\tremaining: 18.3s\n",
      "282:\tlearn: 0.4311392\ttotal: 7.23s\tremaining: 18.3s\n",
      "284:\tlearn: 0.4299317\ttotal: 7.29s\tremaining: 18.3s\n",
      "286:\tlearn: 0.4293200\ttotal: 7.34s\tremaining: 18.2s\n",
      "288:\tlearn: 0.4286959\ttotal: 7.4s\tremaining: 18.2s\n",
      "290:\tlearn: 0.4279969\ttotal: 7.45s\tremaining: 18.2s\n",
      "292:\tlearn: 0.4269520\ttotal: 7.51s\tremaining: 18.1s\n",
      "294:\tlearn: 0.4260668\ttotal: 7.56s\tremaining: 18.1s\n",
      "296:\tlearn: 0.4252018\ttotal: 7.62s\tremaining: 18s\n",
      "298:\tlearn: 0.4247962\ttotal: 7.67s\tremaining: 18s\n",
      "300:\tlearn: 0.4242414\ttotal: 7.72s\tremaining: 17.9s\n",
      "302:\tlearn: 0.4239036\ttotal: 7.76s\tremaining: 17.9s\n",
      "304:\tlearn: 0.4234882\ttotal: 7.82s\tremaining: 17.8s\n",
      "306:\tlearn: 0.4228410\ttotal: 7.87s\tremaining: 17.8s\n",
      "308:\tlearn: 0.4218682\ttotal: 7.92s\tremaining: 17.7s\n",
      "310:\tlearn: 0.4213773\ttotal: 7.96s\tremaining: 17.6s\n",
      "312:\tlearn: 0.4207828\ttotal: 8.02s\tremaining: 17.6s\n",
      "314:\tlearn: 0.4199281\ttotal: 8.07s\tremaining: 17.6s\n",
      "316:\tlearn: 0.4191391\ttotal: 8.12s\tremaining: 17.5s\n",
      "318:\tlearn: 0.4187259\ttotal: 8.17s\tremaining: 17.4s\n",
      "320:\tlearn: 0.4181838\ttotal: 8.22s\tremaining: 17.4s\n",
      "322:\tlearn: 0.4175316\ttotal: 8.27s\tremaining: 17.3s\n",
      "324:\tlearn: 0.4168182\ttotal: 8.32s\tremaining: 17.3s\n",
      "326:\tlearn: 0.4163884\ttotal: 8.37s\tremaining: 17.2s\n",
      "328:\tlearn: 0.4156723\ttotal: 8.41s\tremaining: 17.2s\n",
      "330:\tlearn: 0.4153925\ttotal: 8.46s\tremaining: 17.1s\n",
      "332:\tlearn: 0.4150304\ttotal: 8.52s\tremaining: 17.1s\n",
      "334:\tlearn: 0.4145712\ttotal: 8.57s\tremaining: 17s\n",
      "336:\tlearn: 0.4139094\ttotal: 8.62s\tremaining: 17s\n",
      "338:\tlearn: 0.4134165\ttotal: 8.67s\tremaining: 16.9s\n",
      "340:\tlearn: 0.4125950\ttotal: 8.72s\tremaining: 16.8s\n",
      "342:\tlearn: 0.4118877\ttotal: 8.77s\tremaining: 16.8s\n",
      "344:\tlearn: 0.4112381\ttotal: 8.82s\tremaining: 16.8s\n",
      "346:\tlearn: 0.4107081\ttotal: 8.87s\tremaining: 16.7s\n",
      "348:\tlearn: 0.4101342\ttotal: 8.92s\tremaining: 16.6s\n",
      "350:\tlearn: 0.4095287\ttotal: 8.97s\tremaining: 16.6s\n",
      "352:\tlearn: 0.4089310\ttotal: 9.02s\tremaining: 16.5s\n",
      "354:\tlearn: 0.4084248\ttotal: 9.07s\tremaining: 16.5s\n",
      "356:\tlearn: 0.4077483\ttotal: 9.13s\tremaining: 16.4s\n",
      "358:\tlearn: 0.4071450\ttotal: 9.18s\tremaining: 16.4s\n",
      "360:\tlearn: 0.4065326\ttotal: 9.23s\tremaining: 16.3s\n",
      "362:\tlearn: 0.4062688\ttotal: 9.28s\tremaining: 16.3s\n",
      "364:\tlearn: 0.4058553\ttotal: 9.34s\tremaining: 16.2s\n",
      "366:\tlearn: 0.4050856\ttotal: 9.39s\tremaining: 16.2s\n",
      "368:\tlearn: 0.4048899\ttotal: 9.44s\tremaining: 16.1s\n",
      "370:\tlearn: 0.4043667\ttotal: 9.49s\tremaining: 16.1s\n",
      "372:\tlearn: 0.4038234\ttotal: 9.54s\tremaining: 16s\n",
      "374:\tlearn: 0.4033705\ttotal: 9.59s\tremaining: 16s\n",
      "376:\tlearn: 0.4024821\ttotal: 9.64s\tremaining: 15.9s\n",
      "378:\tlearn: 0.4016857\ttotal: 9.69s\tremaining: 15.9s\n",
      "380:\tlearn: 0.4011507\ttotal: 9.74s\tremaining: 15.8s\n",
      "382:\tlearn: 0.4004868\ttotal: 9.79s\tremaining: 15.8s\n",
      "384:\tlearn: 0.4001387\ttotal: 9.84s\tremaining: 15.7s\n",
      "386:\tlearn: 0.3993614\ttotal: 9.89s\tremaining: 15.7s\n",
      "388:\tlearn: 0.3990022\ttotal: 9.93s\tremaining: 15.6s\n",
      "390:\tlearn: 0.3986671\ttotal: 9.98s\tremaining: 15.5s\n",
      "392:\tlearn: 0.3980145\ttotal: 10s\tremaining: 15.5s\n",
      "394:\tlearn: 0.3975518\ttotal: 10.1s\tremaining: 15.4s\n",
      "396:\tlearn: 0.3970795\ttotal: 10.1s\tremaining: 15.4s\n",
      "398:\tlearn: 0.3968297\ttotal: 10.2s\tremaining: 15.3s\n",
      "400:\tlearn: 0.3961947\ttotal: 10.2s\tremaining: 15.3s\n",
      "402:\tlearn: 0.3955686\ttotal: 10.3s\tremaining: 15.3s\n",
      "404:\tlearn: 0.3949838\ttotal: 10.4s\tremaining: 15.2s\n",
      "406:\tlearn: 0.3942833\ttotal: 10.4s\tremaining: 15.2s\n",
      "408:\tlearn: 0.3938282\ttotal: 10.5s\tremaining: 15.1s\n",
      "410:\tlearn: 0.3929693\ttotal: 10.5s\tremaining: 15.1s\n",
      "412:\tlearn: 0.3926530\ttotal: 10.6s\tremaining: 15s\n",
      "414:\tlearn: 0.3922304\ttotal: 10.6s\tremaining: 15s\n",
      "416:\tlearn: 0.3919727\ttotal: 10.7s\tremaining: 14.9s\n",
      "418:\tlearn: 0.3914102\ttotal: 10.7s\tremaining: 14.9s\n",
      "420:\tlearn: 0.3910425\ttotal: 10.8s\tremaining: 14.8s\n",
      "422:\tlearn: 0.3905410\ttotal: 10.8s\tremaining: 14.8s\n",
      "424:\tlearn: 0.3902443\ttotal: 10.9s\tremaining: 14.7s\n",
      "426:\tlearn: 0.3898271\ttotal: 10.9s\tremaining: 14.7s\n",
      "428:\tlearn: 0.3889481\ttotal: 11s\tremaining: 14.6s\n",
      "430:\tlearn: 0.3885709\ttotal: 11s\tremaining: 14.6s\n",
      "432:\tlearn: 0.3880220\ttotal: 11.1s\tremaining: 14.5s\n",
      "434:\tlearn: 0.3877116\ttotal: 11.1s\tremaining: 14.5s\n",
      "436:\tlearn: 0.3873411\ttotal: 11.2s\tremaining: 14.4s\n",
      "438:\tlearn: 0.3868701\ttotal: 11.3s\tremaining: 14.4s\n",
      "440:\tlearn: 0.3863635\ttotal: 11.3s\tremaining: 14.3s\n",
      "442:\tlearn: 0.3859407\ttotal: 11.4s\tremaining: 14.3s\n",
      "444:\tlearn: 0.3856447\ttotal: 11.4s\tremaining: 14.2s\n",
      "446:\tlearn: 0.3850862\ttotal: 11.5s\tremaining: 14.2s\n",
      "448:\tlearn: 0.3847454\ttotal: 11.5s\tremaining: 14.1s\n",
      "450:\tlearn: 0.3841863\ttotal: 11.6s\tremaining: 14.1s\n",
      "452:\tlearn: 0.3836996\ttotal: 11.6s\tremaining: 14s\n",
      "454:\tlearn: 0.3832219\ttotal: 11.7s\tremaining: 14s\n",
      "456:\tlearn: 0.3829250\ttotal: 11.8s\tremaining: 14s\n",
      "458:\tlearn: 0.3823833\ttotal: 11.8s\tremaining: 13.9s\n",
      "460:\tlearn: 0.3820568\ttotal: 11.9s\tremaining: 13.9s\n",
      "462:\tlearn: 0.3816029\ttotal: 12s\tremaining: 13.9s\n",
      "464:\tlearn: 0.3810058\ttotal: 12s\tremaining: 13.9s\n",
      "466:\tlearn: 0.3802184\ttotal: 12.1s\tremaining: 13.8s\n",
      "468:\tlearn: 0.3796452\ttotal: 12.2s\tremaining: 13.8s\n",
      "470:\tlearn: 0.3790642\ttotal: 12.2s\tremaining: 13.7s\n",
      "472:\tlearn: 0.3785171\ttotal: 12.3s\tremaining: 13.7s\n",
      "474:\tlearn: 0.3778159\ttotal: 12.3s\tremaining: 13.6s\n",
      "476:\tlearn: 0.3773601\ttotal: 12.4s\tremaining: 13.6s\n",
      "478:\tlearn: 0.3769030\ttotal: 12.4s\tremaining: 13.5s\n",
      "480:\tlearn: 0.3765387\ttotal: 12.5s\tremaining: 13.5s\n",
      "482:\tlearn: 0.3760985\ttotal: 12.5s\tremaining: 13.4s\n",
      "484:\tlearn: 0.3756708\ttotal: 12.6s\tremaining: 13.4s\n",
      "486:\tlearn: 0.3753843\ttotal: 12.6s\tremaining: 13.3s\n",
      "488:\tlearn: 0.3749032\ttotal: 12.7s\tremaining: 13.3s\n",
      "490:\tlearn: 0.3742710\ttotal: 12.8s\tremaining: 13.2s\n",
      "492:\tlearn: 0.3740540\ttotal: 12.8s\tremaining: 13.2s\n",
      "494:\tlearn: 0.3735199\ttotal: 12.8s\tremaining: 13.1s\n",
      "496:\tlearn: 0.3731156\ttotal: 12.9s\tremaining: 13.1s\n",
      "498:\tlearn: 0.3726980\ttotal: 12.9s\tremaining: 13s\n",
      "500:\tlearn: 0.3722371\ttotal: 13s\tremaining: 12.9s\n",
      "502:\tlearn: 0.3719259\ttotal: 13s\tremaining: 12.9s\n",
      "504:\tlearn: 0.3714509\ttotal: 13.1s\tremaining: 12.8s\n",
      "506:\tlearn: 0.3712383\ttotal: 13.1s\tremaining: 12.8s\n",
      "508:\tlearn: 0.3706264\ttotal: 13.2s\tremaining: 12.7s\n",
      "510:\tlearn: 0.3704458\ttotal: 13.2s\tremaining: 12.7s\n",
      "512:\tlearn: 0.3700118\ttotal: 13.3s\tremaining: 12.6s\n",
      "514:\tlearn: 0.3693146\ttotal: 13.3s\tremaining: 12.6s\n",
      "516:\tlearn: 0.3691178\ttotal: 13.4s\tremaining: 12.5s\n",
      "518:\tlearn: 0.3687245\ttotal: 13.4s\tremaining: 12.4s\n",
      "520:\tlearn: 0.3683605\ttotal: 13.5s\tremaining: 12.4s\n",
      "522:\tlearn: 0.3679832\ttotal: 13.5s\tremaining: 12.3s\n",
      "524:\tlearn: 0.3675131\ttotal: 13.6s\tremaining: 12.3s\n",
      "526:\tlearn: 0.3672720\ttotal: 13.6s\tremaining: 12.2s\n",
      "528:\tlearn: 0.3670651\ttotal: 13.7s\tremaining: 12.2s\n",
      "530:\tlearn: 0.3667500\ttotal: 13.7s\tremaining: 12.1s\n",
      "532:\tlearn: 0.3665142\ttotal: 13.8s\tremaining: 12.1s\n",
      "534:\tlearn: 0.3656974\ttotal: 13.8s\tremaining: 12s\n",
      "536:\tlearn: 0.3653853\ttotal: 13.9s\tremaining: 12s\n",
      "538:\tlearn: 0.3649804\ttotal: 13.9s\tremaining: 11.9s\n",
      "540:\tlearn: 0.3644864\ttotal: 14s\tremaining: 11.9s\n",
      "542:\tlearn: 0.3640036\ttotal: 14s\tremaining: 11.8s\n",
      "544:\tlearn: 0.3633866\ttotal: 14.1s\tremaining: 11.8s\n",
      "546:\tlearn: 0.3631059\ttotal: 14.1s\tremaining: 11.7s\n",
      "548:\tlearn: 0.3627114\ttotal: 14.2s\tremaining: 11.7s\n",
      "550:\tlearn: 0.3620214\ttotal: 14.2s\tremaining: 11.6s\n",
      "552:\tlearn: 0.3615709\ttotal: 14.3s\tremaining: 11.5s\n",
      "554:\tlearn: 0.3611750\ttotal: 14.3s\tremaining: 11.5s\n",
      "556:\tlearn: 0.3607983\ttotal: 14.4s\tremaining: 11.4s\n",
      "558:\tlearn: 0.3603221\ttotal: 14.4s\tremaining: 11.4s\n",
      "560:\tlearn: 0.3600701\ttotal: 14.5s\tremaining: 11.3s\n",
      "562:\tlearn: 0.3597542\ttotal: 14.5s\tremaining: 11.3s\n",
      "564:\tlearn: 0.3594164\ttotal: 14.6s\tremaining: 11.2s\n",
      "566:\tlearn: 0.3590706\ttotal: 14.6s\tremaining: 11.2s\n",
      "568:\tlearn: 0.3587357\ttotal: 14.7s\tremaining: 11.1s\n",
      "570:\tlearn: 0.3584035\ttotal: 14.7s\tremaining: 11.1s\n",
      "572:\tlearn: 0.3579952\ttotal: 14.8s\tremaining: 11s\n",
      "574:\tlearn: 0.3574336\ttotal: 14.8s\tremaining: 11s\n",
      "576:\tlearn: 0.3567832\ttotal: 14.9s\tremaining: 10.9s\n",
      "578:\tlearn: 0.3563742\ttotal: 14.9s\tremaining: 10.8s\n",
      "580:\tlearn: 0.3560771\ttotal: 15s\tremaining: 10.8s\n",
      "582:\tlearn: 0.3556635\ttotal: 15s\tremaining: 10.7s\n",
      "584:\tlearn: 0.3554854\ttotal: 15.1s\tremaining: 10.7s\n",
      "586:\tlearn: 0.3552355\ttotal: 15.1s\tremaining: 10.6s\n",
      "588:\tlearn: 0.3547520\ttotal: 15.2s\tremaining: 10.6s\n",
      "590:\tlearn: 0.3543196\ttotal: 15.2s\tremaining: 10.5s\n",
      "592:\tlearn: 0.3538094\ttotal: 15.3s\tremaining: 10.5s\n",
      "594:\tlearn: 0.3533993\ttotal: 15.3s\tremaining: 10.4s\n",
      "596:\tlearn: 0.3531503\ttotal: 15.4s\tremaining: 10.4s\n",
      "598:\tlearn: 0.3527857\ttotal: 15.4s\tremaining: 10.3s\n",
      "600:\tlearn: 0.3523302\ttotal: 15.5s\tremaining: 10.3s\n",
      "602:\tlearn: 0.3520125\ttotal: 15.5s\tremaining: 10.2s\n",
      "604:\tlearn: 0.3515213\ttotal: 15.6s\tremaining: 10.2s\n",
      "606:\tlearn: 0.3512257\ttotal: 15.6s\tremaining: 10.1s\n",
      "608:\tlearn: 0.3507667\ttotal: 15.7s\tremaining: 10.1s\n",
      "610:\tlearn: 0.3503367\ttotal: 15.7s\tremaining: 10s\n",
      "612:\tlearn: 0.3501135\ttotal: 15.8s\tremaining: 9.96s\n",
      "614:\tlearn: 0.3494748\ttotal: 15.8s\tremaining: 9.9s\n",
      "616:\tlearn: 0.3492959\ttotal: 15.9s\tremaining: 9.85s\n",
      "618:\tlearn: 0.3488917\ttotal: 15.9s\tremaining: 9.8s\n",
      "620:\tlearn: 0.3485412\ttotal: 16s\tremaining: 9.75s\n",
      "622:\tlearn: 0.3481091\ttotal: 16s\tremaining: 9.7s\n",
      "624:\tlearn: 0.3475918\ttotal: 16.1s\tremaining: 9.64s\n",
      "626:\tlearn: 0.3474395\ttotal: 16.1s\tremaining: 9.59s\n",
      "628:\tlearn: 0.3470363\ttotal: 16.2s\tremaining: 9.54s\n",
      "630:\tlearn: 0.3467218\ttotal: 16.2s\tremaining: 9.49s\n",
      "632:\tlearn: 0.3463769\ttotal: 16.3s\tremaining: 9.44s\n",
      "634:\tlearn: 0.3459143\ttotal: 16.3s\tremaining: 9.39s\n",
      "636:\tlearn: 0.3455431\ttotal: 16.4s\tremaining: 9.34s\n",
      "638:\tlearn: 0.3451400\ttotal: 16.4s\tremaining: 9.29s\n",
      "640:\tlearn: 0.3448651\ttotal: 16.5s\tremaining: 9.24s\n",
      "642:\tlearn: 0.3445939\ttotal: 16.5s\tremaining: 9.19s\n",
      "644:\tlearn: 0.3441331\ttotal: 16.6s\tremaining: 9.13s\n",
      "646:\tlearn: 0.3435666\ttotal: 16.7s\tremaining: 9.09s\n",
      "648:\tlearn: 0.3434018\ttotal: 16.8s\tremaining: 9.06s\n",
      "650:\tlearn: 0.3431516\ttotal: 16.8s\tremaining: 9.02s\n",
      "652:\tlearn: 0.3427715\ttotal: 16.9s\tremaining: 8.98s\n",
      "654:\tlearn: 0.3424495\ttotal: 17s\tremaining: 8.94s\n",
      "656:\tlearn: 0.3422177\ttotal: 17s\tremaining: 8.9s\n",
      "658:\tlearn: 0.3419466\ttotal: 17.1s\tremaining: 8.86s\n",
      "660:\tlearn: 0.3413374\ttotal: 17.2s\tremaining: 8.81s\n",
      "662:\tlearn: 0.3411706\ttotal: 17.2s\tremaining: 8.76s\n",
      "664:\tlearn: 0.3410090\ttotal: 17.3s\tremaining: 8.71s\n",
      "666:\tlearn: 0.3406728\ttotal: 17.3s\tremaining: 8.66s\n",
      "668:\tlearn: 0.3404609\ttotal: 17.4s\tremaining: 8.6s\n",
      "670:\tlearn: 0.3399327\ttotal: 17.4s\tremaining: 8.55s\n",
      "672:\tlearn: 0.3397584\ttotal: 17.5s\tremaining: 8.5s\n",
      "674:\tlearn: 0.3396057\ttotal: 17.5s\tremaining: 8.45s\n",
      "676:\tlearn: 0.3392844\ttotal: 17.6s\tremaining: 8.39s\n",
      "678:\tlearn: 0.3389060\ttotal: 17.6s\tremaining: 8.34s\n",
      "680:\tlearn: 0.3385440\ttotal: 17.7s\tremaining: 8.29s\n",
      "682:\tlearn: 0.3381031\ttotal: 17.7s\tremaining: 8.24s\n",
      "684:\tlearn: 0.3378332\ttotal: 17.8s\tremaining: 8.18s\n",
      "686:\tlearn: 0.3374284\ttotal: 17.8s\tremaining: 8.13s\n",
      "688:\tlearn: 0.3371233\ttotal: 17.9s\tremaining: 8.08s\n",
      "690:\tlearn: 0.3369000\ttotal: 17.9s\tremaining: 8.03s\n",
      "692:\tlearn: 0.3363059\ttotal: 18s\tremaining: 7.97s\n",
      "694:\tlearn: 0.3361083\ttotal: 18s\tremaining: 7.92s\n",
      "696:\tlearn: 0.3359546\ttotal: 18.1s\tremaining: 7.87s\n",
      "698:\tlearn: 0.3357795\ttotal: 18.1s\tremaining: 7.81s\n",
      "700:\tlearn: 0.3355921\ttotal: 18.2s\tremaining: 7.76s\n",
      "702:\tlearn: 0.3354482\ttotal: 18.2s\tremaining: 7.71s\n",
      "704:\tlearn: 0.3351271\ttotal: 18.3s\tremaining: 7.66s\n",
      "706:\tlearn: 0.3348493\ttotal: 18.3s\tremaining: 7.6s\n",
      "708:\tlearn: 0.3345872\ttotal: 18.4s\tremaining: 7.55s\n",
      "710:\tlearn: 0.3342521\ttotal: 18.4s\tremaining: 7.5s\n",
      "712:\tlearn: 0.3339186\ttotal: 18.5s\tremaining: 7.45s\n",
      "714:\tlearn: 0.3335102\ttotal: 18.5s\tremaining: 7.39s\n",
      "716:\tlearn: 0.3327611\ttotal: 18.6s\tremaining: 7.34s\n",
      "718:\tlearn: 0.3325097\ttotal: 18.6s\tremaining: 7.29s\n",
      "720:\tlearn: 0.3322119\ttotal: 18.7s\tremaining: 7.24s\n",
      "722:\tlearn: 0.3319047\ttotal: 18.8s\tremaining: 7.18s\n",
      "724:\tlearn: 0.3315558\ttotal: 18.8s\tremaining: 7.13s\n",
      "726:\tlearn: 0.3312819\ttotal: 18.9s\tremaining: 7.08s\n",
      "728:\tlearn: 0.3307901\ttotal: 18.9s\tremaining: 7.03s\n",
      "730:\tlearn: 0.3306045\ttotal: 19s\tremaining: 6.98s\n",
      "732:\tlearn: 0.3302758\ttotal: 19s\tremaining: 6.92s\n",
      "734:\tlearn: 0.3299504\ttotal: 19.1s\tremaining: 6.87s\n",
      "736:\tlearn: 0.3297748\ttotal: 19.1s\tremaining: 6.82s\n",
      "738:\tlearn: 0.3294834\ttotal: 19.2s\tremaining: 6.77s\n",
      "740:\tlearn: 0.3293198\ttotal: 19.2s\tremaining: 6.72s\n",
      "742:\tlearn: 0.3290315\ttotal: 19.3s\tremaining: 6.67s\n",
      "744:\tlearn: 0.3286660\ttotal: 19.3s\tremaining: 6.62s\n",
      "746:\tlearn: 0.3283275\ttotal: 19.4s\tremaining: 6.57s\n",
      "748:\tlearn: 0.3280723\ttotal: 19.4s\tremaining: 6.52s\n",
      "750:\tlearn: 0.3276036\ttotal: 19.5s\tremaining: 6.47s\n",
      "752:\tlearn: 0.3274398\ttotal: 19.6s\tremaining: 6.41s\n",
      "754:\tlearn: 0.3270753\ttotal: 19.6s\tremaining: 6.36s\n",
      "756:\tlearn: 0.3268829\ttotal: 19.7s\tremaining: 6.31s\n",
      "758:\tlearn: 0.3267273\ttotal: 19.7s\tremaining: 6.26s\n",
      "760:\tlearn: 0.3265524\ttotal: 19.8s\tremaining: 6.21s\n",
      "762:\tlearn: 0.3263600\ttotal: 19.8s\tremaining: 6.15s\n",
      "764:\tlearn: 0.3258057\ttotal: 19.9s\tremaining: 6.1s\n",
      "766:\tlearn: 0.3255122\ttotal: 19.9s\tremaining: 6.05s\n",
      "768:\tlearn: 0.3252129\ttotal: 20s\tremaining: 6s\n",
      "770:\tlearn: 0.3249004\ttotal: 20s\tremaining: 5.95s\n",
      "772:\tlearn: 0.3246567\ttotal: 20.1s\tremaining: 5.89s\n",
      "774:\tlearn: 0.3243239\ttotal: 20.1s\tremaining: 5.84s\n",
      "776:\tlearn: 0.3239798\ttotal: 20.2s\tremaining: 5.79s\n",
      "778:\tlearn: 0.3236609\ttotal: 20.2s\tremaining: 5.74s\n",
      "780:\tlearn: 0.3232550\ttotal: 20.3s\tremaining: 5.69s\n",
      "782:\tlearn: 0.3229705\ttotal: 20.3s\tremaining: 5.63s\n",
      "784:\tlearn: 0.3226718\ttotal: 20.4s\tremaining: 5.58s\n",
      "786:\tlearn: 0.3224739\ttotal: 20.4s\tremaining: 5.53s\n",
      "788:\tlearn: 0.3222675\ttotal: 20.5s\tremaining: 5.48s\n",
      "790:\tlearn: 0.3218166\ttotal: 20.5s\tremaining: 5.42s\n",
      "792:\tlearn: 0.3213490\ttotal: 20.6s\tremaining: 5.37s\n",
      "794:\tlearn: 0.3210303\ttotal: 20.6s\tremaining: 5.32s\n",
      "796:\tlearn: 0.3208879\ttotal: 20.7s\tremaining: 5.27s\n",
      "798:\tlearn: 0.3207136\ttotal: 20.7s\tremaining: 5.21s\n",
      "800:\tlearn: 0.3203457\ttotal: 20.8s\tremaining: 5.16s\n",
      "802:\tlearn: 0.3200520\ttotal: 20.8s\tremaining: 5.11s\n",
      "804:\tlearn: 0.3195244\ttotal: 20.9s\tremaining: 5.06s\n",
      "806:\tlearn: 0.3193709\ttotal: 20.9s\tremaining: 5.01s\n",
      "808:\tlearn: 0.3191019\ttotal: 21s\tremaining: 4.96s\n",
      "810:\tlearn: 0.3189099\ttotal: 21s\tremaining: 4.9s\n",
      "812:\tlearn: 0.3187393\ttotal: 21.1s\tremaining: 4.85s\n",
      "814:\tlearn: 0.3184847\ttotal: 21.2s\tremaining: 4.8s\n",
      "816:\tlearn: 0.3183225\ttotal: 21.2s\tremaining: 4.76s\n",
      "818:\tlearn: 0.3179534\ttotal: 21.3s\tremaining: 4.71s\n",
      "820:\tlearn: 0.3177327\ttotal: 21.4s\tremaining: 4.66s\n",
      "822:\tlearn: 0.3172480\ttotal: 21.4s\tremaining: 4.61s\n",
      "824:\tlearn: 0.3170564\ttotal: 21.5s\tremaining: 4.56s\n",
      "826:\tlearn: 0.3166852\ttotal: 21.6s\tremaining: 4.51s\n",
      "828:\tlearn: 0.3163839\ttotal: 21.6s\tremaining: 4.46s\n",
      "830:\tlearn: 0.3159508\ttotal: 21.7s\tremaining: 4.41s\n",
      "832:\tlearn: 0.3155052\ttotal: 21.7s\tremaining: 4.36s\n",
      "834:\tlearn: 0.3152428\ttotal: 21.8s\tremaining: 4.31s\n",
      "836:\tlearn: 0.3150872\ttotal: 21.8s\tremaining: 4.25s\n",
      "838:\tlearn: 0.3148626\ttotal: 21.9s\tremaining: 4.2s\n",
      "840:\tlearn: 0.3143947\ttotal: 21.9s\tremaining: 4.15s\n",
      "842:\tlearn: 0.3139806\ttotal: 22s\tremaining: 4.1s\n",
      "844:\tlearn: 0.3138392\ttotal: 22.1s\tremaining: 4.05s\n",
      "846:\tlearn: 0.3136786\ttotal: 22.1s\tremaining: 4s\n",
      "848:\tlearn: 0.3135195\ttotal: 22.2s\tremaining: 3.94s\n",
      "850:\tlearn: 0.3132600\ttotal: 22.2s\tremaining: 3.9s\n",
      "852:\tlearn: 0.3128474\ttotal: 22.3s\tremaining: 3.85s\n",
      "854:\tlearn: 0.3127157\ttotal: 22.4s\tremaining: 3.8s\n",
      "856:\tlearn: 0.3125239\ttotal: 22.5s\tremaining: 3.75s\n",
      "858:\tlearn: 0.3123184\ttotal: 22.5s\tremaining: 3.7s\n",
      "860:\tlearn: 0.3120531\ttotal: 22.6s\tremaining: 3.65s\n",
      "862:\tlearn: 0.3119095\ttotal: 22.7s\tremaining: 3.6s\n",
      "864:\tlearn: 0.3114871\ttotal: 22.7s\tremaining: 3.54s\n",
      "866:\tlearn: 0.3111806\ttotal: 22.8s\tremaining: 3.49s\n",
      "868:\tlearn: 0.3108591\ttotal: 22.8s\tremaining: 3.44s\n",
      "870:\tlearn: 0.3105429\ttotal: 22.9s\tremaining: 3.39s\n",
      "872:\tlearn: 0.3099754\ttotal: 23s\tremaining: 3.34s\n",
      "874:\tlearn: 0.3096689\ttotal: 23s\tremaining: 3.29s\n",
      "876:\tlearn: 0.3092522\ttotal: 23.1s\tremaining: 3.24s\n",
      "878:\tlearn: 0.3091028\ttotal: 23.2s\tremaining: 3.19s\n",
      "880:\tlearn: 0.3088393\ttotal: 23.2s\tremaining: 3.14s\n",
      "882:\tlearn: 0.3086566\ttotal: 23.3s\tremaining: 3.09s\n",
      "884:\tlearn: 0.3083735\ttotal: 23.4s\tremaining: 3.04s\n",
      "886:\tlearn: 0.3082480\ttotal: 23.5s\tremaining: 2.99s\n",
      "888:\tlearn: 0.3077789\ttotal: 23.5s\tremaining: 2.94s\n",
      "890:\tlearn: 0.3075991\ttotal: 23.6s\tremaining: 2.88s\n",
      "892:\tlearn: 0.3074787\ttotal: 23.6s\tremaining: 2.83s\n",
      "894:\tlearn: 0.3073516\ttotal: 23.7s\tremaining: 2.78s\n",
      "896:\tlearn: 0.3069637\ttotal: 23.7s\tremaining: 2.73s\n",
      "898:\tlearn: 0.3067219\ttotal: 23.8s\tremaining: 2.67s\n",
      "900:\tlearn: 0.3065737\ttotal: 23.8s\tremaining: 2.62s\n",
      "902:\tlearn: 0.3064335\ttotal: 23.9s\tremaining: 2.57s\n",
      "904:\tlearn: 0.3062795\ttotal: 23.9s\tremaining: 2.51s\n",
      "906:\tlearn: 0.3061049\ttotal: 24s\tremaining: 2.46s\n",
      "908:\tlearn: 0.3057701\ttotal: 24.1s\tremaining: 2.41s\n",
      "910:\tlearn: 0.3055759\ttotal: 24.1s\tremaining: 2.36s\n",
      "912:\tlearn: 0.3052570\ttotal: 24.2s\tremaining: 2.31s\n",
      "914:\tlearn: 0.3049471\ttotal: 24.3s\tremaining: 2.25s\n",
      "916:\tlearn: 0.3046149\ttotal: 24.3s\tremaining: 2.2s\n",
      "918:\tlearn: 0.3044270\ttotal: 24.4s\tremaining: 2.15s\n",
      "920:\tlearn: 0.3041018\ttotal: 24.5s\tremaining: 2.1s\n",
      "922:\tlearn: 0.3038295\ttotal: 24.6s\tremaining: 2.05s\n",
      "924:\tlearn: 0.3035613\ttotal: 24.6s\tremaining: 2s\n",
      "926:\tlearn: 0.3031499\ttotal: 24.7s\tremaining: 1.94s\n",
      "928:\tlearn: 0.3029998\ttotal: 24.8s\tremaining: 1.89s\n",
      "930:\tlearn: 0.3028384\ttotal: 24.8s\tremaining: 1.84s\n",
      "932:\tlearn: 0.3024240\ttotal: 24.9s\tremaining: 1.79s\n",
      "934:\tlearn: 0.3021882\ttotal: 25s\tremaining: 1.74s\n",
      "936:\tlearn: 0.3018578\ttotal: 25s\tremaining: 1.68s\n",
      "938:\tlearn: 0.3015503\ttotal: 25.1s\tremaining: 1.63s\n",
      "940:\tlearn: 0.3014222\ttotal: 25.2s\tremaining: 1.58s\n",
      "942:\tlearn: 0.3012375\ttotal: 25.3s\tremaining: 1.53s\n",
      "944:\tlearn: 0.3008973\ttotal: 25.4s\tremaining: 1.48s\n",
      "946:\tlearn: 0.3007791\ttotal: 25.4s\tremaining: 1.42s\n",
      "948:\tlearn: 0.3004913\ttotal: 25.5s\tremaining: 1.37s\n",
      "950:\tlearn: 0.3002397\ttotal: 25.6s\tremaining: 1.32s\n",
      "952:\tlearn: 0.3000818\ttotal: 25.7s\tremaining: 1.27s\n",
      "954:\tlearn: 0.2998012\ttotal: 25.7s\tremaining: 1.21s\n",
      "956:\tlearn: 0.2995426\ttotal: 25.8s\tremaining: 1.16s\n",
      "958:\tlearn: 0.2993134\ttotal: 25.8s\tremaining: 1.1s\n",
      "960:\tlearn: 0.2991208\ttotal: 25.9s\tremaining: 1.05s\n",
      "962:\tlearn: 0.2988175\ttotal: 26s\tremaining: 997ms\n",
      "964:\tlearn: 0.2986750\ttotal: 26s\tremaining: 944ms\n",
      "966:\tlearn: 0.2983009\ttotal: 26.1s\tremaining: 891ms\n",
      "968:\tlearn: 0.2981454\ttotal: 26.2s\tremaining: 838ms\n",
      "970:\tlearn: 0.2980126\ttotal: 26.2s\tremaining: 784ms\n",
      "972:\tlearn: 0.2978250\ttotal: 26.3s\tremaining: 730ms\n",
      "974:\tlearn: 0.2975932\ttotal: 26.4s\tremaining: 676ms\n",
      "976:\tlearn: 0.2973354\ttotal: 26.4s\tremaining: 622ms\n",
      "978:\tlearn: 0.2971365\ttotal: 26.5s\tremaining: 568ms\n",
      "980:\tlearn: 0.2969176\ttotal: 26.5s\tremaining: 514ms\n",
      "982:\tlearn: 0.2967223\ttotal: 26.6s\tremaining: 460ms\n",
      "984:\tlearn: 0.2964706\ttotal: 26.6s\tremaining: 406ms\n",
      "986:\tlearn: 0.2960170\ttotal: 26.7s\tremaining: 352ms\n",
      "988:\tlearn: 0.2957428\ttotal: 26.7s\tremaining: 298ms\n",
      "990:\tlearn: 0.2954974\ttotal: 26.8s\tremaining: 244ms\n",
      "992:\tlearn: 0.2952252\ttotal: 26.9s\tremaining: 190ms\n",
      "994:\tlearn: 0.2951031\ttotal: 27s\tremaining: 135ms\n",
      "996:\tlearn: 0.2949103\ttotal: 27s\tremaining: 81.3ms\n",
      "998:\tlearn: 0.2947807\ttotal: 27.1s\tremaining: 27.1ms\n",
      "999:\tlearn: 0.2946474\ttotal: 27.1s\tremaining: 0us\n",
      "(      Metric     Value\n",
      "0   F1 Score  0.787287\n",
      "1  Precision  0.796497\n",
      "2     Recall  0.791858,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 779                  90\n",
      "Actual Positive                 227                 427)\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost_classifier = CatBoostClassifier(iterations=1000, depth=4, learning_rate=0.1, verbose=2)\n",
    "catboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = catboost_classifier.predict(X_valid)\n",
    "\n",
    "catboost_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(catboost_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6830062\ttotal: 28.3ms\tremaining: 28.3s\n",
      "2:\tlearn: 0.6635779\ttotal: 83.1ms\tremaining: 27.6s\n",
      "4:\tlearn: 0.6506161\ttotal: 136ms\tremaining: 27s\n",
      "6:\tlearn: 0.6415673\ttotal: 188ms\tremaining: 26.7s\n",
      "8:\tlearn: 0.6337147\ttotal: 240ms\tremaining: 26.5s\n",
      "10:\tlearn: 0.6258401\ttotal: 295ms\tremaining: 26.6s\n",
      "12:\tlearn: 0.6205170\ttotal: 347ms\tremaining: 26.3s\n",
      "14:\tlearn: 0.6152465\ttotal: 400ms\tremaining: 26.3s\n",
      "16:\tlearn: 0.6112210\ttotal: 453ms\tremaining: 26.2s\n",
      "18:\tlearn: 0.6066602\ttotal: 507ms\tremaining: 26.2s\n",
      "20:\tlearn: 0.6036622\ttotal: 559ms\tremaining: 26.1s\n",
      "22:\tlearn: 0.6004812\ttotal: 612ms\tremaining: 26s\n",
      "24:\tlearn: 0.5975389\ttotal: 665ms\tremaining: 25.9s\n",
      "26:\tlearn: 0.5946601\ttotal: 718ms\tremaining: 25.9s\n",
      "28:\tlearn: 0.5914057\ttotal: 771ms\tremaining: 25.8s\n",
      "30:\tlearn: 0.5874459\ttotal: 824ms\tremaining: 25.8s\n",
      "32:\tlearn: 0.5840149\ttotal: 880ms\tremaining: 25.8s\n",
      "34:\tlearn: 0.5819688\ttotal: 936ms\tremaining: 25.8s\n",
      "36:\tlearn: 0.5796179\ttotal: 989ms\tremaining: 25.7s\n",
      "38:\tlearn: 0.5774566\ttotal: 1.04s\tremaining: 25.7s\n",
      "40:\tlearn: 0.5752932\ttotal: 1.09s\tremaining: 25.6s\n",
      "42:\tlearn: 0.5733644\ttotal: 1.15s\tremaining: 25.5s\n",
      "44:\tlearn: 0.5713063\ttotal: 1.2s\tremaining: 25.5s\n",
      "46:\tlearn: 0.5696551\ttotal: 1.25s\tremaining: 25.5s\n",
      "48:\tlearn: 0.5676224\ttotal: 1.31s\tremaining: 25.4s\n",
      "50:\tlearn: 0.5653741\ttotal: 1.36s\tremaining: 25.4s\n",
      "52:\tlearn: 0.5636199\ttotal: 1.42s\tremaining: 25.4s\n",
      "54:\tlearn: 0.5615193\ttotal: 1.47s\tremaining: 25.3s\n",
      "56:\tlearn: 0.5595586\ttotal: 1.52s\tremaining: 25.2s\n",
      "58:\tlearn: 0.5579757\ttotal: 1.58s\tremaining: 25.2s\n",
      "60:\tlearn: 0.5567404\ttotal: 1.63s\tremaining: 25.1s\n",
      "62:\tlearn: 0.5552066\ttotal: 1.68s\tremaining: 25s\n",
      "64:\tlearn: 0.5531169\ttotal: 1.74s\tremaining: 25s\n",
      "66:\tlearn: 0.5517165\ttotal: 1.79s\tremaining: 25s\n",
      "68:\tlearn: 0.5501875\ttotal: 1.85s\tremaining: 25s\n",
      "70:\tlearn: 0.5488514\ttotal: 1.91s\tremaining: 24.9s\n",
      "72:\tlearn: 0.5472511\ttotal: 1.96s\tremaining: 24.9s\n",
      "74:\tlearn: 0.5452319\ttotal: 2.02s\tremaining: 24.9s\n",
      "76:\tlearn: 0.5439056\ttotal: 2.07s\tremaining: 24.8s\n",
      "78:\tlearn: 0.5422229\ttotal: 2.12s\tremaining: 24.7s\n",
      "80:\tlearn: 0.5405010\ttotal: 2.17s\tremaining: 24.7s\n",
      "82:\tlearn: 0.5391290\ttotal: 2.23s\tremaining: 24.6s\n",
      "84:\tlearn: 0.5376498\ttotal: 2.28s\tremaining: 24.5s\n",
      "86:\tlearn: 0.5363120\ttotal: 2.33s\tremaining: 24.5s\n",
      "88:\tlearn: 0.5346387\ttotal: 2.38s\tremaining: 24.4s\n",
      "90:\tlearn: 0.5331878\ttotal: 2.44s\tremaining: 24.3s\n",
      "92:\tlearn: 0.5314834\ttotal: 2.5s\tremaining: 24.4s\n",
      "94:\tlearn: 0.5297902\ttotal: 2.56s\tremaining: 24.4s\n",
      "96:\tlearn: 0.5284523\ttotal: 2.62s\tremaining: 24.4s\n",
      "98:\tlearn: 0.5271477\ttotal: 2.69s\tremaining: 24.5s\n",
      "100:\tlearn: 0.5255963\ttotal: 2.75s\tremaining: 24.5s\n",
      "102:\tlearn: 0.5240034\ttotal: 2.82s\tremaining: 24.5s\n",
      "104:\tlearn: 0.5227641\ttotal: 2.88s\tremaining: 24.6s\n",
      "106:\tlearn: 0.5214401\ttotal: 2.94s\tremaining: 24.5s\n",
      "108:\tlearn: 0.5201072\ttotal: 3s\tremaining: 24.5s\n",
      "110:\tlearn: 0.5185683\ttotal: 3.06s\tremaining: 24.5s\n",
      "112:\tlearn: 0.5169913\ttotal: 3.12s\tremaining: 24.5s\n",
      "114:\tlearn: 0.5156173\ttotal: 3.17s\tremaining: 24.4s\n",
      "116:\tlearn: 0.5141129\ttotal: 3.23s\tremaining: 24.4s\n",
      "118:\tlearn: 0.5127791\ttotal: 3.29s\tremaining: 24.4s\n",
      "120:\tlearn: 0.5115450\ttotal: 3.35s\tremaining: 24.4s\n",
      "122:\tlearn: 0.5101394\ttotal: 3.44s\tremaining: 24.5s\n",
      "124:\tlearn: 0.5090032\ttotal: 3.53s\tremaining: 24.7s\n",
      "126:\tlearn: 0.5078224\ttotal: 3.6s\tremaining: 24.7s\n",
      "128:\tlearn: 0.5065476\ttotal: 3.66s\tremaining: 24.7s\n",
      "130:\tlearn: 0.5052482\ttotal: 3.72s\tremaining: 24.7s\n",
      "132:\tlearn: 0.5036573\ttotal: 3.78s\tremaining: 24.6s\n",
      "134:\tlearn: 0.5023829\ttotal: 3.84s\tremaining: 24.6s\n",
      "136:\tlearn: 0.5008425\ttotal: 3.89s\tremaining: 24.5s\n",
      "138:\tlearn: 0.4995065\ttotal: 3.95s\tremaining: 24.4s\n",
      "140:\tlearn: 0.4984241\ttotal: 4.01s\tremaining: 24.4s\n",
      "142:\tlearn: 0.4973358\ttotal: 4.08s\tremaining: 24.5s\n",
      "144:\tlearn: 0.4960726\ttotal: 4.16s\tremaining: 24.5s\n",
      "146:\tlearn: 0.4951145\ttotal: 4.23s\tremaining: 24.5s\n",
      "148:\tlearn: 0.4936506\ttotal: 4.29s\tremaining: 24.5s\n",
      "150:\tlearn: 0.4924485\ttotal: 4.35s\tremaining: 24.4s\n",
      "152:\tlearn: 0.4910990\ttotal: 4.41s\tremaining: 24.4s\n",
      "154:\tlearn: 0.4898199\ttotal: 4.47s\tremaining: 24.4s\n",
      "156:\tlearn: 0.4885409\ttotal: 4.52s\tremaining: 24.3s\n",
      "158:\tlearn: 0.4874118\ttotal: 4.58s\tremaining: 24.2s\n",
      "160:\tlearn: 0.4863282\ttotal: 4.64s\tremaining: 24.2s\n",
      "162:\tlearn: 0.4853862\ttotal: 4.72s\tremaining: 24.2s\n",
      "164:\tlearn: 0.4842359\ttotal: 4.78s\tremaining: 24.2s\n",
      "166:\tlearn: 0.4829805\ttotal: 4.84s\tremaining: 24.2s\n",
      "168:\tlearn: 0.4820366\ttotal: 4.9s\tremaining: 24.1s\n",
      "170:\tlearn: 0.4810091\ttotal: 4.95s\tremaining: 24s\n",
      "172:\tlearn: 0.4797289\ttotal: 5.01s\tremaining: 23.9s\n",
      "174:\tlearn: 0.4788531\ttotal: 5.07s\tremaining: 23.9s\n",
      "176:\tlearn: 0.4779995\ttotal: 5.13s\tremaining: 23.8s\n",
      "178:\tlearn: 0.4772875\ttotal: 5.19s\tremaining: 23.8s\n",
      "180:\tlearn: 0.4761554\ttotal: 5.26s\tremaining: 23.8s\n",
      "182:\tlearn: 0.4752220\ttotal: 5.32s\tremaining: 23.7s\n",
      "184:\tlearn: 0.4742562\ttotal: 5.38s\tremaining: 23.7s\n",
      "186:\tlearn: 0.4735663\ttotal: 5.43s\tremaining: 23.6s\n",
      "188:\tlearn: 0.4725228\ttotal: 5.49s\tremaining: 23.6s\n",
      "190:\tlearn: 0.4718600\ttotal: 5.54s\tremaining: 23.5s\n",
      "192:\tlearn: 0.4709865\ttotal: 5.6s\tremaining: 23.4s\n",
      "194:\tlearn: 0.4701243\ttotal: 5.65s\tremaining: 23.3s\n",
      "196:\tlearn: 0.4689553\ttotal: 5.7s\tremaining: 23.2s\n",
      "198:\tlearn: 0.4683115\ttotal: 5.76s\tremaining: 23.2s\n",
      "200:\tlearn: 0.4675148\ttotal: 5.82s\tremaining: 23.1s\n",
      "202:\tlearn: 0.4668943\ttotal: 5.89s\tremaining: 23.1s\n",
      "204:\tlearn: 0.4658377\ttotal: 5.96s\tremaining: 23.1s\n",
      "206:\tlearn: 0.4649449\ttotal: 6.01s\tremaining: 23s\n",
      "208:\tlearn: 0.4639209\ttotal: 6.07s\tremaining: 23s\n",
      "210:\tlearn: 0.4632730\ttotal: 6.12s\tremaining: 22.9s\n",
      "212:\tlearn: 0.4620014\ttotal: 6.18s\tremaining: 22.8s\n",
      "214:\tlearn: 0.4608762\ttotal: 6.24s\tremaining: 22.8s\n",
      "216:\tlearn: 0.4601006\ttotal: 6.3s\tremaining: 22.7s\n",
      "218:\tlearn: 0.4591792\ttotal: 6.36s\tremaining: 22.7s\n",
      "220:\tlearn: 0.4584828\ttotal: 6.41s\tremaining: 22.6s\n",
      "222:\tlearn: 0.4576762\ttotal: 6.47s\tremaining: 22.5s\n",
      "224:\tlearn: 0.4568199\ttotal: 6.52s\tremaining: 22.5s\n",
      "226:\tlearn: 0.4561735\ttotal: 6.57s\tremaining: 22.4s\n",
      "228:\tlearn: 0.4554194\ttotal: 6.63s\tremaining: 22.3s\n",
      "230:\tlearn: 0.4546983\ttotal: 6.69s\tremaining: 22.3s\n",
      "232:\tlearn: 0.4536641\ttotal: 6.75s\tremaining: 22.2s\n",
      "234:\tlearn: 0.4529739\ttotal: 6.82s\tremaining: 22.2s\n",
      "236:\tlearn: 0.4521224\ttotal: 6.88s\tremaining: 22.1s\n",
      "238:\tlearn: 0.4514866\ttotal: 6.94s\tremaining: 22.1s\n",
      "240:\tlearn: 0.4504480\ttotal: 7s\tremaining: 22s\n",
      "242:\tlearn: 0.4499077\ttotal: 7.06s\tremaining: 22s\n",
      "244:\tlearn: 0.4489527\ttotal: 7.12s\tremaining: 22s\n",
      "246:\tlearn: 0.4481287\ttotal: 7.18s\tremaining: 21.9s\n",
      "248:\tlearn: 0.4474195\ttotal: 7.24s\tremaining: 21.8s\n",
      "250:\tlearn: 0.4465809\ttotal: 7.3s\tremaining: 21.8s\n",
      "252:\tlearn: 0.4459812\ttotal: 7.35s\tremaining: 21.7s\n",
      "254:\tlearn: 0.4453923\ttotal: 7.41s\tremaining: 21.7s\n",
      "256:\tlearn: 0.4446102\ttotal: 7.47s\tremaining: 21.6s\n",
      "258:\tlearn: 0.4439586\ttotal: 7.57s\tremaining: 21.7s\n",
      "260:\tlearn: 0.4431073\ttotal: 7.65s\tremaining: 21.7s\n",
      "262:\tlearn: 0.4424471\ttotal: 7.71s\tremaining: 21.6s\n",
      "264:\tlearn: 0.4419935\ttotal: 7.78s\tremaining: 21.6s\n",
      "266:\tlearn: 0.4412930\ttotal: 7.85s\tremaining: 21.6s\n",
      "268:\tlearn: 0.4402421\ttotal: 7.92s\tremaining: 21.5s\n",
      "270:\tlearn: 0.4395484\ttotal: 7.99s\tremaining: 21.5s\n",
      "272:\tlearn: 0.4389262\ttotal: 8.07s\tremaining: 21.5s\n",
      "274:\tlearn: 0.4382703\ttotal: 8.16s\tremaining: 21.5s\n",
      "276:\tlearn: 0.4376189\ttotal: 8.22s\tremaining: 21.5s\n",
      "278:\tlearn: 0.4368907\ttotal: 8.28s\tremaining: 21.4s\n",
      "280:\tlearn: 0.4362656\ttotal: 8.35s\tremaining: 21.4s\n",
      "282:\tlearn: 0.4355398\ttotal: 8.41s\tremaining: 21.3s\n",
      "284:\tlearn: 0.4350651\ttotal: 8.48s\tremaining: 21.3s\n",
      "286:\tlearn: 0.4344972\ttotal: 8.55s\tremaining: 21.2s\n",
      "288:\tlearn: 0.4335987\ttotal: 8.61s\tremaining: 21.2s\n",
      "290:\tlearn: 0.4330020\ttotal: 8.67s\tremaining: 21.1s\n",
      "292:\tlearn: 0.4321646\ttotal: 8.74s\tremaining: 21.1s\n",
      "294:\tlearn: 0.4315625\ttotal: 8.8s\tremaining: 21s\n",
      "296:\tlearn: 0.4309242\ttotal: 8.86s\tremaining: 21s\n",
      "298:\tlearn: 0.4301708\ttotal: 8.92s\tremaining: 20.9s\n",
      "300:\tlearn: 0.4293989\ttotal: 8.99s\tremaining: 20.9s\n",
      "302:\tlearn: 0.4287094\ttotal: 9.05s\tremaining: 20.8s\n",
      "304:\tlearn: 0.4283646\ttotal: 9.11s\tremaining: 20.7s\n",
      "306:\tlearn: 0.4273997\ttotal: 9.16s\tremaining: 20.7s\n",
      "308:\tlearn: 0.4265669\ttotal: 9.23s\tremaining: 20.6s\n",
      "310:\tlearn: 0.4261030\ttotal: 9.29s\tremaining: 20.6s\n",
      "312:\tlearn: 0.4255086\ttotal: 9.35s\tremaining: 20.5s\n",
      "314:\tlearn: 0.4249051\ttotal: 9.41s\tremaining: 20.5s\n",
      "316:\tlearn: 0.4242406\ttotal: 9.47s\tremaining: 20.4s\n",
      "318:\tlearn: 0.4238149\ttotal: 9.53s\tremaining: 20.4s\n",
      "320:\tlearn: 0.4235271\ttotal: 9.61s\tremaining: 20.3s\n",
      "322:\tlearn: 0.4228477\ttotal: 9.68s\tremaining: 20.3s\n",
      "324:\tlearn: 0.4221748\ttotal: 9.73s\tremaining: 20.2s\n",
      "326:\tlearn: 0.4215443\ttotal: 9.79s\tremaining: 20.1s\n",
      "328:\tlearn: 0.4212147\ttotal: 9.85s\tremaining: 20.1s\n",
      "330:\tlearn: 0.4205366\ttotal: 9.9s\tremaining: 20s\n",
      "332:\tlearn: 0.4198106\ttotal: 9.96s\tremaining: 19.9s\n",
      "334:\tlearn: 0.4195034\ttotal: 10s\tremaining: 19.9s\n",
      "336:\tlearn: 0.4190382\ttotal: 10.1s\tremaining: 19.8s\n",
      "338:\tlearn: 0.4186900\ttotal: 10.1s\tremaining: 19.7s\n",
      "340:\tlearn: 0.4180054\ttotal: 10.2s\tremaining: 19.7s\n",
      "342:\tlearn: 0.4175559\ttotal: 10.2s\tremaining: 19.6s\n",
      "344:\tlearn: 0.4169507\ttotal: 10.3s\tremaining: 19.6s\n",
      "346:\tlearn: 0.4167330\ttotal: 10.4s\tremaining: 19.5s\n",
      "348:\tlearn: 0.4162334\ttotal: 10.4s\tremaining: 19.4s\n",
      "350:\tlearn: 0.4156883\ttotal: 10.5s\tremaining: 19.4s\n",
      "352:\tlearn: 0.4148163\ttotal: 10.5s\tremaining: 19.3s\n",
      "354:\tlearn: 0.4141782\ttotal: 10.6s\tremaining: 19.3s\n",
      "356:\tlearn: 0.4136416\ttotal: 10.7s\tremaining: 19.2s\n",
      "358:\tlearn: 0.4132241\ttotal: 10.7s\tremaining: 19.2s\n",
      "360:\tlearn: 0.4129053\ttotal: 10.8s\tremaining: 19.1s\n",
      "362:\tlearn: 0.4124418\ttotal: 10.9s\tremaining: 19.1s\n",
      "364:\tlearn: 0.4118699\ttotal: 11s\tremaining: 19.1s\n",
      "366:\tlearn: 0.4114425\ttotal: 11s\tremaining: 19s\n",
      "368:\tlearn: 0.4107915\ttotal: 11.1s\tremaining: 18.9s\n",
      "370:\tlearn: 0.4104171\ttotal: 11.1s\tremaining: 18.9s\n",
      "372:\tlearn: 0.4100582\ttotal: 11.2s\tremaining: 18.8s\n",
      "374:\tlearn: 0.4095818\ttotal: 11.2s\tremaining: 18.7s\n",
      "376:\tlearn: 0.4091576\ttotal: 11.3s\tremaining: 18.7s\n",
      "378:\tlearn: 0.4086278\ttotal: 11.3s\tremaining: 18.6s\n",
      "380:\tlearn: 0.4081349\ttotal: 11.4s\tremaining: 18.5s\n",
      "382:\tlearn: 0.4076389\ttotal: 11.5s\tremaining: 18.5s\n",
      "384:\tlearn: 0.4068788\ttotal: 11.5s\tremaining: 18.4s\n",
      "386:\tlearn: 0.4063378\ttotal: 11.6s\tremaining: 18.3s\n",
      "388:\tlearn: 0.4058247\ttotal: 11.6s\tremaining: 18.3s\n",
      "390:\tlearn: 0.4055030\ttotal: 11.7s\tremaining: 18.2s\n",
      "392:\tlearn: 0.4048369\ttotal: 11.7s\tremaining: 18.1s\n",
      "394:\tlearn: 0.4042509\ttotal: 11.8s\tremaining: 18.1s\n",
      "396:\tlearn: 0.4033215\ttotal: 11.8s\tremaining: 18s\n",
      "398:\tlearn: 0.4026069\ttotal: 11.9s\tremaining: 17.9s\n",
      "400:\tlearn: 0.4020069\ttotal: 12s\tremaining: 17.9s\n",
      "402:\tlearn: 0.4015757\ttotal: 12s\tremaining: 17.8s\n",
      "404:\tlearn: 0.4011340\ttotal: 12.1s\tremaining: 17.7s\n",
      "406:\tlearn: 0.4006782\ttotal: 12.1s\tremaining: 17.7s\n",
      "408:\tlearn: 0.4003245\ttotal: 12.2s\tremaining: 17.6s\n",
      "410:\tlearn: 0.3998219\ttotal: 12.3s\tremaining: 17.6s\n",
      "412:\tlearn: 0.3991100\ttotal: 12.3s\tremaining: 17.5s\n",
      "414:\tlearn: 0.3984094\ttotal: 12.4s\tremaining: 17.5s\n",
      "416:\tlearn: 0.3981452\ttotal: 12.5s\tremaining: 17.4s\n",
      "418:\tlearn: 0.3979272\ttotal: 12.5s\tremaining: 17.4s\n",
      "420:\tlearn: 0.3972665\ttotal: 12.6s\tremaining: 17.3s\n",
      "422:\tlearn: 0.3966901\ttotal: 12.6s\tremaining: 17.2s\n",
      "424:\tlearn: 0.3961348\ttotal: 12.7s\tremaining: 17.2s\n",
      "426:\tlearn: 0.3957229\ttotal: 12.8s\tremaining: 17.1s\n",
      "428:\tlearn: 0.3950565\ttotal: 12.8s\tremaining: 17.1s\n",
      "430:\tlearn: 0.3947985\ttotal: 12.9s\tremaining: 17s\n",
      "432:\tlearn: 0.3942231\ttotal: 12.9s\tremaining: 16.9s\n",
      "434:\tlearn: 0.3940271\ttotal: 13s\tremaining: 16.9s\n",
      "436:\tlearn: 0.3936444\ttotal: 13.1s\tremaining: 16.8s\n",
      "438:\tlearn: 0.3930319\ttotal: 13.1s\tremaining: 16.8s\n",
      "440:\tlearn: 0.3924702\ttotal: 13.2s\tremaining: 16.7s\n",
      "442:\tlearn: 0.3918970\ttotal: 13.2s\tremaining: 16.6s\n",
      "444:\tlearn: 0.3915172\ttotal: 13.3s\tremaining: 16.6s\n",
      "446:\tlearn: 0.3910999\ttotal: 13.3s\tremaining: 16.5s\n",
      "448:\tlearn: 0.3906982\ttotal: 13.4s\tremaining: 16.4s\n",
      "450:\tlearn: 0.3902286\ttotal: 13.5s\tremaining: 16.4s\n",
      "452:\tlearn: 0.3897003\ttotal: 13.5s\tremaining: 16.3s\n",
      "454:\tlearn: 0.3892326\ttotal: 13.6s\tremaining: 16.3s\n",
      "456:\tlearn: 0.3884355\ttotal: 13.6s\tremaining: 16.2s\n",
      "458:\tlearn: 0.3880538\ttotal: 13.7s\tremaining: 16.2s\n",
      "460:\tlearn: 0.3876361\ttotal: 13.8s\tremaining: 16.1s\n",
      "462:\tlearn: 0.3871891\ttotal: 13.8s\tremaining: 16s\n",
      "464:\tlearn: 0.3867423\ttotal: 13.9s\tremaining: 16s\n",
      "466:\tlearn: 0.3863187\ttotal: 13.9s\tremaining: 15.9s\n",
      "468:\tlearn: 0.3861417\ttotal: 14s\tremaining: 15.8s\n",
      "470:\tlearn: 0.3855966\ttotal: 14.1s\tremaining: 15.8s\n",
      "472:\tlearn: 0.3850937\ttotal: 14.1s\tremaining: 15.7s\n",
      "474:\tlearn: 0.3844877\ttotal: 14.2s\tremaining: 15.7s\n",
      "476:\tlearn: 0.3840530\ttotal: 14.2s\tremaining: 15.6s\n",
      "478:\tlearn: 0.3837806\ttotal: 14.3s\tremaining: 15.5s\n",
      "480:\tlearn: 0.3831453\ttotal: 14.3s\tremaining: 15.5s\n",
      "482:\tlearn: 0.3828587\ttotal: 14.4s\tremaining: 15.4s\n",
      "484:\tlearn: 0.3826378\ttotal: 14.5s\tremaining: 15.3s\n",
      "486:\tlearn: 0.3821613\ttotal: 14.5s\tremaining: 15.3s\n",
      "488:\tlearn: 0.3814133\ttotal: 14.6s\tremaining: 15.2s\n",
      "490:\tlearn: 0.3810008\ttotal: 14.6s\tremaining: 15.2s\n",
      "492:\tlearn: 0.3806524\ttotal: 14.7s\tremaining: 15.1s\n",
      "494:\tlearn: 0.3802146\ttotal: 14.7s\tremaining: 15s\n",
      "496:\tlearn: 0.3797326\ttotal: 14.8s\tremaining: 15s\n",
      "498:\tlearn: 0.3795368\ttotal: 14.8s\tremaining: 14.9s\n",
      "500:\tlearn: 0.3789619\ttotal: 14.9s\tremaining: 14.8s\n",
      "502:\tlearn: 0.3787556\ttotal: 14.9s\tremaining: 14.8s\n",
      "504:\tlearn: 0.3782305\ttotal: 15s\tremaining: 14.7s\n",
      "506:\tlearn: 0.3778112\ttotal: 15.1s\tremaining: 14.6s\n",
      "508:\tlearn: 0.3776318\ttotal: 15.1s\tremaining: 14.6s\n",
      "510:\tlearn: 0.3770908\ttotal: 15.2s\tremaining: 14.5s\n",
      "512:\tlearn: 0.3765355\ttotal: 15.2s\tremaining: 14.5s\n",
      "514:\tlearn: 0.3762163\ttotal: 15.3s\tremaining: 14.4s\n",
      "516:\tlearn: 0.3757305\ttotal: 15.4s\tremaining: 14.4s\n",
      "518:\tlearn: 0.3753779\ttotal: 15.5s\tremaining: 14.4s\n",
      "520:\tlearn: 0.3749696\ttotal: 15.6s\tremaining: 14.3s\n",
      "522:\tlearn: 0.3744814\ttotal: 15.6s\tremaining: 14.3s\n",
      "524:\tlearn: 0.3742592\ttotal: 15.7s\tremaining: 14.2s\n",
      "526:\tlearn: 0.3736332\ttotal: 15.8s\tremaining: 14.2s\n",
      "528:\tlearn: 0.3732326\ttotal: 15.9s\tremaining: 14.1s\n",
      "530:\tlearn: 0.3728985\ttotal: 16s\tremaining: 14.1s\n",
      "532:\tlearn: 0.3723871\ttotal: 16.1s\tremaining: 14.1s\n",
      "534:\tlearn: 0.3720064\ttotal: 16.2s\tremaining: 14.1s\n",
      "536:\tlearn: 0.3717858\ttotal: 16.3s\tremaining: 14.1s\n",
      "538:\tlearn: 0.3713035\ttotal: 16.4s\tremaining: 14s\n",
      "540:\tlearn: 0.3707687\ttotal: 16.5s\tremaining: 14s\n",
      "542:\tlearn: 0.3703272\ttotal: 16.6s\tremaining: 13.9s\n",
      "544:\tlearn: 0.3700160\ttotal: 16.6s\tremaining: 13.9s\n",
      "546:\tlearn: 0.3695107\ttotal: 16.7s\tremaining: 13.8s\n",
      "548:\tlearn: 0.3691661\ttotal: 16.8s\tremaining: 13.8s\n",
      "550:\tlearn: 0.3688165\ttotal: 16.8s\tremaining: 13.7s\n",
      "552:\tlearn: 0.3683668\ttotal: 16.9s\tremaining: 13.7s\n",
      "554:\tlearn: 0.3678741\ttotal: 17s\tremaining: 13.6s\n",
      "556:\tlearn: 0.3675322\ttotal: 17s\tremaining: 13.5s\n",
      "558:\tlearn: 0.3672491\ttotal: 17.1s\tremaining: 13.5s\n",
      "560:\tlearn: 0.3668954\ttotal: 17.2s\tremaining: 13.4s\n",
      "562:\tlearn: 0.3663726\ttotal: 17.2s\tremaining: 13.4s\n",
      "564:\tlearn: 0.3658980\ttotal: 17.3s\tremaining: 13.3s\n",
      "566:\tlearn: 0.3656767\ttotal: 17.4s\tremaining: 13.3s\n",
      "568:\tlearn: 0.3651381\ttotal: 17.4s\tremaining: 13.2s\n",
      "570:\tlearn: 0.3646603\ttotal: 17.5s\tremaining: 13.1s\n",
      "572:\tlearn: 0.3644867\ttotal: 17.5s\tremaining: 13.1s\n",
      "574:\tlearn: 0.3642111\ttotal: 17.6s\tremaining: 13s\n",
      "576:\tlearn: 0.3638465\ttotal: 17.6s\tremaining: 12.9s\n",
      "578:\tlearn: 0.3634989\ttotal: 17.7s\tremaining: 12.9s\n",
      "580:\tlearn: 0.3631448\ttotal: 17.7s\tremaining: 12.8s\n",
      "582:\tlearn: 0.3627603\ttotal: 17.8s\tremaining: 12.7s\n",
      "584:\tlearn: 0.3623469\ttotal: 17.9s\tremaining: 12.7s\n",
      "586:\tlearn: 0.3621793\ttotal: 17.9s\tremaining: 12.6s\n",
      "588:\tlearn: 0.3618029\ttotal: 18s\tremaining: 12.5s\n",
      "590:\tlearn: 0.3614512\ttotal: 18s\tremaining: 12.5s\n",
      "592:\tlearn: 0.3611105\ttotal: 18.1s\tremaining: 12.4s\n",
      "594:\tlearn: 0.3606999\ttotal: 18.2s\tremaining: 12.4s\n",
      "596:\tlearn: 0.3603413\ttotal: 18.2s\tremaining: 12.3s\n",
      "598:\tlearn: 0.3599943\ttotal: 18.3s\tremaining: 12.3s\n",
      "600:\tlearn: 0.3596709\ttotal: 18.4s\tremaining: 12.2s\n",
      "602:\tlearn: 0.3592663\ttotal: 18.4s\tremaining: 12.1s\n",
      "604:\tlearn: 0.3588836\ttotal: 18.5s\tremaining: 12.1s\n",
      "606:\tlearn: 0.3585729\ttotal: 18.5s\tremaining: 12s\n",
      "608:\tlearn: 0.3582181\ttotal: 18.6s\tremaining: 11.9s\n",
      "610:\tlearn: 0.3577629\ttotal: 18.7s\tremaining: 11.9s\n",
      "612:\tlearn: 0.3575181\ttotal: 18.7s\tremaining: 11.8s\n",
      "614:\tlearn: 0.3573204\ttotal: 18.8s\tremaining: 11.8s\n",
      "616:\tlearn: 0.3567812\ttotal: 18.8s\tremaining: 11.7s\n",
      "618:\tlearn: 0.3562013\ttotal: 18.9s\tremaining: 11.6s\n",
      "620:\tlearn: 0.3558039\ttotal: 19s\tremaining: 11.6s\n",
      "622:\tlearn: 0.3553492\ttotal: 19.1s\tremaining: 11.5s\n",
      "624:\tlearn: 0.3547253\ttotal: 19.1s\tremaining: 11.5s\n",
      "626:\tlearn: 0.3543313\ttotal: 19.2s\tremaining: 11.4s\n",
      "628:\tlearn: 0.3541467\ttotal: 19.3s\tremaining: 11.4s\n",
      "630:\tlearn: 0.3537820\ttotal: 19.3s\tremaining: 11.3s\n",
      "632:\tlearn: 0.3534922\ttotal: 19.4s\tremaining: 11.2s\n",
      "634:\tlearn: 0.3532195\ttotal: 19.4s\tremaining: 11.2s\n",
      "636:\tlearn: 0.3528648\ttotal: 19.5s\tremaining: 11.1s\n",
      "638:\tlearn: 0.3525429\ttotal: 19.6s\tremaining: 11.1s\n",
      "640:\tlearn: 0.3522058\ttotal: 19.6s\tremaining: 11s\n",
      "642:\tlearn: 0.3520242\ttotal: 19.7s\tremaining: 10.9s\n",
      "644:\tlearn: 0.3518579\ttotal: 19.7s\tremaining: 10.9s\n",
      "646:\tlearn: 0.3515877\ttotal: 19.8s\tremaining: 10.8s\n",
      "648:\tlearn: 0.3511270\ttotal: 19.9s\tremaining: 10.7s\n",
      "650:\tlearn: 0.3508579\ttotal: 19.9s\tremaining: 10.7s\n",
      "652:\tlearn: 0.3504470\ttotal: 20s\tremaining: 10.6s\n",
      "654:\tlearn: 0.3501057\ttotal: 20s\tremaining: 10.6s\n",
      "656:\tlearn: 0.3497781\ttotal: 20.1s\tremaining: 10.5s\n",
      "658:\tlearn: 0.3494030\ttotal: 20.2s\tremaining: 10.4s\n",
      "660:\tlearn: 0.3488993\ttotal: 20.2s\tremaining: 10.4s\n",
      "662:\tlearn: 0.3486408\ttotal: 20.3s\tremaining: 10.3s\n",
      "664:\tlearn: 0.3484003\ttotal: 20.4s\tremaining: 10.3s\n",
      "666:\tlearn: 0.3482469\ttotal: 20.4s\tremaining: 10.2s\n",
      "668:\tlearn: 0.3479963\ttotal: 20.5s\tremaining: 10.1s\n",
      "670:\tlearn: 0.3476297\ttotal: 20.5s\tremaining: 10.1s\n",
      "672:\tlearn: 0.3473852\ttotal: 20.6s\tremaining: 10s\n",
      "674:\tlearn: 0.3471667\ttotal: 20.7s\tremaining: 9.96s\n",
      "676:\tlearn: 0.3469911\ttotal: 20.8s\tremaining: 9.91s\n",
      "678:\tlearn: 0.3465237\ttotal: 20.8s\tremaining: 9.85s\n",
      "680:\tlearn: 0.3462553\ttotal: 20.9s\tremaining: 9.79s\n",
      "682:\tlearn: 0.3460968\ttotal: 21s\tremaining: 9.73s\n",
      "684:\tlearn: 0.3457154\ttotal: 21s\tremaining: 9.67s\n",
      "686:\tlearn: 0.3454106\ttotal: 21.1s\tremaining: 9.6s\n",
      "688:\tlearn: 0.3451698\ttotal: 21.1s\tremaining: 9.54s\n",
      "690:\tlearn: 0.3447502\ttotal: 21.2s\tremaining: 9.47s\n",
      "692:\tlearn: 0.3443901\ttotal: 21.2s\tremaining: 9.41s\n",
      "694:\tlearn: 0.3438683\ttotal: 21.3s\tremaining: 9.35s\n",
      "696:\tlearn: 0.3434332\ttotal: 21.4s\tremaining: 9.28s\n",
      "698:\tlearn: 0.3429431\ttotal: 21.4s\tremaining: 9.22s\n",
      "700:\tlearn: 0.3427000\ttotal: 21.5s\tremaining: 9.15s\n",
      "702:\tlearn: 0.3423849\ttotal: 21.5s\tremaining: 9.09s\n",
      "704:\tlearn: 0.3421505\ttotal: 21.6s\tremaining: 9.03s\n",
      "706:\tlearn: 0.3418099\ttotal: 21.6s\tremaining: 8.96s\n",
      "708:\tlearn: 0.3415161\ttotal: 21.7s\tremaining: 8.9s\n",
      "710:\tlearn: 0.3411619\ttotal: 21.7s\tremaining: 8.84s\n",
      "712:\tlearn: 0.3409123\ttotal: 21.8s\tremaining: 8.77s\n",
      "714:\tlearn: 0.3408138\ttotal: 21.9s\tremaining: 8.71s\n",
      "716:\tlearn: 0.3404167\ttotal: 21.9s\tremaining: 8.65s\n",
      "718:\tlearn: 0.3402298\ttotal: 22s\tremaining: 8.59s\n",
      "720:\tlearn: 0.3397341\ttotal: 22s\tremaining: 8.53s\n",
      "722:\tlearn: 0.3394655\ttotal: 22.1s\tremaining: 8.46s\n",
      "724:\tlearn: 0.3391736\ttotal: 22.1s\tremaining: 8.4s\n",
      "726:\tlearn: 0.3386640\ttotal: 22.2s\tremaining: 8.34s\n",
      "728:\tlearn: 0.3382375\ttotal: 22.3s\tremaining: 8.28s\n",
      "730:\tlearn: 0.3379758\ttotal: 22.3s\tremaining: 8.21s\n",
      "732:\tlearn: 0.3374752\ttotal: 22.4s\tremaining: 8.15s\n",
      "734:\tlearn: 0.3370763\ttotal: 22.4s\tremaining: 8.09s\n",
      "736:\tlearn: 0.3366733\ttotal: 22.5s\tremaining: 8.03s\n",
      "738:\tlearn: 0.3362495\ttotal: 22.5s\tremaining: 7.96s\n",
      "740:\tlearn: 0.3359716\ttotal: 22.6s\tremaining: 7.9s\n",
      "742:\tlearn: 0.3354968\ttotal: 22.7s\tremaining: 7.83s\n",
      "744:\tlearn: 0.3350625\ttotal: 22.7s\tremaining: 7.77s\n",
      "746:\tlearn: 0.3346145\ttotal: 22.8s\tremaining: 7.71s\n",
      "748:\tlearn: 0.3342860\ttotal: 22.8s\tremaining: 7.65s\n",
      "750:\tlearn: 0.3340251\ttotal: 22.9s\tremaining: 7.58s\n",
      "752:\tlearn: 0.3337560\ttotal: 22.9s\tremaining: 7.52s\n",
      "754:\tlearn: 0.3333828\ttotal: 23s\tremaining: 7.46s\n",
      "756:\tlearn: 0.3331433\ttotal: 23s\tremaining: 7.39s\n",
      "758:\tlearn: 0.3328314\ttotal: 23.1s\tremaining: 7.33s\n",
      "760:\tlearn: 0.3324525\ttotal: 23.1s\tremaining: 7.27s\n",
      "762:\tlearn: 0.3322781\ttotal: 23.2s\tremaining: 7.21s\n",
      "764:\tlearn: 0.3320158\ttotal: 23.3s\tremaining: 7.14s\n",
      "766:\tlearn: 0.3316536\ttotal: 23.3s\tremaining: 7.08s\n",
      "768:\tlearn: 0.3312867\ttotal: 23.4s\tremaining: 7.02s\n",
      "770:\tlearn: 0.3309984\ttotal: 23.4s\tremaining: 6.96s\n",
      "772:\tlearn: 0.3307490\ttotal: 23.5s\tremaining: 6.89s\n",
      "774:\tlearn: 0.3305333\ttotal: 23.5s\tremaining: 6.83s\n",
      "776:\tlearn: 0.3303691\ttotal: 23.6s\tremaining: 6.77s\n",
      "778:\tlearn: 0.3300660\ttotal: 23.6s\tremaining: 6.71s\n",
      "780:\tlearn: 0.3296366\ttotal: 23.7s\tremaining: 6.64s\n",
      "782:\tlearn: 0.3294057\ttotal: 23.7s\tremaining: 6.58s\n",
      "784:\tlearn: 0.3289954\ttotal: 23.8s\tremaining: 6.52s\n",
      "786:\tlearn: 0.3288543\ttotal: 23.9s\tremaining: 6.46s\n",
      "788:\tlearn: 0.3287044\ttotal: 23.9s\tremaining: 6.39s\n",
      "790:\tlearn: 0.3285646\ttotal: 24s\tremaining: 6.33s\n",
      "792:\tlearn: 0.3283838\ttotal: 24s\tremaining: 6.27s\n",
      "794:\tlearn: 0.3280697\ttotal: 24.1s\tremaining: 6.21s\n",
      "796:\tlearn: 0.3278221\ttotal: 24.1s\tremaining: 6.15s\n",
      "798:\tlearn: 0.3276822\ttotal: 24.2s\tremaining: 6.09s\n",
      "800:\tlearn: 0.3273198\ttotal: 24.2s\tremaining: 6.02s\n",
      "802:\tlearn: 0.3270389\ttotal: 24.3s\tremaining: 5.96s\n",
      "804:\tlearn: 0.3267418\ttotal: 24.4s\tremaining: 5.9s\n",
      "806:\tlearn: 0.3265515\ttotal: 24.4s\tremaining: 5.84s\n",
      "808:\tlearn: 0.3263685\ttotal: 24.5s\tremaining: 5.78s\n",
      "810:\tlearn: 0.3261813\ttotal: 24.5s\tremaining: 5.71s\n",
      "812:\tlearn: 0.3257763\ttotal: 24.6s\tremaining: 5.65s\n",
      "814:\tlearn: 0.3256334\ttotal: 24.6s\tremaining: 5.59s\n",
      "816:\tlearn: 0.3254445\ttotal: 24.7s\tremaining: 5.53s\n",
      "818:\tlearn: 0.3252329\ttotal: 24.7s\tremaining: 5.46s\n",
      "820:\tlearn: 0.3248349\ttotal: 24.8s\tremaining: 5.4s\n",
      "822:\tlearn: 0.3244727\ttotal: 24.8s\tremaining: 5.34s\n",
      "824:\tlearn: 0.3241912\ttotal: 24.9s\tremaining: 5.28s\n",
      "826:\tlearn: 0.3240040\ttotal: 24.9s\tremaining: 5.22s\n",
      "828:\tlearn: 0.3237545\ttotal: 25s\tremaining: 5.16s\n",
      "830:\tlearn: 0.3233957\ttotal: 25.1s\tremaining: 5.1s\n",
      "832:\tlearn: 0.3230901\ttotal: 25.1s\tremaining: 5.04s\n",
      "834:\tlearn: 0.3229134\ttotal: 25.2s\tremaining: 4.97s\n",
      "836:\tlearn: 0.3225476\ttotal: 25.2s\tremaining: 4.92s\n",
      "838:\tlearn: 0.3224080\ttotal: 25.3s\tremaining: 4.85s\n",
      "840:\tlearn: 0.3221582\ttotal: 25.4s\tremaining: 4.79s\n",
      "842:\tlearn: 0.3219105\ttotal: 25.4s\tremaining: 4.73s\n",
      "844:\tlearn: 0.3217749\ttotal: 25.5s\tremaining: 4.67s\n",
      "846:\tlearn: 0.3215544\ttotal: 25.5s\tremaining: 4.61s\n",
      "848:\tlearn: 0.3212434\ttotal: 25.6s\tremaining: 4.55s\n",
      "850:\tlearn: 0.3211240\ttotal: 25.6s\tremaining: 4.49s\n",
      "852:\tlearn: 0.3209626\ttotal: 25.7s\tremaining: 4.43s\n",
      "854:\tlearn: 0.3208040\ttotal: 25.8s\tremaining: 4.37s\n",
      "856:\tlearn: 0.3205860\ttotal: 25.8s\tremaining: 4.3s\n",
      "858:\tlearn: 0.3204660\ttotal: 25.9s\tremaining: 4.24s\n",
      "860:\tlearn: 0.3201073\ttotal: 25.9s\tremaining: 4.18s\n",
      "862:\tlearn: 0.3197144\ttotal: 26s\tremaining: 4.12s\n",
      "864:\tlearn: 0.3195719\ttotal: 26s\tremaining: 4.06s\n",
      "866:\tlearn: 0.3192735\ttotal: 26.1s\tremaining: 4s\n",
      "868:\tlearn: 0.3189937\ttotal: 26.1s\tremaining: 3.94s\n",
      "870:\tlearn: 0.3186807\ttotal: 26.2s\tremaining: 3.88s\n",
      "872:\tlearn: 0.3184403\ttotal: 26.2s\tremaining: 3.82s\n",
      "874:\tlearn: 0.3183091\ttotal: 26.3s\tremaining: 3.76s\n",
      "876:\tlearn: 0.3180569\ttotal: 26.4s\tremaining: 3.7s\n",
      "878:\tlearn: 0.3178082\ttotal: 26.4s\tremaining: 3.63s\n",
      "880:\tlearn: 0.3176445\ttotal: 26.5s\tremaining: 3.57s\n",
      "882:\tlearn: 0.3173337\ttotal: 26.5s\tremaining: 3.51s\n",
      "884:\tlearn: 0.3168144\ttotal: 26.6s\tremaining: 3.45s\n",
      "886:\tlearn: 0.3165425\ttotal: 26.6s\tremaining: 3.39s\n",
      "888:\tlearn: 0.3162400\ttotal: 26.7s\tremaining: 3.33s\n",
      "890:\tlearn: 0.3159711\ttotal: 26.7s\tremaining: 3.27s\n",
      "892:\tlearn: 0.3157707\ttotal: 26.8s\tremaining: 3.21s\n",
      "894:\tlearn: 0.3154206\ttotal: 26.9s\tremaining: 3.15s\n",
      "896:\tlearn: 0.3151370\ttotal: 26.9s\tremaining: 3.09s\n",
      "898:\tlearn: 0.3147828\ttotal: 27s\tremaining: 3.03s\n",
      "900:\tlearn: 0.3144623\ttotal: 27s\tremaining: 2.97s\n",
      "902:\tlearn: 0.3143042\ttotal: 27.1s\tremaining: 2.91s\n",
      "904:\tlearn: 0.3138485\ttotal: 27.1s\tremaining: 2.85s\n",
      "906:\tlearn: 0.3135833\ttotal: 27.2s\tremaining: 2.79s\n",
      "908:\tlearn: 0.3132655\ttotal: 27.2s\tremaining: 2.73s\n",
      "910:\tlearn: 0.3130245\ttotal: 27.3s\tremaining: 2.67s\n",
      "912:\tlearn: 0.3128209\ttotal: 27.3s\tremaining: 2.6s\n",
      "914:\tlearn: 0.3126414\ttotal: 27.4s\tremaining: 2.54s\n",
      "916:\tlearn: 0.3123661\ttotal: 27.4s\tremaining: 2.48s\n",
      "918:\tlearn: 0.3122273\ttotal: 27.5s\tremaining: 2.42s\n",
      "920:\tlearn: 0.3120020\ttotal: 27.6s\tremaining: 2.36s\n",
      "922:\tlearn: 0.3117562\ttotal: 27.6s\tremaining: 2.3s\n",
      "924:\tlearn: 0.3115409\ttotal: 27.7s\tremaining: 2.24s\n",
      "926:\tlearn: 0.3111395\ttotal: 27.7s\tremaining: 2.18s\n",
      "928:\tlearn: 0.3109335\ttotal: 27.8s\tremaining: 2.12s\n",
      "930:\tlearn: 0.3108033\ttotal: 27.8s\tremaining: 2.06s\n",
      "932:\tlearn: 0.3105550\ttotal: 27.9s\tremaining: 2s\n",
      "934:\tlearn: 0.3102954\ttotal: 27.9s\tremaining: 1.94s\n",
      "936:\tlearn: 0.3101348\ttotal: 28s\tremaining: 1.88s\n",
      "938:\tlearn: 0.3097618\ttotal: 28s\tremaining: 1.82s\n",
      "940:\tlearn: 0.3095503\ttotal: 28.1s\tremaining: 1.76s\n",
      "942:\tlearn: 0.3093470\ttotal: 28.2s\tremaining: 1.7s\n",
      "944:\tlearn: 0.3090294\ttotal: 28.2s\tremaining: 1.64s\n",
      "946:\tlearn: 0.3087356\ttotal: 28.3s\tremaining: 1.58s\n",
      "948:\tlearn: 0.3085229\ttotal: 28.3s\tremaining: 1.52s\n",
      "950:\tlearn: 0.3082437\ttotal: 28.4s\tremaining: 1.46s\n",
      "952:\tlearn: 0.3080954\ttotal: 28.4s\tremaining: 1.4s\n",
      "954:\tlearn: 0.3077562\ttotal: 28.5s\tremaining: 1.34s\n",
      "956:\tlearn: 0.3074492\ttotal: 28.6s\tremaining: 1.28s\n",
      "958:\tlearn: 0.3071613\ttotal: 28.6s\tremaining: 1.22s\n",
      "960:\tlearn: 0.3070497\ttotal: 28.7s\tremaining: 1.16s\n",
      "962:\tlearn: 0.3068210\ttotal: 28.7s\tremaining: 1.1s\n",
      "964:\tlearn: 0.3067027\ttotal: 28.8s\tremaining: 1.04s\n",
      "966:\tlearn: 0.3065770\ttotal: 28.9s\tremaining: 985ms\n",
      "968:\tlearn: 0.3061756\ttotal: 28.9s\tremaining: 925ms\n",
      "970:\tlearn: 0.3059542\ttotal: 29s\tremaining: 865ms\n",
      "972:\tlearn: 0.3057851\ttotal: 29s\tremaining: 805ms\n",
      "974:\tlearn: 0.3053679\ttotal: 29.1s\tremaining: 745ms\n",
      "976:\tlearn: 0.3052162\ttotal: 29.1s\tremaining: 686ms\n",
      "978:\tlearn: 0.3049764\ttotal: 29.2s\tremaining: 626ms\n",
      "980:\tlearn: 0.3046473\ttotal: 29.2s\tremaining: 566ms\n",
      "982:\tlearn: 0.3043887\ttotal: 29.3s\tremaining: 506ms\n",
      "984:\tlearn: 0.3041407\ttotal: 29.3s\tremaining: 447ms\n",
      "986:\tlearn: 0.3038280\ttotal: 29.4s\tremaining: 387ms\n",
      "988:\tlearn: 0.3034946\ttotal: 29.4s\tremaining: 327ms\n",
      "990:\tlearn: 0.3031034\ttotal: 29.5s\tremaining: 268ms\n",
      "992:\tlearn: 0.3029913\ttotal: 29.5s\tremaining: 208ms\n",
      "994:\tlearn: 0.3028926\ttotal: 29.6s\tremaining: 149ms\n",
      "996:\tlearn: 0.3026011\ttotal: 29.6s\tremaining: 89.2ms\n",
      "998:\tlearn: 0.3024184\ttotal: 29.7s\tremaining: 29.7ms\n",
      "999:\tlearn: 0.3023564\ttotal: 29.7s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "catboost_classifier_full = CatBoostClassifier(iterations=1000, depth=4, learning_rate=0.1, verbose=2)\n",
    "catboost_classifier_full.fit(X_full_train, y_full_train)\n",
    "\n",
    "y_full_pred = catboost_classifier_full.predict(X_full_test)\n",
    "\n",
    "catboost_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hugging_face_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 22:38:21.599877: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-01 22:38:21.629607: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-01 22:38:21.639531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 22:38:21.665732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 22:38:23.377331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/transformers/training_args.py:1574: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f28660eaf504d59a454c3443550c4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 66\u001b[0m\n\u001b[1;32m     53\u001b[0m bert_valid_dataset_full \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: bert_valid_encodings_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: bert_valid_encodings_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: bert_y_valid_full\n\u001b[1;32m     57\u001b[0m })\n\u001b[1;32m     59\u001b[0m bert_trainer_full \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     60\u001b[0m     model\u001b[38;5;241m=\u001b[39mbert_model_full,\n\u001b[1;32m     61\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     62\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mbert_train_dataset_full,\n\u001b[1;32m     63\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mbert_valid_dataset_full\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[43mbert_trainer_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m bert_predictions_full \u001b[38;5;241m=\u001b[39m bert_trainer_full\u001b[38;5;241m.\u001b[39mpredict(bert_valid_dataset_full)\n\u001b[1;32m     69\u001b[0m bert_logits_full \u001b[38;5;241m=\u001b[39m bert_predictions_full\u001b[38;5;241m.\u001b[39mpredictions\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/trainer.py:2474\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2480\u001b[0m ):\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3572\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3578\u001b[0m ):\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/trainer.py:3625\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3623\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3624\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3625\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3626\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1668\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1668\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1680\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1682\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:524\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    516\u001b[0m         hidden_states,\n\u001b[1;32m    517\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m         output_attentions,\n\u001b[1;32m    523\u001b[0m     )\n\u001b[0;32m--> 524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:467\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "bert_tokenizer_full = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model_full = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "bert_encodings_full = bert_tokenizer_full(list(df_train_full['text']), truncation=True, padding=True, max_length=256)\n",
    "bert_labels_full = torch.tensor(list(df_train_full['target']))\n",
    "\n",
    "bert_input_ids_train_full, bert_input_ids_valid_full, bert_token_type_ids_train_full, bert_token_type_ids_valid_full, \\\n",
    "bert_attention_mask_train_full, bert_attention_mask_valid_full, bert_y_train_full, bert_y_valid_full = train_test_split(\n",
    "    bert_encodings_full['input_ids'], \n",
    "    bert_encodings_full['token_type_ids'], \n",
    "    bert_encodings_full['attention_mask'], \n",
    "    bert_labels_full, \n",
    "    test_size=0.15, \n",
    "    stratify=bert_labels_full, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "bert_train_encodings_full = {\n",
    "    'input_ids': torch.tensor(bert_input_ids_train_full),\n",
    "    'token_type_ids': torch.tensor(bert_token_type_ids_train_full),\n",
    "    'attention_mask': torch.tensor(bert_attention_mask_train_full)\n",
    "}\n",
    "\n",
    "bert_valid_encodings_full = {\n",
    "    'input_ids': torch.tensor(bert_input_ids_valid_full),\n",
    "    'token_type_ids': torch.tensor(bert_token_type_ids_valid_full),\n",
    "    'attention_mask': torch.tensor(bert_attention_mask_valid_full)\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True \n",
    ")\n",
    "\n",
    "bert_train_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": bert_train_encodings_full['input_ids'],\n",
    "    \"attention_mask\": bert_train_encodings_full['attention_mask'],\n",
    "    \"labels\": bert_y_train_full\n",
    "})\n",
    "\n",
    "bert_valid_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": bert_valid_encodings_full['input_ids'],\n",
    "    \"attention_mask\": bert_valid_encodings_full['attention_mask'],\n",
    "    \"labels\": bert_y_valid_full\n",
    "})\n",
    "\n",
    "bert_trainer_full = Trainer(\n",
    "    model=bert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=bert_train_dataset_full,\n",
    "    eval_dataset=bert_valid_dataset_full\n",
    ")\n",
    "\n",
    "bert_trainer_full.train()\n",
    "\n",
    "bert_predictions_full = bert_trainer_full.predict(bert_valid_dataset_full)\n",
    "bert_logits_full = bert_predictions_full.predictions\n",
    "bert_y_pred_full = np.argmax(bert_logits_full, axis=1)\n",
    "\n",
    "bert_trainer_full_assessement = evaluate_classifier(bert_y_valid_full.numpy(), bert_y_pred_full)\n",
    "\n",
    "bert_trainer_full.save_model(\"disastertweets_bert_model\")\n",
    "bert_trainer_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_bert_model\")\n",
    "\n",
    "bert_test_encodings_full = bert_tokenizer_full(list(df_test_full['text']), truncation=True, padding=True, max_length=256)\n",
    "bert_test_encodings_full = {key: torch.tensor(val) for key, val in bert_test_encodings_full.items()}\n",
    "bert_test_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": bert_test_encodings_full['input_ids'],\n",
    "    \"attention_mask\": bert_test_encodings_full['attention_mask']\n",
    "})\n",
    "\n",
    "bert_test_predictions_full = bert_trainer_full.predict(bert_test_dataset_full)\n",
    "bert_test_logits_full = bert_test_predictions_full.predictions\n",
    "bert_test_y_pred_full = np.argmax(bert_test_logits_full, axis=1)\n",
    "\n",
    "bert_test_submission_full = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': bert_test_y_pred_full.flatten()\n",
    "})\n",
    "\n",
    "bert_trainer_full.save_pretrained(\"kaggle_disastertweets_bert_trainer\")\n",
    "bert_tokenizer_full.save_pretrained(\"kaggle_disastertweets_bert_tokenizer\")\n",
    "\n",
    "hf_bert_test_submission_full = Dataset.from_pandas(bert_test_submission_full)\n",
    "hf_bert_test_submission_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_bert_submission_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipywidgets) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
     ]
    }
   ],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/transformers/training_args.py:1574: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d1d1965b3640ceb360814ef023cda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7026, 'grad_norm': 3.5670106410980225, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 0.6921, 'grad_norm': 1.994215726852417, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 0.6435, 'grad_norm': 4.213106155395508, 'learning_rate': 3e-06, 'epoch': 0.02}\n",
      "{'loss': 0.7004, 'grad_norm': 3.5397398471832275, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6915, 'grad_norm': 2.5893571376800537, 'learning_rate': 5e-06, 'epoch': 0.03}\n",
      "{'loss': 0.657, 'grad_norm': 2.7244741916656494, 'learning_rate': 6e-06, 'epoch': 0.04}\n",
      "{'loss': 0.6811, 'grad_norm': 3.9375064373016357, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 0.656, 'grad_norm': 2.504549980163574, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 0.6243, 'grad_norm': 3.951978921890259, 'learning_rate': 9e-06, 'epoch': 0.06}\n",
      "{'loss': 0.6251, 'grad_norm': 5.363071441650391, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5678, 'grad_norm': 4.704944133758545, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5501, 'grad_norm': 8.372086524963379, 'learning_rate': 1.2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5197, 'grad_norm': 3.7211408615112305, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.4649, 'grad_norm': 6.9544243812561035, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.4853, 'grad_norm': 9.666234016418457, 'learning_rate': 1.5e-05, 'epoch': 0.09}\n",
      "{'loss': 0.4371, 'grad_norm': 3.9508330821990967, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.1}\n",
      "{'loss': 0.4414, 'grad_norm': 16.764955520629883, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5788, 'grad_norm': 8.655075073242188, 'learning_rate': 1.8e-05, 'epoch': 0.11}\n",
      "{'loss': 0.4445, 'grad_norm': 3.7206337451934814, 'learning_rate': 1.9e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5972, 'grad_norm': 9.187714576721191, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.47, 'grad_norm': 6.444240093231201, 'learning_rate': 2.1e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4584, 'grad_norm': 37.67231369018555, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.6117, 'grad_norm': 9.92576789855957, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4405, 'grad_norm': 28.50066375732422, 'learning_rate': 2.4e-05, 'epoch': 0.15}\n",
      "{'loss': 0.3033, 'grad_norm': 19.149791717529297, 'learning_rate': 2.5e-05, 'epoch': 0.15}\n",
      "{'loss': 0.9093, 'grad_norm': 1.167273998260498, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 0.563, 'grad_norm': 4.768909454345703, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 0.526, 'grad_norm': 17.029176712036133, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 0.6716, 'grad_norm': 6.285086631774902, 'learning_rate': 2.9e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2981, 'grad_norm': 32.34112548828125, 'learning_rate': 3e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5741, 'grad_norm': 14.909221649169922, 'learning_rate': 3.1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3511, 'grad_norm': 5.379212379455566, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4235, 'grad_norm': 0.7199530005455017, 'learning_rate': 3.3e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4861, 'grad_norm': 0.7135953903198242, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3785, 'grad_norm': 26.93695831298828, 'learning_rate': 3.5e-05, 'epoch': 0.22}\n",
      "{'loss': 0.6887, 'grad_norm': 6.536654949188232, 'learning_rate': 3.6e-05, 'epoch': 0.22}\n",
      "{'loss': 0.8305, 'grad_norm': 27.85598373413086, 'learning_rate': 3.7e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5575, 'grad_norm': 8.938628196716309, 'learning_rate': 3.8e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5906, 'grad_norm': 6.709065914154053, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4705, 'grad_norm': 9.611557960510254, 'learning_rate': 4e-05, 'epoch': 0.25}\n",
      "{'loss': 0.5584, 'grad_norm': 13.15637493133545, 'learning_rate': 4.1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.6531, 'grad_norm': 8.334380149841309, 'learning_rate': 4.2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5685, 'grad_norm': 10.849437713623047, 'learning_rate': 4.3e-05, 'epoch': 0.27}\n",
      "{'loss': 0.346, 'grad_norm': 1.6612293720245361, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.27}\n",
      "{'loss': 0.6352, 'grad_norm': 45.43898391723633, 'learning_rate': 4.5e-05, 'epoch': 0.28}\n",
      "{'loss': 0.81, 'grad_norm': 17.271770477294922, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.8239, 'grad_norm': 8.439245223999023, 'learning_rate': 4.7e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5419, 'grad_norm': 7.5578837394714355, 'learning_rate': 4.8e-05, 'epoch': 0.3}\n",
      "{'loss': 0.648, 'grad_norm': 35.5512809753418, 'learning_rate': 4.9e-05, 'epoch': 0.3}\n",
      "{'loss': 0.655, 'grad_norm': 10.09285831451416, 'learning_rate': 5e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5574, 'grad_norm': 32.60334777832031, 'learning_rate': 4.988516306844282e-05, 'epoch': 0.32}\n",
      "{'loss': 0.59, 'grad_norm': 5.841433048248291, 'learning_rate': 4.9770326136885625e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3556, 'grad_norm': 5.101006984710693, 'learning_rate': 4.965548920532844e-05, 'epoch': 0.33}\n",
      "{'loss': 0.7822, 'grad_norm': 6.384467601776123, 'learning_rate': 4.954065227377125e-05, 'epoch': 0.33}\n",
      "{'loss': 0.5089, 'grad_norm': 3.42268967628479, 'learning_rate': 4.942581534221406e-05, 'epoch': 0.34}\n",
      "{'loss': 0.5078, 'grad_norm': 20.8052978515625, 'learning_rate': 4.931097841065687e-05, 'epoch': 0.35}\n",
      "{'loss': 0.51, 'grad_norm': 11.91041374206543, 'learning_rate': 4.9196141479099684e-05, 'epoch': 0.35}\n",
      "{'loss': 0.6253, 'grad_norm': 6.61528205871582, 'learning_rate': 4.908130454754249e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7284, 'grad_norm': 11.961146354675293, 'learning_rate': 4.89664676159853e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4054, 'grad_norm': 6.137106418609619, 'learning_rate': 4.8851630684428114e-05, 'epoch': 0.37}\n",
      "{'loss': 0.427, 'grad_norm': 0.8357782959938049, 'learning_rate': 4.873679375287092e-05, 'epoch': 0.38}\n",
      "{'loss': 0.7683, 'grad_norm': 12.255596160888672, 'learning_rate': 4.8621956821313736e-05, 'epoch': 0.38}\n",
      "{'loss': 0.4892, 'grad_norm': 4.291518211364746, 'learning_rate': 4.8507119889756544e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3, 'grad_norm': 0.4821838140487671, 'learning_rate': 4.839228295819936e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7249, 'grad_norm': 52.15385055541992, 'learning_rate': 4.827744602664217e-05, 'epoch': 0.4}\n",
      "{'loss': 1.001, 'grad_norm': 1.0689712762832642, 'learning_rate': 4.816260909508498e-05, 'epoch': 0.41}\n",
      "{'loss': 0.7255, 'grad_norm': 13.420166015625, 'learning_rate': 4.8047772163527796e-05, 'epoch': 0.41}\n",
      "{'loss': 0.8677, 'grad_norm': 35.563743591308594, 'learning_rate': 4.7932935231970603e-05, 'epoch': 0.42}\n",
      "{'loss': 0.325, 'grad_norm': 104.67382049560547, 'learning_rate': 4.781809830041342e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5345, 'grad_norm': 3.7283318042755127, 'learning_rate': 4.7703261368856226e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5317, 'grad_norm': 3.7153842449188232, 'learning_rate': 4.758842443729904e-05, 'epoch': 0.44}\n",
      "{'loss': 0.7927, 'grad_norm': 3.550306797027588, 'learning_rate': 4.747358750574185e-05, 'epoch': 0.44}\n",
      "{'loss': 0.6179, 'grad_norm': 2.976581573486328, 'learning_rate': 4.735875057418466e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3668, 'grad_norm': 1.8515663146972656, 'learning_rate': 4.724391364262747e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3361, 'grad_norm': 0.7556580305099487, 'learning_rate': 4.712907671107028e-05, 'epoch': 0.46}\n",
      "{'loss': 1.0675, 'grad_norm': 10.420924186706543, 'learning_rate': 4.701423977951309e-05, 'epoch': 0.47}\n",
      "{'loss': 0.7071, 'grad_norm': 2.48254656791687, 'learning_rate': 4.68994028479559e-05, 'epoch': 0.48}\n",
      "{'loss': 0.5366, 'grad_norm': 5.2942376136779785, 'learning_rate': 4.6784565916398715e-05, 'epoch': 0.48}\n",
      "{'loss': 0.8038, 'grad_norm': 26.574153900146484, 'learning_rate': 4.666972898484153e-05, 'epoch': 0.49}\n",
      "{'loss': 0.5686, 'grad_norm': 1.740252137184143, 'learning_rate': 4.655489205328434e-05, 'epoch': 0.49}\n",
      "{'loss': 0.4829, 'grad_norm': 5.412481784820557, 'learning_rate': 4.644005512172715e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6396, 'grad_norm': 26.424213409423828, 'learning_rate': 4.632521819016996e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5158, 'grad_norm': 1.968256950378418, 'learning_rate': 4.6210381258612774e-05, 'epoch': 0.51}\n",
      "{'loss': 0.4738, 'grad_norm': 3.5740835666656494, 'learning_rate': 4.609554432705558e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4381, 'grad_norm': 0.7943377494812012, 'learning_rate': 4.59807073954984e-05, 'epoch': 0.53}\n",
      "{'loss': 0.4294, 'grad_norm': 5.1284379959106445, 'learning_rate': 4.5865870463941204e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6132, 'grad_norm': 15.262413024902344, 'learning_rate': 4.575103353238402e-05, 'epoch': 0.54}\n",
      "{'loss': 0.561, 'grad_norm': 24.147172927856445, 'learning_rate': 4.5636196600826834e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7129, 'grad_norm': 17.370071411132812, 'learning_rate': 4.552135966926964e-05, 'epoch': 0.55}\n",
      "{'loss': 0.626, 'grad_norm': 0.4935477077960968, 'learning_rate': 4.540652273771245e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5811, 'grad_norm': 38.164939880371094, 'learning_rate': 4.529168580615526e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2873, 'grad_norm': 5.070781230926514, 'learning_rate': 4.517684887459807e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5098, 'grad_norm': 6.975024700164795, 'learning_rate': 4.5062011943040886e-05, 'epoch': 0.57}\n",
      "{'loss': 0.76, 'grad_norm': 7.048118591308594, 'learning_rate': 4.4947175011483694e-05, 'epoch': 0.58}\n",
      "{'loss': 0.866, 'grad_norm': 53.343631744384766, 'learning_rate': 4.483233807992651e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7499, 'grad_norm': 5.4210591316223145, 'learning_rate': 4.4717501148369316e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6031, 'grad_norm': 24.212505340576172, 'learning_rate': 4.460266421681213e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7761, 'grad_norm': 4.504744529724121, 'learning_rate': 4.448782728525494e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5686, 'grad_norm': 18.32889747619629, 'learning_rate': 4.437299035369775e-05, 'epoch': 0.61}\n",
      "{'loss': 0.4037, 'grad_norm': 1.4594203233718872, 'learning_rate': 4.425815342214056e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3795, 'grad_norm': 0.47722670435905457, 'learning_rate': 4.4143316490583375e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5622, 'grad_norm': 23.218292236328125, 'learning_rate': 4.402847955902619e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4222, 'grad_norm': 30.71406364440918, 'learning_rate': 4.3913642627469e-05, 'epoch': 0.64}\n",
      "{'loss': 0.8565, 'grad_norm': 63.85649490356445, 'learning_rate': 4.379880569591181e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5829, 'grad_norm': 54.18203353881836, 'learning_rate': 4.368396876435461e-05, 'epoch': 0.65}\n",
      "{'loss': 0.411, 'grad_norm': 3.305633306503296, 'learning_rate': 4.356913183279743e-05, 'epoch': 0.66}\n",
      "{'loss': 0.71, 'grad_norm': 14.323213577270508, 'learning_rate': 4.345429490124024e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5608, 'grad_norm': 0.3578566610813141, 'learning_rate': 4.333945796968305e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7173, 'grad_norm': 52.60603713989258, 'learning_rate': 4.3224621038125865e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6618, 'grad_norm': 5.327340126037598, 'learning_rate': 4.310978410656867e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6783, 'grad_norm': 5.005570888519287, 'learning_rate': 4.299494717501149e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4865, 'grad_norm': 1.0070890188217163, 'learning_rate': 4.2880110243454295e-05, 'epoch': 0.69}\n",
      "{'loss': 0.432, 'grad_norm': 0.7563148140907288, 'learning_rate': 4.276527331189711e-05, 'epoch': 0.7}\n",
      "{'loss': 0.4229, 'grad_norm': 0.7651091814041138, 'learning_rate': 4.265043638033992e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5178, 'grad_norm': 0.37398508191108704, 'learning_rate': 4.253559944878273e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2846, 'grad_norm': 4.361455917358398, 'learning_rate': 4.2420762517225546e-05, 'epoch': 0.72}\n",
      "{'loss': 0.4637, 'grad_norm': 63.19210433959961, 'learning_rate': 4.2305925585668354e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8081, 'grad_norm': 5.526137351989746, 'learning_rate': 4.219108865411117e-05, 'epoch': 0.73}\n",
      "{'loss': 0.806, 'grad_norm': 12.162820816040039, 'learning_rate': 4.2076251722553976e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5932, 'grad_norm': 5.016473293304443, 'learning_rate': 4.196141479099679e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5173, 'grad_norm': 1.5766133069992065, 'learning_rate': 4.184657785943959e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6273, 'grad_norm': 5.896426677703857, 'learning_rate': 4.1731740927882406e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3349, 'grad_norm': 14.91759204864502, 'learning_rate': 4.161690399632522e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2591, 'grad_norm': 0.6062202453613281, 'learning_rate': 4.150206706476803e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4447, 'grad_norm': 68.53394317626953, 'learning_rate': 4.138723013321084e-05, 'epoch': 0.77}\n",
      "{'loss': 0.397, 'grad_norm': 0.200619176030159, 'learning_rate': 4.127239320165365e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4346, 'grad_norm': 9.113361358642578, 'learning_rate': 4.1157556270096466e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4014, 'grad_norm': 333.20452880859375, 'learning_rate': 4.1042719338539273e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7615, 'grad_norm': 5.516716003417969, 'learning_rate': 4.092788240698209e-05, 'epoch': 0.8}\n",
      "{'loss': 0.8116, 'grad_norm': 40.216453552246094, 'learning_rate': 4.08130454754249e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5607, 'grad_norm': 8.775863647460938, 'learning_rate': 4.069820854386771e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3403, 'grad_norm': 0.6430354118347168, 'learning_rate': 4.0583371612310525e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7102, 'grad_norm': 4.809781074523926, 'learning_rate': 4.046853468075333e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8462, 'grad_norm': 3.6179869174957275, 'learning_rate': 4.035369774919615e-05, 'epoch': 0.83}\n",
      "{'loss': 0.4976, 'grad_norm': 6.851798057556152, 'learning_rate': 4.0238860817638955e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3282, 'grad_norm': 8.611151695251465, 'learning_rate': 4.012402388608177e-05, 'epoch': 0.84}\n",
      "{'loss': 0.592, 'grad_norm': 3.5719082355499268, 'learning_rate': 4.000918695452458e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5028, 'grad_norm': 5.337527751922607, 'learning_rate': 3.9894350022967385e-05, 'epoch': 0.85}\n",
      "{'loss': 0.2612, 'grad_norm': 14.217957496643066, 'learning_rate': 3.97795130914102e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5172, 'grad_norm': 4.562532424926758, 'learning_rate': 3.966467615985301e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5233, 'grad_norm': 0.4337611198425293, 'learning_rate': 3.954983922829582e-05, 'epoch': 0.87}\n",
      "{'loss': 0.4861, 'grad_norm': 8.754817962646484, 'learning_rate': 3.943500229673863e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6007, 'grad_norm': 90.12278747558594, 'learning_rate': 3.9320165365181444e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3649, 'grad_norm': 0.714046835899353, 'learning_rate': 3.920532843362425e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6846, 'grad_norm': 165.2958526611328, 'learning_rate': 3.909049150206707e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5639, 'grad_norm': 0.7907235622406006, 'learning_rate': 3.897565457050988e-05, 'epoch': 0.9}\n",
      "{'loss': 0.4564, 'grad_norm': 18.14560890197754, 'learning_rate': 3.886081763895269e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4574, 'grad_norm': 0.7998684644699097, 'learning_rate': 3.8745980707395504e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3698, 'grad_norm': 7.570950031280518, 'learning_rate': 3.863114377583831e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4853, 'grad_norm': 6.211344242095947, 'learning_rate': 3.8516306844281126e-05, 'epoch': 0.93}\n",
      "{'loss': 1.0157, 'grad_norm': 11.148828506469727, 'learning_rate': 3.8401469912723934e-05, 'epoch': 0.93}\n",
      "{'loss': 0.331, 'grad_norm': 42.11668395996094, 'learning_rate': 3.828663298116674e-05, 'epoch': 0.94}\n",
      "{'loss': 0.696, 'grad_norm': 3.1930954456329346, 'learning_rate': 3.8171796049609556e-05, 'epoch': 0.95}\n",
      "{'loss': 0.492, 'grad_norm': 27.308008193969727, 'learning_rate': 3.8056959118052364e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5346, 'grad_norm': 12.604668617248535, 'learning_rate': 3.794212218649518e-05, 'epoch': 0.96}\n",
      "{'loss': 0.4895, 'grad_norm': 0.9344724416732788, 'learning_rate': 3.7827285254937986e-05, 'epoch': 0.96}\n",
      "{'loss': 0.7022, 'grad_norm': 12.223705291748047, 'learning_rate': 3.77124483233808e-05, 'epoch': 0.97}\n",
      "{'loss': 0.1857, 'grad_norm': 0.39219170808792114, 'learning_rate': 3.759761139182361e-05, 'epoch': 0.98}\n",
      "{'loss': 0.7904, 'grad_norm': 4.122222423553467, 'learning_rate': 3.748277446026642e-05, 'epoch': 0.98}\n",
      "{'loss': 0.7688, 'grad_norm': 23.37590789794922, 'learning_rate': 3.736793752870924e-05, 'epoch': 0.99}\n",
      "{'loss': 0.7125, 'grad_norm': 20.93402099609375, 'learning_rate': 3.7253100597152045e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc52d67e0c047c78e509d8435daab54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4842619001865387, 'eval_runtime': 53.463, 'eval_samples_per_second': 21.361, 'eval_steps_per_second': 5.349, 'epoch': 1.0}\n",
      "{'loss': 0.3562, 'grad_norm': 10.87424373626709, 'learning_rate': 3.713826366559486e-05, 'epoch': 1.0}\n",
      "{'loss': 0.4723, 'grad_norm': 29.80830192565918, 'learning_rate': 3.702342673403767e-05, 'epoch': 1.01}\n",
      "{'loss': 0.6169, 'grad_norm': 1.1386029720306396, 'learning_rate': 3.690858980248048e-05, 'epoch': 1.01}\n",
      "{'loss': 0.4432, 'grad_norm': 0.7023907899856567, 'learning_rate': 3.679375287092329e-05, 'epoch': 1.02}\n",
      "{'loss': 0.447, 'grad_norm': 4.798180103302002, 'learning_rate': 3.6678915939366105e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3589, 'grad_norm': 10.056122779846191, 'learning_rate': 3.656407900780892e-05, 'epoch': 1.03}\n",
      "{'loss': 0.9171, 'grad_norm': 36.724205017089844, 'learning_rate': 3.644924207625172e-05, 'epoch': 1.04}\n",
      "{'loss': 0.5244, 'grad_norm': 7.010728359222412, 'learning_rate': 3.6334405144694535e-05, 'epoch': 1.04}\n",
      "{'loss': 0.4184, 'grad_norm': 5.629269599914551, 'learning_rate': 3.621956821313734e-05, 'epoch': 1.05}\n",
      "{'loss': 0.9282, 'grad_norm': 13.860461235046387, 'learning_rate': 3.610473128158016e-05, 'epoch': 1.06}\n",
      "{'loss': 0.3758, 'grad_norm': 30.015687942504883, 'learning_rate': 3.5989894350022965e-05, 'epoch': 1.06}\n",
      "{'loss': 0.6284, 'grad_norm': 1.092551827430725, 'learning_rate': 3.587505741846578e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2268, 'grad_norm': 0.32922446727752686, 'learning_rate': 3.5760220486908594e-05, 'epoch': 1.08}\n",
      "{'loss': 0.2836, 'grad_norm': 0.3697349429130554, 'learning_rate': 3.56453835553514e-05, 'epoch': 1.08}\n",
      "{'loss': 0.2706, 'grad_norm': 20.517080307006836, 'learning_rate': 3.5530546623794216e-05, 'epoch': 1.09}\n",
      "{'loss': 0.4662, 'grad_norm': 13.691488265991211, 'learning_rate': 3.5415709692237024e-05, 'epoch': 1.09}\n",
      "{'loss': 0.4194, 'grad_norm': 4.811746120452881, 'learning_rate': 3.530087276067984e-05, 'epoch': 1.1}\n",
      "{'loss': 0.4753, 'grad_norm': 2.668562889099121, 'learning_rate': 3.5186035829122646e-05, 'epoch': 1.11}\n",
      "{'loss': 0.676, 'grad_norm': 4.746697425842285, 'learning_rate': 3.507119889756546e-05, 'epoch': 1.11}\n",
      "{'loss': 0.5674, 'grad_norm': 5.822161674499512, 'learning_rate': 3.495636196600827e-05, 'epoch': 1.12}\n",
      "{'loss': 0.4414, 'grad_norm': 4.323966979980469, 'learning_rate': 3.484152503445108e-05, 'epoch': 1.12}\n",
      "{'loss': 0.2692, 'grad_norm': 5.237429141998291, 'learning_rate': 3.47266881028939e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2872, 'grad_norm': 0.4397863745689392, 'learning_rate': 3.46118511713367e-05, 'epoch': 1.14}\n",
      "{'loss': 0.6409, 'grad_norm': 0.3559364676475525, 'learning_rate': 3.449701423977951e-05, 'epoch': 1.14}\n",
      "{'loss': 0.2611, 'grad_norm': 7.84084415435791, 'learning_rate': 3.438217730822232e-05, 'epoch': 1.15}\n",
      "{'loss': 0.5969, 'grad_norm': 5.297881603240967, 'learning_rate': 3.4267340376665136e-05, 'epoch': 1.16}\n",
      "{'loss': 0.5555, 'grad_norm': 9.422525405883789, 'learning_rate': 3.415250344510795e-05, 'epoch': 1.16}\n",
      "{'loss': 0.4246, 'grad_norm': 0.5477539300918579, 'learning_rate': 3.403766651355076e-05, 'epoch': 1.17}\n",
      "{'loss': 0.4866, 'grad_norm': 1.8288496732711792, 'learning_rate': 3.392282958199357e-05, 'epoch': 1.17}\n",
      "{'loss': 0.6013, 'grad_norm': 11.022430419921875, 'learning_rate': 3.380799265043638e-05, 'epoch': 1.18}\n",
      "{'loss': 0.4133, 'grad_norm': 2.292264699935913, 'learning_rate': 3.3693155718879195e-05, 'epoch': 1.19}\n",
      "{'loss': 0.1434, 'grad_norm': 7.243320465087891, 'learning_rate': 3.3578318787322e-05, 'epoch': 1.19}\n",
      "{'loss': 0.5664, 'grad_norm': 11.144972801208496, 'learning_rate': 3.346348185576482e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4718, 'grad_norm': 4.192474842071533, 'learning_rate': 3.3348644924207625e-05, 'epoch': 1.21}\n",
      "{'loss': 0.2855, 'grad_norm': 0.49861714243888855, 'learning_rate': 3.323380799265044e-05, 'epoch': 1.21}\n",
      "{'loss': 0.4639, 'grad_norm': 0.43588465452194214, 'learning_rate': 3.3118971061093254e-05, 'epoch': 1.22}\n",
      "{'loss': 0.4702, 'grad_norm': 4.724614143371582, 'learning_rate': 3.300413412953606e-05, 'epoch': 1.22}\n",
      "{'loss': 0.297, 'grad_norm': 5.623672962188721, 'learning_rate': 3.288929719797887e-05, 'epoch': 1.23}\n",
      "{'loss': 0.5816, 'grad_norm': 0.5165952444076538, 'learning_rate': 3.277446026642168e-05, 'epoch': 1.24}\n",
      "{'loss': 0.5135, 'grad_norm': 65.03108215332031, 'learning_rate': 3.265962333486449e-05, 'epoch': 1.24}\n",
      "{'loss': 0.5534, 'grad_norm': 10.501544952392578, 'learning_rate': 3.2544786403307307e-05, 'epoch': 1.25}\n",
      "{'loss': 0.3439, 'grad_norm': 60.682437896728516, 'learning_rate': 3.2429949471750114e-05, 'epoch': 1.25}\n",
      "{'loss': 0.4645, 'grad_norm': 43.35062789916992, 'learning_rate': 3.231511254019293e-05, 'epoch': 1.26}\n",
      "{'loss': 0.5013, 'grad_norm': 0.32892102003097534, 'learning_rate': 3.220027560863574e-05, 'epoch': 1.27}\n",
      "{'loss': 0.4006, 'grad_norm': 0.2789168953895569, 'learning_rate': 3.208543867707855e-05, 'epoch': 1.27}\n",
      "{'loss': 0.4816, 'grad_norm': 0.7315773367881775, 'learning_rate': 3.197060174552136e-05, 'epoch': 1.28}\n",
      "{'loss': 0.526, 'grad_norm': 8.898871421813965, 'learning_rate': 3.1855764813964174e-05, 'epoch': 1.29}\n",
      "{'loss': 0.7712, 'grad_norm': 8.109396934509277, 'learning_rate': 3.174092788240698e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3876, 'grad_norm': 1.1062043905258179, 'learning_rate': 3.1626090950849796e-05, 'epoch': 1.3}\n",
      "{'loss': 0.1933, 'grad_norm': 4.57364559173584, 'learning_rate': 3.151125401929261e-05, 'epoch': 1.3}\n",
      "{'loss': 0.178, 'grad_norm': 0.46392685174942017, 'learning_rate': 3.139641708773542e-05, 'epoch': 1.31}\n",
      "{'loss': 0.4332, 'grad_norm': 0.25777947902679443, 'learning_rate': 3.128158015617823e-05, 'epoch': 1.32}\n",
      "{'loss': 0.9795, 'grad_norm': 78.65079498291016, 'learning_rate': 3.116674322462104e-05, 'epoch': 1.32}\n",
      "{'loss': 0.5743, 'grad_norm': 79.15377044677734, 'learning_rate': 3.105190629306385e-05, 'epoch': 1.33}\n",
      "{'loss': 0.4987, 'grad_norm': 77.1655044555664, 'learning_rate': 3.093706936150666e-05, 'epoch': 1.33}\n",
      "{'loss': 0.4089, 'grad_norm': 0.4704669713973999, 'learning_rate': 3.082223242994947e-05, 'epoch': 1.34}\n",
      "{'loss': 0.3206, 'grad_norm': 25.0103816986084, 'learning_rate': 3.0707395498392285e-05, 'epoch': 1.35}\n",
      "{'loss': 0.2228, 'grad_norm': 0.4521302878856659, 'learning_rate': 3.059255856683509e-05, 'epoch': 1.35}\n",
      "{'loss': 0.2144, 'grad_norm': 0.3045256435871124, 'learning_rate': 3.0477721635277908e-05, 'epoch': 1.36}\n",
      "{'loss': 0.5187, 'grad_norm': 0.23174133896827698, 'learning_rate': 3.036288470372072e-05, 'epoch': 1.37}\n",
      "{'loss': 0.7556, 'grad_norm': 0.19377616047859192, 'learning_rate': 3.024804777216353e-05, 'epoch': 1.37}\n",
      "{'loss': 0.6023, 'grad_norm': 4.752946853637695, 'learning_rate': 3.013321084060634e-05, 'epoch': 1.38}\n",
      "{'loss': 0.3647, 'grad_norm': 4.974104404449463, 'learning_rate': 3.0018373909049152e-05, 'epoch': 1.38}\n",
      "{'loss': 0.459, 'grad_norm': 12.94384479522705, 'learning_rate': 2.9903536977491963e-05, 'epoch': 1.39}\n",
      "{'loss': 0.4916, 'grad_norm': 75.07469940185547, 'learning_rate': 2.9788700045934775e-05, 'epoch': 1.4}\n",
      "{'loss': 0.7082, 'grad_norm': 14.094118118286133, 'learning_rate': 2.9673863114377586e-05, 'epoch': 1.4}\n",
      "{'loss': 0.6474, 'grad_norm': 17.03914451599121, 'learning_rate': 2.9559026182820397e-05, 'epoch': 1.41}\n",
      "{'loss': 0.2488, 'grad_norm': 0.4308687448501587, 'learning_rate': 2.944418925126321e-05, 'epoch': 1.42}\n",
      "{'loss': 0.66, 'grad_norm': 0.5464397668838501, 'learning_rate': 2.9329352319706023e-05, 'epoch': 1.42}\n",
      "{'loss': 0.265, 'grad_norm': 0.4990885853767395, 'learning_rate': 2.9214515388148827e-05, 'epoch': 1.43}\n",
      "{'loss': 0.5381, 'grad_norm': 0.45339086651802063, 'learning_rate': 2.9099678456591638e-05, 'epoch': 1.43}\n",
      "{'loss': 0.397, 'grad_norm': 5.0870771408081055, 'learning_rate': 2.898484152503445e-05, 'epoch': 1.44}\n",
      "{'loss': 0.2015, 'grad_norm': 0.49769327044487, 'learning_rate': 2.8870004593477264e-05, 'epoch': 1.45}\n",
      "{'loss': 0.3993, 'grad_norm': 0.270734578371048, 'learning_rate': 2.8755167661920075e-05, 'epoch': 1.45}\n",
      "{'loss': 0.6587, 'grad_norm': 27.1279354095459, 'learning_rate': 2.8640330730362886e-05, 'epoch': 1.46}\n",
      "{'loss': 0.4315, 'grad_norm': 22.847793579101562, 'learning_rate': 2.8525493798805697e-05, 'epoch': 1.46}\n",
      "{'loss': 0.6572, 'grad_norm': 6.844101428985596, 'learning_rate': 2.841065686724851e-05, 'epoch': 1.47}\n",
      "{'loss': 0.482, 'grad_norm': 17.461040496826172, 'learning_rate': 2.829581993569132e-05, 'epoch': 1.48}\n",
      "{'loss': 0.7132, 'grad_norm': 0.6105679273605347, 'learning_rate': 2.818098300413413e-05, 'epoch': 1.48}\n",
      "{'loss': 0.3564, 'grad_norm': 5.091015815734863, 'learning_rate': 2.8066146072576942e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3034, 'grad_norm': 8.183510780334473, 'learning_rate': 2.7951309141019753e-05, 'epoch': 1.5}\n",
      "{'loss': 0.1922, 'grad_norm': 0.18787527084350586, 'learning_rate': 2.7836472209462568e-05, 'epoch': 1.5}\n",
      "{'loss': 0.5345, 'grad_norm': 4.9076690673828125, 'learning_rate': 2.772163527790538e-05, 'epoch': 1.51}\n",
      "{'loss': 0.1203, 'grad_norm': 1.6232634782791138, 'learning_rate': 2.760679834634819e-05, 'epoch': 1.51}\n",
      "{'loss': 0.352, 'grad_norm': 5.185641765594482, 'learning_rate': 2.7491961414790994e-05, 'epoch': 1.52}\n",
      "{'loss': 0.4633, 'grad_norm': 0.36217355728149414, 'learning_rate': 2.7377124483233806e-05, 'epoch': 1.53}\n",
      "{'loss': 0.6683, 'grad_norm': 13.348642349243164, 'learning_rate': 2.726228755167662e-05, 'epoch': 1.53}\n",
      "{'loss': 0.4123, 'grad_norm': 0.9551035761833191, 'learning_rate': 2.714745062011943e-05, 'epoch': 1.54}\n",
      "{'loss': 0.4784, 'grad_norm': 0.4153752326965332, 'learning_rate': 2.7032613688562243e-05, 'epoch': 1.55}\n",
      "{'loss': 0.5874, 'grad_norm': 5.207117557525635, 'learning_rate': 2.6917776757005054e-05, 'epoch': 1.55}\n",
      "{'loss': 0.3036, 'grad_norm': 0.37162479758262634, 'learning_rate': 2.6802939825447865e-05, 'epoch': 1.56}\n",
      "{'loss': 0.4315, 'grad_norm': 10.143495559692383, 'learning_rate': 2.6688102893890676e-05, 'epoch': 1.56}\n",
      "{'loss': 0.6511, 'grad_norm': 0.3057239055633545, 'learning_rate': 2.6573265962333487e-05, 'epoch': 1.57}\n",
      "{'loss': 0.7713, 'grad_norm': 24.20215606689453, 'learning_rate': 2.64584290307763e-05, 'epoch': 1.58}\n",
      "{'loss': 0.3696, 'grad_norm': 16.94559097290039, 'learning_rate': 2.634359209921911e-05, 'epoch': 1.58}\n",
      "{'loss': 0.2451, 'grad_norm': 0.36054927110671997, 'learning_rate': 2.6228755167661924e-05, 'epoch': 1.59}\n",
      "{'loss': 0.4326, 'grad_norm': 8.259593963623047, 'learning_rate': 2.6113918236104735e-05, 'epoch': 1.59}\n",
      "{'loss': 0.4618, 'grad_norm': 0.2530471980571747, 'learning_rate': 2.5999081304547546e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2844, 'grad_norm': 0.858771800994873, 'learning_rate': 2.5884244372990358e-05, 'epoch': 1.61}\n",
      "{'loss': 0.2829, 'grad_norm': 1.1330914497375488, 'learning_rate': 2.576940744143317e-05, 'epoch': 1.61}\n",
      "{'loss': 0.6725, 'grad_norm': 6.2465715408325195, 'learning_rate': 2.5654570509875973e-05, 'epoch': 1.62}\n",
      "{'loss': 0.1373, 'grad_norm': 1.840787649154663, 'learning_rate': 2.5539733578318788e-05, 'epoch': 1.63}\n",
      "{'loss': 0.4308, 'grad_norm': 48.6442985534668, 'learning_rate': 2.54248966467616e-05, 'epoch': 1.63}\n",
      "{'loss': 0.4224, 'grad_norm': 0.201826810836792, 'learning_rate': 2.531005971520441e-05, 'epoch': 1.64}\n",
      "{'loss': 0.2114, 'grad_norm': 0.26054856181144714, 'learning_rate': 2.519522278364722e-05, 'epoch': 1.64}\n",
      "{'loss': 0.1275, 'grad_norm': 0.1313231736421585, 'learning_rate': 2.5080385852090032e-05, 'epoch': 1.65}\n",
      "{'loss': 0.6305, 'grad_norm': 6.9520463943481445, 'learning_rate': 2.4965548920532844e-05, 'epoch': 1.66}\n",
      "{'loss': 0.6146, 'grad_norm': 0.32661154866218567, 'learning_rate': 2.4850711988975655e-05, 'epoch': 1.66}\n",
      "{'loss': 0.3499, 'grad_norm': 149.9110565185547, 'learning_rate': 2.4735875057418466e-05, 'epoch': 1.67}\n",
      "{'loss': 0.7385, 'grad_norm': 1.713085651397705, 'learning_rate': 2.462103812586128e-05, 'epoch': 1.67}\n",
      "{'loss': 0.3319, 'grad_norm': 0.3961580693721771, 'learning_rate': 2.450620119430409e-05, 'epoch': 1.68}\n",
      "{'loss': 0.4756, 'grad_norm': 35.22323989868164, 'learning_rate': 2.4391364262746903e-05, 'epoch': 1.69}\n",
      "{'loss': 0.556, 'grad_norm': 4.9891676902771, 'learning_rate': 2.427652733118971e-05, 'epoch': 1.69}\n",
      "{'loss': 0.3244, 'grad_norm': 0.7285727858543396, 'learning_rate': 2.4161690399632522e-05, 'epoch': 1.7}\n",
      "{'loss': 0.7746, 'grad_norm': 6.909202575683594, 'learning_rate': 2.4046853468075333e-05, 'epoch': 1.71}\n",
      "{'loss': 0.3568, 'grad_norm': 0.5471294522285461, 'learning_rate': 2.3932016536518144e-05, 'epoch': 1.71}\n",
      "{'loss': 0.4929, 'grad_norm': 0.46096736192703247, 'learning_rate': 2.381717960496096e-05, 'epoch': 1.72}\n",
      "{'loss': 0.5865, 'grad_norm': 24.498374938964844, 'learning_rate': 2.370234267340377e-05, 'epoch': 1.72}\n",
      "{'loss': 0.2693, 'grad_norm': 6.944360733032227, 'learning_rate': 2.358750574184658e-05, 'epoch': 1.73}\n",
      "{'loss': 0.2939, 'grad_norm': 0.34419330954551697, 'learning_rate': 2.3472668810289392e-05, 'epoch': 1.74}\n",
      "{'loss': 0.3944, 'grad_norm': 6.0341949462890625, 'learning_rate': 2.33578318787322e-05, 'epoch': 1.74}\n",
      "{'loss': 0.2635, 'grad_norm': 34.139625549316406, 'learning_rate': 2.324299494717501e-05, 'epoch': 1.75}\n",
      "{'loss': 0.6497, 'grad_norm': 7.817192077636719, 'learning_rate': 2.3128158015617822e-05, 'epoch': 1.76}\n",
      "{'loss': 0.666, 'grad_norm': 5.09325647354126, 'learning_rate': 2.3013321084060633e-05, 'epoch': 1.76}\n",
      "{'loss': 0.4796, 'grad_norm': 6.837093353271484, 'learning_rate': 2.2898484152503448e-05, 'epoch': 1.77}\n",
      "{'loss': 0.3136, 'grad_norm': 8.208651542663574, 'learning_rate': 2.278364722094626e-05, 'epoch': 1.77}\n",
      "{'loss': 0.1834, 'grad_norm': 28.695781707763672, 'learning_rate': 2.266881028938907e-05, 'epoch': 1.78}\n",
      "{'loss': 0.4765, 'grad_norm': 4.738690376281738, 'learning_rate': 2.2553973357831878e-05, 'epoch': 1.79}\n",
      "{'loss': 0.2563, 'grad_norm': 0.23182250559329987, 'learning_rate': 2.243913642627469e-05, 'epoch': 1.79}\n",
      "{'loss': 0.4568, 'grad_norm': 35.53349685668945, 'learning_rate': 2.23242994947175e-05, 'epoch': 1.8}\n",
      "{'loss': 0.6736, 'grad_norm': 110.32799530029297, 'learning_rate': 2.220946256316031e-05, 'epoch': 1.8}\n",
      "{'loss': 0.5386, 'grad_norm': 12.0220365524292, 'learning_rate': 2.2094625631603126e-05, 'epoch': 1.81}\n",
      "{'loss': 0.1343, 'grad_norm': 0.24727250635623932, 'learning_rate': 2.1979788700045937e-05, 'epoch': 1.82}\n",
      "{'loss': 0.6129, 'grad_norm': 0.404653936624527, 'learning_rate': 2.186495176848875e-05, 'epoch': 1.82}\n",
      "{'loss': 0.4105, 'grad_norm': 49.574668884277344, 'learning_rate': 2.175011483693156e-05, 'epoch': 1.83}\n",
      "{'loss': 0.6421, 'grad_norm': 10.91152572631836, 'learning_rate': 2.1635277905374367e-05, 'epoch': 1.84}\n",
      "{'loss': 0.229, 'grad_norm': 1.4296396970748901, 'learning_rate': 2.152044097381718e-05, 'epoch': 1.84}\n",
      "{'loss': 0.6116, 'grad_norm': 0.28723713755607605, 'learning_rate': 2.140560404225999e-05, 'epoch': 1.85}\n",
      "{'loss': 0.6062, 'grad_norm': 5.22382116317749, 'learning_rate': 2.1290767110702804e-05, 'epoch': 1.85}\n",
      "{'loss': 0.4209, 'grad_norm': 5.527688980102539, 'learning_rate': 2.1175930179145615e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0752, 'grad_norm': 1.1377754211425781, 'learning_rate': 2.1061093247588427e-05, 'epoch': 1.87}\n",
      "{'loss': 0.3529, 'grad_norm': 17.921403884887695, 'learning_rate': 2.0946256316031238e-05, 'epoch': 1.87}\n",
      "{'loss': 0.2639, 'grad_norm': 0.17246656119823456, 'learning_rate': 2.083141938447405e-05, 'epoch': 1.88}\n",
      "{'loss': 0.5418, 'grad_norm': 12.102102279663086, 'learning_rate': 2.0716582452916857e-05, 'epoch': 1.89}\n",
      "{'loss': 0.3532, 'grad_norm': 0.28023412823677063, 'learning_rate': 2.0601745521359668e-05, 'epoch': 1.89}\n",
      "{'loss': 0.2944, 'grad_norm': 0.5746502876281738, 'learning_rate': 2.0486908589802482e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0962, 'grad_norm': 0.31464600563049316, 'learning_rate': 2.0372071658245294e-05, 'epoch': 1.9}\n",
      "{'loss': 0.6131, 'grad_norm': 0.23597452044487, 'learning_rate': 2.0257234726688105e-05, 'epoch': 1.91}\n",
      "{'loss': 0.3837, 'grad_norm': 42.40318298339844, 'learning_rate': 2.0142397795130916e-05, 'epoch': 1.92}\n",
      "{'loss': 0.5104, 'grad_norm': 11.589066505432129, 'learning_rate': 2.0027560863573727e-05, 'epoch': 1.92}\n",
      "{'loss': 0.732, 'grad_norm': 0.2555161714553833, 'learning_rate': 1.9912723932016538e-05, 'epoch': 1.93}\n",
      "{'loss': 0.7864, 'grad_norm': 19.36669921875, 'learning_rate': 1.9797887000459346e-05, 'epoch': 1.93}\n",
      "{'loss': 0.3086, 'grad_norm': 0.6024945378303528, 'learning_rate': 1.968305006890216e-05, 'epoch': 1.94}\n",
      "{'loss': 0.3915, 'grad_norm': 5.517807960510254, 'learning_rate': 1.9568213137344972e-05, 'epoch': 1.95}\n",
      "{'loss': 0.2751, 'grad_norm': 0.19403068721294403, 'learning_rate': 1.9453376205787783e-05, 'epoch': 1.95}\n",
      "{'loss': 0.5717, 'grad_norm': 4.643619537353516, 'learning_rate': 1.9338539274230594e-05, 'epoch': 1.96}\n",
      "{'loss': 0.43, 'grad_norm': 13.31860065460205, 'learning_rate': 1.9223702342673405e-05, 'epoch': 1.97}\n",
      "{'loss': 0.302, 'grad_norm': 0.41985172033309937, 'learning_rate': 1.9108865411116216e-05, 'epoch': 1.97}\n",
      "{'loss': 0.1488, 'grad_norm': 0.5254518985748291, 'learning_rate': 1.8994028479559028e-05, 'epoch': 1.98}\n",
      "{'loss': 0.7329, 'grad_norm': 0.25817254185676575, 'learning_rate': 1.887919154800184e-05, 'epoch': 1.98}\n",
      "{'loss': 0.4326, 'grad_norm': 7.519045352935791, 'learning_rate': 1.876435461644465e-05, 'epoch': 1.99}\n",
      "{'loss': 0.6997, 'grad_norm': 4.440229415893555, 'learning_rate': 1.864951768488746e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453207d6fe984c14b3e2a5856ed0e143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5593322515487671, 'eval_runtime': 51.032, 'eval_samples_per_second': 22.378, 'eval_steps_per_second': 5.604, 'epoch': 2.0}\n",
      "{'loss': 0.4539, 'grad_norm': 10.26856517791748, 'learning_rate': 1.8534680753330272e-05, 'epoch': 2.0}\n",
      "{'loss': 0.3353, 'grad_norm': 12.857574462890625, 'learning_rate': 1.8419843821773083e-05, 'epoch': 2.01}\n",
      "{'loss': 0.3823, 'grad_norm': 0.723122775554657, 'learning_rate': 1.8305006890215895e-05, 'epoch': 2.01}\n",
      "{'loss': 0.2373, 'grad_norm': 0.30110883712768555, 'learning_rate': 1.8190169958658706e-05, 'epoch': 2.02}\n",
      "{'loss': 0.3145, 'grad_norm': 0.21213671565055847, 'learning_rate': 1.8075333027101517e-05, 'epoch': 2.03}\n",
      "{'loss': 0.4964, 'grad_norm': 0.2984541654586792, 'learning_rate': 1.7960496095544328e-05, 'epoch': 2.03}\n",
      "{'loss': 0.3382, 'grad_norm': 7.802870273590088, 'learning_rate': 1.784565916398714e-05, 'epoch': 2.04}\n",
      "{'loss': 0.4932, 'grad_norm': 5.152284145355225, 'learning_rate': 1.773082223242995e-05, 'epoch': 2.05}\n",
      "{'loss': 0.2384, 'grad_norm': 1.1488378047943115, 'learning_rate': 1.761598530087276e-05, 'epoch': 2.05}\n",
      "{'loss': 0.4016, 'grad_norm': 0.9870135188102722, 'learning_rate': 1.7501148369315573e-05, 'epoch': 2.06}\n",
      "{'loss': 0.4135, 'grad_norm': 0.5237569808959961, 'learning_rate': 1.7386311437758384e-05, 'epoch': 2.06}\n",
      "{'loss': 0.2252, 'grad_norm': 0.38772422075271606, 'learning_rate': 1.7271474506201195e-05, 'epoch': 2.07}\n",
      "{'loss': 0.1985, 'grad_norm': 0.2539727985858917, 'learning_rate': 1.7156637574644006e-05, 'epoch': 2.08}\n",
      "{'loss': 0.2854, 'grad_norm': 0.10770692676305771, 'learning_rate': 1.7041800643086817e-05, 'epoch': 2.08}\n",
      "{'loss': 0.2061, 'grad_norm': 0.14744633436203003, 'learning_rate': 1.692696371152963e-05, 'epoch': 2.09}\n",
      "{'loss': 0.3119, 'grad_norm': 26.187332153320312, 'learning_rate': 1.681212677997244e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1185, 'grad_norm': 0.1489853709936142, 'learning_rate': 1.669728984841525e-05, 'epoch': 2.1}\n",
      "{'loss': 0.2882, 'grad_norm': 11.76550006866455, 'learning_rate': 1.6582452916858062e-05, 'epoch': 2.11}\n",
      "{'loss': 0.343, 'grad_norm': 9.362370491027832, 'learning_rate': 1.6467615985300873e-05, 'epoch': 2.11}\n",
      "{'loss': 0.4028, 'grad_norm': 0.312688946723938, 'learning_rate': 1.6352779053743684e-05, 'epoch': 2.12}\n",
      "{'loss': 0.1315, 'grad_norm': 0.07795011252164841, 'learning_rate': 1.6237942122186496e-05, 'epoch': 2.13}\n",
      "{'loss': 0.3193, 'grad_norm': 0.06214664503931999, 'learning_rate': 1.6123105190629307e-05, 'epoch': 2.13}\n",
      "{'loss': 0.1338, 'grad_norm': 0.2929031252861023, 'learning_rate': 1.6008268259072118e-05, 'epoch': 2.14}\n",
      "{'loss': 0.6128, 'grad_norm': 14.374372482299805, 'learning_rate': 1.589343132751493e-05, 'epoch': 2.14}\n",
      "{'loss': 0.3677, 'grad_norm': 0.19874589145183563, 'learning_rate': 1.577859439595774e-05, 'epoch': 2.15}\n",
      "{'loss': 0.1256, 'grad_norm': 0.3329567313194275, 'learning_rate': 1.566375746440055e-05, 'epoch': 2.16}\n",
      "{'loss': 0.4086, 'grad_norm': 8.513900756835938, 'learning_rate': 1.5548920532843363e-05, 'epoch': 2.16}\n",
      "{'loss': 0.2778, 'grad_norm': 0.34709811210632324, 'learning_rate': 1.5434083601286177e-05, 'epoch': 2.17}\n",
      "{'loss': 0.2415, 'grad_norm': 0.6257111430168152, 'learning_rate': 1.5319246669728985e-05, 'epoch': 2.18}\n",
      "{'loss': 0.2197, 'grad_norm': 14.422706604003906, 'learning_rate': 1.5204409738171796e-05, 'epoch': 2.18}\n",
      "{'loss': 0.218, 'grad_norm': 0.33817237615585327, 'learning_rate': 1.5089572806614607e-05, 'epoch': 2.19}\n",
      "{'loss': 0.2701, 'grad_norm': 0.21030603349208832, 'learning_rate': 1.4974735875057418e-05, 'epoch': 2.19}\n",
      "{'loss': 0.1598, 'grad_norm': 0.18308910727500916, 'learning_rate': 1.4859898943500231e-05, 'epoch': 2.2}\n",
      "{'loss': 0.576, 'grad_norm': 0.3103858232498169, 'learning_rate': 1.4745062011943042e-05, 'epoch': 2.21}\n",
      "{'loss': 0.2942, 'grad_norm': 0.9020191431045532, 'learning_rate': 1.4630225080385854e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0158, 'grad_norm': 0.17746928334236145, 'learning_rate': 1.4515388148828665e-05, 'epoch': 2.22}\n",
      "{'loss': 0.3149, 'grad_norm': 0.31454724073410034, 'learning_rate': 1.4400551217271474e-05, 'epoch': 2.22}\n",
      "{'loss': 0.2493, 'grad_norm': 0.13590262830257416, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.23}\n",
      "{'loss': 0.1513, 'grad_norm': 0.23245401680469513, 'learning_rate': 1.4170877354157097e-05, 'epoch': 2.24}\n",
      "{'loss': 0.2551, 'grad_norm': 5.461882591247559, 'learning_rate': 1.405604042259991e-05, 'epoch': 2.24}\n",
      "{'loss': 0.2896, 'grad_norm': 0.3112846612930298, 'learning_rate': 1.394120349104272e-05, 'epoch': 2.25}\n",
      "{'loss': 0.2792, 'grad_norm': 0.22086377441883087, 'learning_rate': 1.3826366559485532e-05, 'epoch': 2.26}\n",
      "{'loss': 0.2231, 'grad_norm': 0.24859106540679932, 'learning_rate': 1.3711529627928343e-05, 'epoch': 2.26}\n",
      "{'loss': 0.1058, 'grad_norm': 0.15652790665626526, 'learning_rate': 1.3596692696371154e-05, 'epoch': 2.27}\n",
      "{'loss': 0.1466, 'grad_norm': 0.11165524274110794, 'learning_rate': 1.3481855764813964e-05, 'epoch': 2.27}\n",
      "{'loss': 0.2417, 'grad_norm': 0.19468402862548828, 'learning_rate': 1.3367018833256775e-05, 'epoch': 2.28}\n",
      "{'loss': 0.4041, 'grad_norm': 0.2580265700817108, 'learning_rate': 1.3252181901699588e-05, 'epoch': 2.29}\n",
      "{'loss': 0.5732, 'grad_norm': 0.24237501621246338, 'learning_rate': 1.3137344970142399e-05, 'epoch': 2.29}\n",
      "{'loss': 0.6254, 'grad_norm': 0.1626816838979721, 'learning_rate': 1.302250803858521e-05, 'epoch': 2.3}\n",
      "{'loss': 0.3217, 'grad_norm': 0.4240697920322418, 'learning_rate': 1.2907671107028021e-05, 'epoch': 2.31}\n",
      "{'loss': 0.3907, 'grad_norm': 0.300578773021698, 'learning_rate': 1.2792834175470832e-05, 'epoch': 2.31}\n",
      "{'loss': 0.4934, 'grad_norm': 0.2099168598651886, 'learning_rate': 1.2677997243913642e-05, 'epoch': 2.32}\n",
      "{'loss': 0.4285, 'grad_norm': 20.458675384521484, 'learning_rate': 1.2563160312356453e-05, 'epoch': 2.32}\n",
      "{'loss': 0.4509, 'grad_norm': 2.563462257385254, 'learning_rate': 1.2448323380799266e-05, 'epoch': 2.33}\n",
      "{'loss': 0.1137, 'grad_norm': 0.35002025961875916, 'learning_rate': 1.2333486449242077e-05, 'epoch': 2.34}\n",
      "{'loss': 0.2453, 'grad_norm': 0.2588508129119873, 'learning_rate': 1.2218649517684888e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0125, 'grad_norm': 0.2813870310783386, 'learning_rate': 1.21038125861277e-05, 'epoch': 2.35}\n",
      "{'loss': 0.1611, 'grad_norm': 0.13834688067436218, 'learning_rate': 1.198897565457051e-05, 'epoch': 2.35}\n",
      "{'loss': 0.3225, 'grad_norm': 5.942294120788574, 'learning_rate': 1.1874138723013322e-05, 'epoch': 2.36}\n",
      "{'loss': 0.3127, 'grad_norm': 6.902495861053467, 'learning_rate': 1.1759301791456133e-05, 'epoch': 2.37}\n",
      "{'loss': 0.1207, 'grad_norm': 8.750060081481934, 'learning_rate': 1.1644464859898944e-05, 'epoch': 2.37}\n",
      "{'loss': 0.3425, 'grad_norm': 0.10644722729921341, 'learning_rate': 1.1529627928341755e-05, 'epoch': 2.38}\n",
      "{'loss': 0.4411, 'grad_norm': 0.21697165071964264, 'learning_rate': 1.1414790996784566e-05, 'epoch': 2.39}\n",
      "{'loss': 0.2227, 'grad_norm': 0.24275332689285278, 'learning_rate': 1.1299954065227377e-05, 'epoch': 2.39}\n",
      "{'loss': 0.2781, 'grad_norm': 0.23710501194000244, 'learning_rate': 1.1185117133670189e-05, 'epoch': 2.4}\n",
      "{'loss': 0.3154, 'grad_norm': 22.32412338256836, 'learning_rate': 1.1070280202113e-05, 'epoch': 2.4}\n",
      "{'loss': 0.3802, 'grad_norm': 0.1644468754529953, 'learning_rate': 1.0955443270555811e-05, 'epoch': 2.41}\n",
      "{'loss': 0.3331, 'grad_norm': 7.205866813659668, 'learning_rate': 1.0840606338998622e-05, 'epoch': 2.42}\n",
      "{'loss': 0.3324, 'grad_norm': 0.16196918487548828, 'learning_rate': 1.0725769407441435e-05, 'epoch': 2.42}\n",
      "{'loss': 0.3024, 'grad_norm': 0.33324527740478516, 'learning_rate': 1.0610932475884244e-05, 'epoch': 2.43}\n",
      "{'loss': 0.4954, 'grad_norm': 0.5060802698135376, 'learning_rate': 1.0496095544327056e-05, 'epoch': 2.44}\n",
      "{'loss': 0.3942, 'grad_norm': 0.15013445913791656, 'learning_rate': 1.0381258612769867e-05, 'epoch': 2.44}\n",
      "{'loss': 0.4212, 'grad_norm': 0.3929247558116913, 'learning_rate': 1.026642168121268e-05, 'epoch': 2.45}\n",
      "{'loss': 0.6305, 'grad_norm': 0.4105452001094818, 'learning_rate': 1.0151584749655489e-05, 'epoch': 2.45}\n",
      "{'loss': 0.1786, 'grad_norm': 0.2407800406217575, 'learning_rate': 1.00367478180983e-05, 'epoch': 2.46}\n",
      "{'loss': 0.2846, 'grad_norm': 1.1606327295303345, 'learning_rate': 9.921910886541111e-06, 'epoch': 2.47}\n",
      "{'loss': 0.1179, 'grad_norm': 26.34351921081543, 'learning_rate': 9.807073954983924e-06, 'epoch': 2.47}\n",
      "{'loss': 0.1903, 'grad_norm': 0.2925650179386139, 'learning_rate': 9.692237023426734e-06, 'epoch': 2.48}\n",
      "{'loss': 0.4209, 'grad_norm': 0.19757865369319916, 'learning_rate': 9.577400091869545e-06, 'epoch': 2.48}\n",
      "{'loss': 0.225, 'grad_norm': 11.84889030456543, 'learning_rate': 9.462563160312358e-06, 'epoch': 2.49}\n",
      "{'loss': 0.2498, 'grad_norm': 0.08944837749004364, 'learning_rate': 9.347726228755169e-06, 'epoch': 2.5}\n",
      "{'loss': 0.2962, 'grad_norm': 1.5594894886016846, 'learning_rate': 9.232889297197978e-06, 'epoch': 2.5}\n",
      "{'loss': 0.2839, 'grad_norm': 0.17106251418590546, 'learning_rate': 9.11805236564079e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0705, 'grad_norm': 0.7313989996910095, 'learning_rate': 9.003215434083602e-06, 'epoch': 2.52}\n",
      "{'loss': 0.1597, 'grad_norm': 0.07570943236351013, 'learning_rate': 8.888378502526414e-06, 'epoch': 2.52}\n",
      "{'loss': 0.4382, 'grad_norm': 8.965367317199707, 'learning_rate': 8.773541570969223e-06, 'epoch': 2.53}\n",
      "{'loss': 0.554, 'grad_norm': 0.09390872716903687, 'learning_rate': 8.658704639412036e-06, 'epoch': 2.53}\n",
      "{'loss': 0.2713, 'grad_norm': 0.10163188725709915, 'learning_rate': 8.543867707854847e-06, 'epoch': 2.54}\n",
      "{'loss': 0.3244, 'grad_norm': 0.3075111508369446, 'learning_rate': 8.429030776297658e-06, 'epoch': 2.55}\n",
      "{'loss': 0.1181, 'grad_norm': 0.11467470228672028, 'learning_rate': 8.314193844740468e-06, 'epoch': 2.55}\n",
      "{'loss': 0.5962, 'grad_norm': 8.948752403259277, 'learning_rate': 8.19935691318328e-06, 'epoch': 2.56}\n",
      "{'loss': 0.2279, 'grad_norm': 9.745403289794922, 'learning_rate': 8.084519981626092e-06, 'epoch': 2.56}\n",
      "{'loss': 0.236, 'grad_norm': 0.18554966151714325, 'learning_rate': 7.969683050068903e-06, 'epoch': 2.57}\n",
      "{'loss': 0.2987, 'grad_norm': 0.08205419033765793, 'learning_rate': 7.854846118511714e-06, 'epoch': 2.58}\n",
      "{'loss': 0.5069, 'grad_norm': 9.411602020263672, 'learning_rate': 7.740009186954525e-06, 'epoch': 2.58}\n",
      "{'loss': 0.366, 'grad_norm': 0.0688508078455925, 'learning_rate': 7.6251722553973365e-06, 'epoch': 2.59}\n",
      "{'loss': 0.3468, 'grad_norm': 0.3502040207386017, 'learning_rate': 7.510335323840148e-06, 'epoch': 2.6}\n",
      "{'loss': 0.2704, 'grad_norm': 0.04975251108407974, 'learning_rate': 7.395498392282958e-06, 'epoch': 2.6}\n",
      "{'loss': 0.2305, 'grad_norm': 8.69126033782959, 'learning_rate': 7.28066146072577e-06, 'epoch': 2.61}\n",
      "{'loss': 0.4162, 'grad_norm': 13.188796043395996, 'learning_rate': 7.165824529168581e-06, 'epoch': 2.61}\n",
      "{'loss': 0.1247, 'grad_norm': 10.838168144226074, 'learning_rate': 7.0509875976113915e-06, 'epoch': 2.62}\n",
      "{'loss': 0.3339, 'grad_norm': 0.14957228302955627, 'learning_rate': 6.936150666054203e-06, 'epoch': 2.63}\n",
      "{'loss': 0.38, 'grad_norm': 0.9162545204162598, 'learning_rate': 6.821313734497015e-06, 'epoch': 2.63}\n",
      "{'loss': 0.2862, 'grad_norm': 0.3659689426422119, 'learning_rate': 6.706476802939826e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0794, 'grad_norm': 0.3441631495952606, 'learning_rate': 6.591639871382636e-06, 'epoch': 2.65}\n",
      "{'loss': 0.314, 'grad_norm': 7.960951328277588, 'learning_rate': 6.476802939825448e-06, 'epoch': 2.65}\n",
      "{'loss': 0.3553, 'grad_norm': 8.817780494689941, 'learning_rate': 6.361966008268259e-06, 'epoch': 2.66}\n",
      "{'loss': 0.1556, 'grad_norm': 0.21540482342243195, 'learning_rate': 6.2471290767110705e-06, 'epoch': 2.66}\n",
      "{'loss': 0.2154, 'grad_norm': 9.5156888961792, 'learning_rate': 6.132292145153882e-06, 'epoch': 2.67}\n",
      "{'loss': 0.2007, 'grad_norm': 1.0966049432754517, 'learning_rate': 6.017455213596693e-06, 'epoch': 2.68}\n",
      "{'loss': 0.4551, 'grad_norm': 0.3297254741191864, 'learning_rate': 5.902618282039504e-06, 'epoch': 2.68}\n",
      "{'loss': 0.1296, 'grad_norm': 55.1816291809082, 'learning_rate': 5.787781350482315e-06, 'epoch': 2.69}\n",
      "{'loss': 0.4777, 'grad_norm': 0.07227098941802979, 'learning_rate': 5.672944418925126e-06, 'epoch': 2.69}\n",
      "{'loss': 0.2667, 'grad_norm': 0.11823321133852005, 'learning_rate': 5.5581074873679375e-06, 'epoch': 2.7}\n",
      "{'loss': 0.4496, 'grad_norm': 16.055143356323242, 'learning_rate': 5.4432705558107495e-06, 'epoch': 2.71}\n",
      "{'loss': 0.4893, 'grad_norm': 0.3405393064022064, 'learning_rate': 5.32843362425356e-06, 'epoch': 2.71}\n",
      "{'loss': 0.2177, 'grad_norm': 53.425987243652344, 'learning_rate': 5.213596692696372e-06, 'epoch': 2.72}\n",
      "{'loss': 0.3026, 'grad_norm': 8.785910606384277, 'learning_rate': 5.098759761139182e-06, 'epoch': 2.73}\n",
      "{'loss': 0.6209, 'grad_norm': 0.23193666338920593, 'learning_rate': 4.983922829581994e-06, 'epoch': 2.73}\n",
      "{'loss': 0.192, 'grad_norm': 0.1419851928949356, 'learning_rate': 4.8690858980248045e-06, 'epoch': 2.74}\n",
      "{'loss': 0.3824, 'grad_norm': 0.17969131469726562, 'learning_rate': 4.7542489664676165e-06, 'epoch': 2.74}\n",
      "{'loss': 0.3509, 'grad_norm': 0.05811915546655655, 'learning_rate': 4.639412034910427e-06, 'epoch': 2.75}\n",
      "{'loss': 0.7819, 'grad_norm': 6.88238000869751, 'learning_rate': 4.524575103353239e-06, 'epoch': 2.76}\n",
      "{'loss': 0.1728, 'grad_norm': 6.166391372680664, 'learning_rate': 4.40973817179605e-06, 'epoch': 2.76}\n",
      "{'loss': 0.2703, 'grad_norm': 6.463698387145996, 'learning_rate': 4.294901240238861e-06, 'epoch': 2.77}\n",
      "{'loss': 0.4762, 'grad_norm': 0.5899231433868408, 'learning_rate': 4.180064308681672e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0826, 'grad_norm': 0.09010054916143417, 'learning_rate': 4.0652273771244835e-06, 'epoch': 2.78}\n",
      "{'loss': 0.2959, 'grad_norm': 25.93358612060547, 'learning_rate': 3.950390445567295e-06, 'epoch': 2.79}\n",
      "{'loss': 0.5254, 'grad_norm': 47.18544006347656, 'learning_rate': 3.835553514010106e-06, 'epoch': 2.79}\n",
      "{'loss': 0.4041, 'grad_norm': 0.2742210924625397, 'learning_rate': 3.720716582452917e-06, 'epoch': 2.8}\n",
      "{'loss': 0.1099, 'grad_norm': 0.1466640830039978, 'learning_rate': 3.6058796508957286e-06, 'epoch': 2.81}\n",
      "{'loss': 0.479, 'grad_norm': 13.76397705078125, 'learning_rate': 3.4910427193385393e-06, 'epoch': 2.81}\n",
      "{'loss': 0.1335, 'grad_norm': 0.09574471414089203, 'learning_rate': 3.376205787781351e-06, 'epoch': 2.82}\n",
      "{'loss': 0.6547, 'grad_norm': 27.88300132751465, 'learning_rate': 3.2613688562241617e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0208, 'grad_norm': 113.16886138916016, 'learning_rate': 3.1465319246669733e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0096, 'grad_norm': 0.12779606878757477, 'learning_rate': 3.031694993109784e-06, 'epoch': 2.84}\n",
      "{'loss': 0.1732, 'grad_norm': 0.1715129315853119, 'learning_rate': 2.9168580615525956e-06, 'epoch': 2.84}\n",
      "{'loss': 0.2693, 'grad_norm': 0.2405693531036377, 'learning_rate': 2.8020211299954068e-06, 'epoch': 2.85}\n",
      "{'loss': 0.378, 'grad_norm': 0.31766024231910706, 'learning_rate': 2.687184198438218e-06, 'epoch': 2.86}\n",
      "{'loss': 0.3376, 'grad_norm': 12.7459135055542, 'learning_rate': 2.572347266881029e-06, 'epoch': 2.86}\n",
      "{'loss': 0.1021, 'grad_norm': 0.14233914017677307, 'learning_rate': 2.4575103353238403e-06, 'epoch': 2.87}\n",
      "{'loss': 0.3668, 'grad_norm': 7.610915660858154, 'learning_rate': 2.3426734037666514e-06, 'epoch': 2.87}\n",
      "{'loss': 0.2015, 'grad_norm': 0.17348583042621613, 'learning_rate': 2.2278364722094626e-06, 'epoch': 2.88}\n",
      "{'loss': 0.3747, 'grad_norm': 13.758037567138672, 'learning_rate': 2.1129995406522738e-06, 'epoch': 2.89}\n",
      "{'loss': 0.2067, 'grad_norm': 0.1327241063117981, 'learning_rate': 1.998162609095085e-06, 'epoch': 2.89}\n",
      "{'loss': 0.3421, 'grad_norm': 0.11162902414798737, 'learning_rate': 1.8833256775378963e-06, 'epoch': 2.9}\n",
      "{'loss': 0.2973, 'grad_norm': 26.18393325805664, 'learning_rate': 1.7684887459807075e-06, 'epoch': 2.9}\n",
      "{'loss': 0.3346, 'grad_norm': 0.20029602944850922, 'learning_rate': 1.6536518144235186e-06, 'epoch': 2.91}\n",
      "{'loss': 0.554, 'grad_norm': 44.36566925048828, 'learning_rate': 1.53881488286633e-06, 'epoch': 2.92}\n",
      "{'loss': 0.7339, 'grad_norm': 7.373260498046875, 'learning_rate': 1.4239779513091412e-06, 'epoch': 2.92}\n",
      "{'loss': 0.1751, 'grad_norm': 0.26910626888275146, 'learning_rate': 1.3091410197519524e-06, 'epoch': 2.93}\n",
      "{'loss': 0.5529, 'grad_norm': 21.004287719726562, 'learning_rate': 1.1943040881947635e-06, 'epoch': 2.94}\n",
      "{'loss': 0.3026, 'grad_norm': 33.885494232177734, 'learning_rate': 1.0794671566375747e-06, 'epoch': 2.94}\n",
      "{'loss': 0.4891, 'grad_norm': 38.874149322509766, 'learning_rate': 9.64630225080386e-07, 'epoch': 2.95}\n",
      "{'loss': 0.2836, 'grad_norm': 0.5064463019371033, 'learning_rate': 8.497932935231971e-07, 'epoch': 2.95}\n",
      "{'loss': 0.2642, 'grad_norm': 0.08864176273345947, 'learning_rate': 7.349563619660083e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0775, 'grad_norm': 2.143752098083496, 'learning_rate': 6.201194304088195e-07, 'epoch': 2.97}\n",
      "{'loss': 0.5565, 'grad_norm': 0.051542315632104874, 'learning_rate': 5.052824988516307e-07, 'epoch': 2.97}\n",
      "{'loss': 0.1385, 'grad_norm': 14.742186546325684, 'learning_rate': 3.904455672944419e-07, 'epoch': 2.98}\n",
      "{'loss': 0.3509, 'grad_norm': 0.24407120048999786, 'learning_rate': 2.756086357372531e-07, 'epoch': 2.99}\n",
      "{'loss': 0.0084, 'grad_norm': 0.06082887202501297, 'learning_rate': 1.6077170418006432e-07, 'epoch': 2.99}\n",
      "{'loss': 0.3887, 'grad_norm': 0.4495667517185211, 'learning_rate': 4.593477262287552e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4070cf71fa1d475aa93872a5b374d186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7248959541320801, 'eval_runtime': 50.9426, 'eval_samples_per_second': 22.417, 'eval_steps_per_second': 5.614, 'epoch': 3.0}\n",
      "{'train_runtime': 4909.4974, 'train_samples_per_second': 3.954, 'train_steps_per_second': 0.989, 'train_loss': 0.44253837397045004, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba5ef968680485bb6083d5ca90c48c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c76cfd526f84b179fbb5c34d99175c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafc13cb65a445c48881dcbf6ec5e8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d1a0757c3b40f5b99bf32d895036da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0127a3b985fe4d0a98475e99a98ef3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796da40e2c8b4aee836c0facbd267cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190d6687ac4b4b5f8ff5e3f88d2167a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/yanncauchepin/kaggle_disastertweets_distill_bert_submission_df/commit/e306c0819327d23bd42456978839b3ea9279dabd', commit_message='Upload dataset', commit_description='', oid='e306c0819327d23bd42456978839b3ea9279dabd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/yanncauchepin/kaggle_disastertweets_distill_bert_submission_df', endpoint='https://huggingface.co', repo_type='dataset', repo_id='yanncauchepin/kaggle_disastertweets_distill_bert_submission_df'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "distill_bert_tokenizer_full = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distill_bert_model_full = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "distill_bert_encodings_full = distill_bert_tokenizer_full(list(df_train_full['text']), truncation=True, padding=True, max_length=256)\n",
    "distill_bert_labels_full = torch.tensor(list(df_train_full['target']))\n",
    "\n",
    "distill_bert_input_ids_train_full, distill_bert_input_ids_valid_full, \\\n",
    "distill_bert_attention_mask_train_full, distill_bert_attention_mask_valid_full, distill_bert_y_train_full, distill_bert_y_valid_full = train_test_split(\n",
    "    distill_bert_encodings_full['input_ids'], \n",
    "    distill_bert_encodings_full['attention_mask'], \n",
    "    distill_bert_labels_full, \n",
    "    test_size=0.15, \n",
    "    stratify=distill_bert_labels_full, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "distill_bert_train_encodings_full = {\n",
    "    'input_ids': torch.tensor(distill_bert_input_ids_train_full),\n",
    "    'attention_mask': torch.tensor(distill_bert_attention_mask_train_full)\n",
    "}\n",
    "\n",
    "distill_bert_valid_encodings_full = {\n",
    "    'input_ids': torch.tensor(distill_bert_input_ids_valid_full),\n",
    "    'attention_mask': torch.tensor(distill_bert_attention_mask_valid_full)\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True \n",
    ")\n",
    "\n",
    "distill_bert_train_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": distill_bert_train_encodings_full['input_ids'],\n",
    "    \"attention_mask\": distill_bert_train_encodings_full['attention_mask'],\n",
    "    \"labels\": distill_bert_y_train_full\n",
    "})\n",
    "\n",
    "distill_bert_valid_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": distill_bert_valid_encodings_full['input_ids'],\n",
    "    \"attention_mask\": distill_bert_valid_encodings_full['attention_mask'],\n",
    "    \"labels\": distill_bert_y_valid_full\n",
    "})\n",
    "\n",
    "distill_bert_trainer_full = Trainer(\n",
    "    model=distill_bert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=distill_bert_train_dataset_full,\n",
    "    eval_dataset=distill_bert_valid_dataset_full\n",
    ")\n",
    "\n",
    "distill_bert_trainer_full.train()\n",
    "\n",
    "distill_bert_predictions_full = distill_bert_trainer_full.predict(distill_bert_valid_dataset_full)\n",
    "distill_bert_logits_full = distill_bert_predictions_full.predictions\n",
    "distill_bert_y_pred_full = np.argmax(distill_bert_logits_full, axis=1)\n",
    "\n",
    "distill_bert_trainer_full_assessement = evaluate_classifier(distill_bert_y_valid_full.numpy(), distill_bert_y_pred_full)\n",
    "\n",
    "distill_bert_test_encodings_full = distill_bert_tokenizer_full(list(df_test_full['text']), truncation=True, padding=True, max_length=256)\n",
    "distill_bert_test_encodings_full = {key: torch.tensor(val) for key, val in distill_bert_test_encodings_full.items()}\n",
    "distill_bert_test_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": distill_bert_test_encodings_full['input_ids'],\n",
    "    \"attention_mask\": distill_bert_test_encodings_full['attention_mask']\n",
    "})\n",
    "\n",
    "distill_bert_test_predictions_full = distill_bert_trainer_full.predict(distill_bert_test_dataset_full)\n",
    "distill_bert_test_logits_full = distill_bert_test_predictions_full.predictions\n",
    "distill_bert_test_y_pred_full = np.argmax(distill_bert_test_logits_full, axis=1)\n",
    "\n",
    "distill_bert_test_submission_full = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': distill_bert_test_y_pred_full.flatten()\n",
    "})\n",
    "\n",
    "distill_bert_trainer_full.save_pretrained(\"disastertweets_distill_bert_model\")\n",
    "distill_bert_tokenizer_full.save_pretrained(\"disastertweets_distill_bert_tokenizer\")\n",
    "\n",
    "distill_bert_trainer_full.model.push_to_hub(\"yanncauchepin/kaggle_disastertweets_distill_bert_model\")\n",
    "distill_bert_tokenizer_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_distill_bert_tokenizer\")\n",
    "\n",
    "hf_distill_bert_test_submission_full = Dataset.from_pandas(distill_bert_test_submission_full)\n",
    "hf_distill_bert_test_submission_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_distill_bert_submission_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/transformers/training_args.py:1574: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df97bf658cd84386a5d6c7d67f00fe9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7047, 'grad_norm': 4.763729572296143, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 0.6909, 'grad_norm': 2.3180086612701416, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 0.6959, 'grad_norm': 4.462715148925781, 'learning_rate': 3e-06, 'epoch': 0.02}\n",
      "{'loss': 0.693, 'grad_norm': 4.434909820556641, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6917, 'grad_norm': 3.107389211654663, 'learning_rate': 5e-06, 'epoch': 0.03}\n",
      "{'loss': 0.6905, 'grad_norm': 5.843577861785889, 'learning_rate': 6e-06, 'epoch': 0.04}\n",
      "{'loss': 0.6944, 'grad_norm': 6.763678073883057, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 0.696, 'grad_norm': 4.3050079345703125, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 0.6812, 'grad_norm': 10.309606552124023, 'learning_rate': 9e-06, 'epoch': 0.06}\n",
      "{'loss': 0.72, 'grad_norm': 4.82548713684082, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 0.6682, 'grad_norm': 1.924714207649231, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6584, 'grad_norm': 7.451861381530762, 'learning_rate': 1.2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6784, 'grad_norm': 8.313910484313965, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6823, 'grad_norm': 12.379124641418457, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5835, 'grad_norm': 13.259035110473633, 'learning_rate': 1.5e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5392, 'grad_norm': 7.364662170410156, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.1}\n",
      "{'loss': 0.4489, 'grad_norm': 29.351869583129883, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 0.6215, 'grad_norm': 12.04206371307373, 'learning_rate': 1.8e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2912, 'grad_norm': 9.139932632446289, 'learning_rate': 1.9e-05, 'epoch': 0.12}\n",
      "{'loss': 0.6888, 'grad_norm': 3.9426677227020264, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.544, 'grad_norm': 14.00932502746582, 'learning_rate': 2.1e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4966, 'grad_norm': 6.0895514488220215, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.7538, 'grad_norm': 26.578554153442383, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4935, 'grad_norm': 5.0815935134887695, 'learning_rate': 2.4e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5417, 'grad_norm': 15.093242645263672, 'learning_rate': 2.5e-05, 'epoch': 0.15}\n",
      "{'loss': 0.6269, 'grad_norm': 2.903424024581909, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6031, 'grad_norm': 2.045637369155884, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5581, 'grad_norm': 204.8267059326172, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7453, 'grad_norm': 33.16191482543945, 'learning_rate': 2.9e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4858, 'grad_norm': 8.152320861816406, 'learning_rate': 3e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7546, 'grad_norm': 1.6986420154571533, 'learning_rate': 3.1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6893, 'grad_norm': 35.01735305786133, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4393, 'grad_norm': 3.7113986015319824, 'learning_rate': 3.3e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3965, 'grad_norm': 6.184817790985107, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4905, 'grad_norm': 86.85446166992188, 'learning_rate': 3.5e-05, 'epoch': 0.22}\n",
      "{'loss': 0.8318, 'grad_norm': 9.386666297912598, 'learning_rate': 3.6e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7051, 'grad_norm': 8.668257713317871, 'learning_rate': 3.7e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5113, 'grad_norm': 3.4011969566345215, 'learning_rate': 3.8e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6094, 'grad_norm': 5.4555487632751465, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5846, 'grad_norm': 43.07642364501953, 'learning_rate': 4e-05, 'epoch': 0.25}\n",
      "{'loss': 0.5677, 'grad_norm': 9.815537452697754, 'learning_rate': 4.1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9908, 'grad_norm': 4.682296276092529, 'learning_rate': 4.2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.846, 'grad_norm': 8.862191200256348, 'learning_rate': 4.3e-05, 'epoch': 0.27}\n",
      "{'loss': 0.6651, 'grad_norm': 6.420687675476074, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.27}\n",
      "{'loss': 0.6549, 'grad_norm': 10.953056335449219, 'learning_rate': 4.5e-05, 'epoch': 0.28}\n",
      "{'loss': 0.566, 'grad_norm': 41.949790954589844, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.6408, 'grad_norm': 26.99002456665039, 'learning_rate': 4.7e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5874, 'grad_norm': 9.021445274353027, 'learning_rate': 4.8e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6421, 'grad_norm': 3.720299482345581, 'learning_rate': 4.9e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6794, 'grad_norm': 2.7502291202545166, 'learning_rate': 5e-05, 'epoch': 0.31}\n",
      "{'loss': 0.6683, 'grad_norm': 5.778182506561279, 'learning_rate': 4.988516306844282e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7026, 'grad_norm': 3.930974006652832, 'learning_rate': 4.9770326136885625e-05, 'epoch': 0.32}\n",
      "{'loss': 0.703, 'grad_norm': 4.514956951141357, 'learning_rate': 4.965548920532844e-05, 'epoch': 0.33}\n",
      "{'loss': 0.683, 'grad_norm': 1.6261942386627197, 'learning_rate': 4.954065227377125e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6769, 'grad_norm': 1.795283317565918, 'learning_rate': 4.942581534221406e-05, 'epoch': 0.34}\n",
      "{'loss': 0.6971, 'grad_norm': 2.2211647033691406, 'learning_rate': 4.931097841065687e-05, 'epoch': 0.35}\n",
      "{'loss': 0.6107, 'grad_norm': 7.113701820373535, 'learning_rate': 4.9196141479099684e-05, 'epoch': 0.35}\n",
      "{'loss': 0.9679, 'grad_norm': 6.814640522003174, 'learning_rate': 4.908130454754249e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7797, 'grad_norm': 2.755709648132324, 'learning_rate': 4.89664676159853e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7102, 'grad_norm': 3.736276626586914, 'learning_rate': 4.8851630684428114e-05, 'epoch': 0.37}\n",
      "{'loss': 0.6903, 'grad_norm': 1.5642104148864746, 'learning_rate': 4.873679375287092e-05, 'epoch': 0.38}\n",
      "{'loss': 0.6238, 'grad_norm': 2.7833423614501953, 'learning_rate': 4.8621956821313736e-05, 'epoch': 0.38}\n",
      "{'loss': 0.5645, 'grad_norm': 3.28306245803833, 'learning_rate': 4.8507119889756544e-05, 'epoch': 0.39}\n",
      "{'loss': 0.4955, 'grad_norm': 1.5386654138565063, 'learning_rate': 4.839228295819936e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7117, 'grad_norm': 5.493505954742432, 'learning_rate': 4.827744602664217e-05, 'epoch': 0.4}\n",
      "{'loss': 0.8125, 'grad_norm': 1.446890950202942, 'learning_rate': 4.816260909508498e-05, 'epoch': 0.41}\n",
      "{'loss': 0.7206, 'grad_norm': 11.437918663024902, 'learning_rate': 4.8047772163527796e-05, 'epoch': 0.41}\n",
      "{'loss': 0.7683, 'grad_norm': 3.3834095001220703, 'learning_rate': 4.7932935231970603e-05, 'epoch': 0.42}\n",
      "{'loss': 0.5338, 'grad_norm': 7.100964546203613, 'learning_rate': 4.781809830041342e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5912, 'grad_norm': 5.0959792137146, 'learning_rate': 4.7703261368856226e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5932, 'grad_norm': 3.7710301876068115, 'learning_rate': 4.758842443729904e-05, 'epoch': 0.44}\n",
      "{'loss': 0.6723, 'grad_norm': 2.6890056133270264, 'learning_rate': 4.747358750574185e-05, 'epoch': 0.44}\n",
      "{'loss': 0.6697, 'grad_norm': 1.8929758071899414, 'learning_rate': 4.735875057418466e-05, 'epoch': 0.45}\n",
      "{'loss': 0.4578, 'grad_norm': 1.2641409635543823, 'learning_rate': 4.724391364262747e-05, 'epoch': 0.46}\n",
      "{'loss': 0.4043, 'grad_norm': 1.0960512161254883, 'learning_rate': 4.712907671107028e-05, 'epoch': 0.46}\n",
      "{'loss': 0.6419, 'grad_norm': 6.005330562591553, 'learning_rate': 4.701423977951309e-05, 'epoch': 0.47}\n",
      "{'loss': 0.6383, 'grad_norm': 2.7479164600372314, 'learning_rate': 4.68994028479559e-05, 'epoch': 0.48}\n",
      "{'loss': 0.6044, 'grad_norm': 5.014618873596191, 'learning_rate': 4.6784565916398715e-05, 'epoch': 0.48}\n",
      "{'loss': 0.8518, 'grad_norm': 1.2754456996917725, 'learning_rate': 4.666972898484153e-05, 'epoch': 0.49}\n",
      "{'loss': 0.6222, 'grad_norm': 1.8035513162612915, 'learning_rate': 4.655489205328434e-05, 'epoch': 0.49}\n",
      "{'loss': 0.5858, 'grad_norm': 2.983696460723877, 'learning_rate': 4.644005512172715e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5363, 'grad_norm': 422.7490539550781, 'learning_rate': 4.632521819016996e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7076, 'grad_norm': 2.0608062744140625, 'learning_rate': 4.6210381258612774e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5806, 'grad_norm': 1.8289809226989746, 'learning_rate': 4.609554432705558e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5732, 'grad_norm': 1.9904758930206299, 'learning_rate': 4.59807073954984e-05, 'epoch': 0.53}\n",
      "{'loss': 0.498, 'grad_norm': 2.5628628730773926, 'learning_rate': 4.5865870463941204e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6007, 'grad_norm': 4.808988094329834, 'learning_rate': 4.575103353238402e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5655, 'grad_norm': 5.3451337814331055, 'learning_rate': 4.5636196600826834e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6636, 'grad_norm': 4.657985687255859, 'learning_rate': 4.552135966926964e-05, 'epoch': 0.55}\n",
      "{'loss': 0.622, 'grad_norm': 1.7668713331222534, 'learning_rate': 4.540652273771245e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6876, 'grad_norm': 1.6591875553131104, 'learning_rate': 4.529168580615526e-05, 'epoch': 0.56}\n",
      "{'loss': 0.578, 'grad_norm': 1.3760631084442139, 'learning_rate': 4.517684887459807e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5078, 'grad_norm': 5.748371601104736, 'learning_rate': 4.5062011943040886e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6013, 'grad_norm': 3.8334600925445557, 'learning_rate': 4.4947175011483694e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6479, 'grad_norm': 1.825980305671692, 'learning_rate': 4.483233807992651e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5014, 'grad_norm': 3.1302292346954346, 'learning_rate': 4.4717501148369316e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5798, 'grad_norm': 6.269925117492676, 'learning_rate': 4.460266421681213e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5752, 'grad_norm': 1.5891783237457275, 'learning_rate': 4.448782728525494e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7878, 'grad_norm': 8.208337783813477, 'learning_rate': 4.437299035369775e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6616, 'grad_norm': 1.696004867553711, 'learning_rate': 4.425815342214056e-05, 'epoch': 0.62}\n",
      "{'loss': 0.4438, 'grad_norm': 1.2763397693634033, 'learning_rate': 4.4143316490583375e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5086, 'grad_norm': 1.1991798877716064, 'learning_rate': 4.402847955902619e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5154, 'grad_norm': 1.1963673830032349, 'learning_rate': 4.3913642627469e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7413, 'grad_norm': 2.161458969116211, 'learning_rate': 4.379880569591181e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7623, 'grad_norm': 6.444956302642822, 'learning_rate': 4.368396876435461e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6629, 'grad_norm': 4.249777793884277, 'learning_rate': 4.356913183279743e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6883, 'grad_norm': 1.5468870401382446, 'learning_rate': 4.345429490124024e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6522, 'grad_norm': 2.7487375736236572, 'learning_rate': 4.333945796968305e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6842, 'grad_norm': 1.7807799577713013, 'learning_rate': 4.3224621038125865e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7002, 'grad_norm': 7.645183563232422, 'learning_rate': 4.310978410656867e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7284, 'grad_norm': 4.35326623916626, 'learning_rate': 4.299494717501149e-05, 'epoch': 0.69}\n",
      "{'loss': 0.715, 'grad_norm': 6.591519832611084, 'learning_rate': 4.2880110243454295e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6675, 'grad_norm': 2.1196556091308594, 'learning_rate': 4.276527331189711e-05, 'epoch': 0.7}\n",
      "{'loss': 0.753, 'grad_norm': 3.79211163520813, 'learning_rate': 4.265043638033992e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6697, 'grad_norm': 3.339721918106079, 'learning_rate': 4.253559944878273e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8025, 'grad_norm': 1.4862970113754272, 'learning_rate': 4.2420762517225546e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7203, 'grad_norm': 4.104820251464844, 'learning_rate': 4.2305925585668354e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7037, 'grad_norm': 6.147402286529541, 'learning_rate': 4.219108865411117e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6615, 'grad_norm': 2.175062656402588, 'learning_rate': 4.2076251722553976e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6793, 'grad_norm': 4.926938056945801, 'learning_rate': 4.196141479099679e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7309, 'grad_norm': 2.524296998977661, 'learning_rate': 4.184657785943959e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7242, 'grad_norm': 7.423033714294434, 'learning_rate': 4.1731740927882406e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7216, 'grad_norm': 2.140219211578369, 'learning_rate': 4.161690399632522e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6715, 'grad_norm': 2.531627893447876, 'learning_rate': 4.150206706476803e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6744, 'grad_norm': 1.6125751733779907, 'learning_rate': 4.138723013321084e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6914, 'grad_norm': 1.4489682912826538, 'learning_rate': 4.127239320165365e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7135, 'grad_norm': 1.6487966775894165, 'learning_rate': 4.1157556270096466e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7114, 'grad_norm': 3.3806650638580322, 'learning_rate': 4.1042719338539273e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6839, 'grad_norm': 1.804524302482605, 'learning_rate': 4.092788240698209e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7359, 'grad_norm': 0.9806353449821472, 'learning_rate': 4.08130454754249e-05, 'epoch': 0.8}\n",
      "{'loss': 0.707, 'grad_norm': 3.5206387042999268, 'learning_rate': 4.069820854386771e-05, 'epoch': 0.81}\n",
      "{'loss': 0.657, 'grad_norm': 1.5553327798843384, 'learning_rate': 4.0583371612310525e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6874, 'grad_norm': 4.982079982757568, 'learning_rate': 4.046853468075333e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7605, 'grad_norm': 2.9066059589385986, 'learning_rate': 4.035369774919615e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6767, 'grad_norm': 1.6586085557937622, 'learning_rate': 4.0238860817638955e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6903, 'grad_norm': 2.9912331104278564, 'learning_rate': 4.012402388608177e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6934, 'grad_norm': 4.430099010467529, 'learning_rate': 4.000918695452458e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5842, 'grad_norm': 1.4188098907470703, 'learning_rate': 3.9894350022967385e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8315, 'grad_norm': 9.887072563171387, 'learning_rate': 3.97795130914102e-05, 'epoch': 0.86}\n",
      "{'loss': 1.4404, 'grad_norm': 6.962924003601074, 'learning_rate': 3.966467615985301e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7038, 'grad_norm': 3.0401017665863037, 'learning_rate': 3.954983922829582e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6494, 'grad_norm': 3.367250680923462, 'learning_rate': 3.943500229673863e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5291, 'grad_norm': 2.7796120643615723, 'learning_rate': 3.9320165365181444e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5371, 'grad_norm': 2.7010791301727295, 'learning_rate': 3.920532843362425e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6275, 'grad_norm': 1.901231050491333, 'learning_rate': 3.909049150206707e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5222, 'grad_norm': 1.2152409553527832, 'learning_rate': 3.897565457050988e-05, 'epoch': 0.9}\n",
      "{'loss': 0.7193, 'grad_norm': 6.158452987670898, 'learning_rate': 3.886081763895269e-05, 'epoch': 0.91}\n",
      "{'loss': 0.551, 'grad_norm': 5.029623985290527, 'learning_rate': 3.8745980707395504e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5262, 'grad_norm': 1.818318247795105, 'learning_rate': 3.863114377583831e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4901, 'grad_norm': 0.9449670314788818, 'learning_rate': 3.8516306844281126e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6037, 'grad_norm': 2.514869213104248, 'learning_rate': 3.8401469912723934e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5976, 'grad_norm': 2.295299768447876, 'learning_rate': 3.828663298116674e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5048, 'grad_norm': 61.82500076293945, 'learning_rate': 3.8171796049609556e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3807, 'grad_norm': 1.1858885288238525, 'learning_rate': 3.8056959118052364e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7166, 'grad_norm': 0.8701443672180176, 'learning_rate': 3.794212218649518e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3784, 'grad_norm': 2.125537633895874, 'learning_rate': 3.7827285254937986e-05, 'epoch': 0.96}\n",
      "{'loss': 0.6739, 'grad_norm': 1.9697895050048828, 'learning_rate': 3.77124483233808e-05, 'epoch': 0.97}\n",
      "{'loss': 0.6249, 'grad_norm': 2.202214002609253, 'learning_rate': 3.759761139182361e-05, 'epoch': 0.98}\n",
      "{'loss': 0.5713, 'grad_norm': 5.535577297210693, 'learning_rate': 3.748277446026642e-05, 'epoch': 0.98}\n",
      "{'loss': 0.5598, 'grad_norm': 2.7242860794067383, 'learning_rate': 3.736793752870924e-05, 'epoch': 0.99}\n",
      "{'loss': 0.5307, 'grad_norm': 1.5947469472885132, 'learning_rate': 3.7253100597152045e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2c7cd8a4e548619ac8d829113e2ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5335590839385986, 'eval_runtime': 99.6202, 'eval_samples_per_second': 11.464, 'eval_steps_per_second': 2.871, 'epoch': 1.0}\n",
      "{'loss': 0.4928, 'grad_norm': 1.1166706085205078, 'learning_rate': 3.713826366559486e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3916, 'grad_norm': 1.8034169673919678, 'learning_rate': 3.702342673403767e-05, 'epoch': 1.01}\n",
      "{'loss': 0.6014, 'grad_norm': 2.653855800628662, 'learning_rate': 3.690858980248048e-05, 'epoch': 1.01}\n",
      "{'loss': 0.47, 'grad_norm': 2.2010912895202637, 'learning_rate': 3.679375287092329e-05, 'epoch': 1.02}\n",
      "{'loss': 0.5237, 'grad_norm': 0.9264560341835022, 'learning_rate': 3.6678915939366105e-05, 'epoch': 1.03}\n",
      "{'loss': 0.5434, 'grad_norm': 3.8087637424468994, 'learning_rate': 3.656407900780892e-05, 'epoch': 1.03}\n",
      "{'loss': 0.6922, 'grad_norm': 4.619762897491455, 'learning_rate': 3.644924207625172e-05, 'epoch': 1.04}\n",
      "{'loss': 0.5363, 'grad_norm': 2.6939892768859863, 'learning_rate': 3.6334405144694535e-05, 'epoch': 1.04}\n",
      "{'loss': 0.3662, 'grad_norm': 0.7608093023300171, 'learning_rate': 3.621956821313734e-05, 'epoch': 1.05}\n",
      "{'loss': 0.7007, 'grad_norm': 2.0635759830474854, 'learning_rate': 3.610473128158016e-05, 'epoch': 1.06}\n",
      "{'loss': 0.6954, 'grad_norm': 0.9885390400886536, 'learning_rate': 3.5989894350022965e-05, 'epoch': 1.06}\n",
      "{'loss': 0.5074, 'grad_norm': 4.118795394897461, 'learning_rate': 3.587505741846578e-05, 'epoch': 1.07}\n",
      "{'loss': 0.4196, 'grad_norm': 2.4429917335510254, 'learning_rate': 3.5760220486908594e-05, 'epoch': 1.08}\n",
      "{'loss': 0.5933, 'grad_norm': 2.4736948013305664, 'learning_rate': 3.56453835553514e-05, 'epoch': 1.08}\n",
      "{'loss': 0.6518, 'grad_norm': 5.054843425750732, 'learning_rate': 3.5530546623794216e-05, 'epoch': 1.09}\n",
      "{'loss': 0.624, 'grad_norm': 8.847526550292969, 'learning_rate': 3.5415709692237024e-05, 'epoch': 1.09}\n",
      "{'loss': 0.6481, 'grad_norm': 1.3889262676239014, 'learning_rate': 3.530087276067984e-05, 'epoch': 1.1}\n",
      "{'loss': 0.5544, 'grad_norm': 1.6299833059310913, 'learning_rate': 3.5186035829122646e-05, 'epoch': 1.11}\n",
      "{'loss': 0.6521, 'grad_norm': 2.47774600982666, 'learning_rate': 3.507119889756546e-05, 'epoch': 1.11}\n",
      "{'loss': 0.7314, 'grad_norm': 3.0027949810028076, 'learning_rate': 3.495636196600827e-05, 'epoch': 1.12}\n",
      "{'loss': 0.5592, 'grad_norm': 1.5980308055877686, 'learning_rate': 3.484152503445108e-05, 'epoch': 1.12}\n",
      "{'loss': 0.5702, 'grad_norm': 3.744659662246704, 'learning_rate': 3.47266881028939e-05, 'epoch': 1.13}\n",
      "{'loss': 0.6261, 'grad_norm': 2.024343490600586, 'learning_rate': 3.46118511713367e-05, 'epoch': 1.14}\n",
      "{'loss': 0.6511, 'grad_norm': 2.16357421875, 'learning_rate': 3.449701423977951e-05, 'epoch': 1.14}\n",
      "{'loss': 0.6486, 'grad_norm': 3.401223659515381, 'learning_rate': 3.438217730822232e-05, 'epoch': 1.15}\n",
      "{'loss': 0.7245, 'grad_norm': 3.4137752056121826, 'learning_rate': 3.4267340376665136e-05, 'epoch': 1.16}\n",
      "{'loss': 0.7115, 'grad_norm': 3.296414375305176, 'learning_rate': 3.415250344510795e-05, 'epoch': 1.16}\n",
      "{'loss': 0.688, 'grad_norm': 1.2298883199691772, 'learning_rate': 3.403766651355076e-05, 'epoch': 1.17}\n",
      "{'loss': 0.5951, 'grad_norm': 3.051445960998535, 'learning_rate': 3.392282958199357e-05, 'epoch': 1.17}\n",
      "{'loss': 0.6089, 'grad_norm': 4.168669700622559, 'learning_rate': 3.380799265043638e-05, 'epoch': 1.18}\n",
      "{'loss': 0.6148, 'grad_norm': 2.3735311031341553, 'learning_rate': 3.3693155718879195e-05, 'epoch': 1.19}\n",
      "{'loss': 0.5781, 'grad_norm': 2.709359645843506, 'learning_rate': 3.3578318787322e-05, 'epoch': 1.19}\n",
      "{'loss': 0.6586, 'grad_norm': 1.5713202953338623, 'learning_rate': 3.346348185576482e-05, 'epoch': 1.2}\n",
      "{'loss': 0.5395, 'grad_norm': 1.0330678224563599, 'learning_rate': 3.3348644924207625e-05, 'epoch': 1.21}\n",
      "{'loss': 0.505, 'grad_norm': 3.543226718902588, 'learning_rate': 3.323380799265044e-05, 'epoch': 1.21}\n",
      "{'loss': 0.457, 'grad_norm': 4.531463623046875, 'learning_rate': 3.3118971061093254e-05, 'epoch': 1.22}\n",
      "{'loss': 0.7537, 'grad_norm': 1.4574912786483765, 'learning_rate': 3.300413412953606e-05, 'epoch': 1.22}\n",
      "{'loss': 0.6067, 'grad_norm': 1.297168254852295, 'learning_rate': 3.288929719797887e-05, 'epoch': 1.23}\n",
      "{'loss': 0.6039, 'grad_norm': 3.048846960067749, 'learning_rate': 3.277446026642168e-05, 'epoch': 1.24}\n",
      "{'loss': 0.5659, 'grad_norm': 8.80235481262207, 'learning_rate': 3.265962333486449e-05, 'epoch': 1.24}\n",
      "{'loss': 0.6569, 'grad_norm': 8.206851959228516, 'learning_rate': 3.2544786403307307e-05, 'epoch': 1.25}\n",
      "{'loss': 0.7453, 'grad_norm': 0.6952835917472839, 'learning_rate': 3.2429949471750114e-05, 'epoch': 1.25}\n",
      "{'loss': 0.63, 'grad_norm': 6.397054672241211, 'learning_rate': 3.231511254019293e-05, 'epoch': 1.26}\n",
      "{'loss': 0.7167, 'grad_norm': 0.9951399564743042, 'learning_rate': 3.220027560863574e-05, 'epoch': 1.27}\n",
      "{'loss': 0.6573, 'grad_norm': 1.7232997417449951, 'learning_rate': 3.208543867707855e-05, 'epoch': 1.27}\n",
      "{'loss': 0.7115, 'grad_norm': 2.4467766284942627, 'learning_rate': 3.197060174552136e-05, 'epoch': 1.28}\n",
      "{'loss': 0.578, 'grad_norm': 5.983935356140137, 'learning_rate': 3.1855764813964174e-05, 'epoch': 1.29}\n",
      "{'loss': 0.7522, 'grad_norm': 12.277365684509277, 'learning_rate': 3.174092788240698e-05, 'epoch': 1.29}\n",
      "{'loss': 0.6918, 'grad_norm': 3.3677077293395996, 'learning_rate': 3.1626090950849796e-05, 'epoch': 1.3}\n",
      "{'loss': 0.6254, 'grad_norm': 5.34920072555542, 'learning_rate': 3.151125401929261e-05, 'epoch': 1.3}\n",
      "{'loss': 0.6041, 'grad_norm': 1374.6845703125, 'learning_rate': 3.139641708773542e-05, 'epoch': 1.31}\n",
      "{'loss': 0.7917, 'grad_norm': 6.877318859100342, 'learning_rate': 3.128158015617823e-05, 'epoch': 1.32}\n",
      "{'loss': 0.741, 'grad_norm': 4.842609882354736, 'learning_rate': 3.116674322462104e-05, 'epoch': 1.32}\n",
      "{'loss': 0.6841, 'grad_norm': 4.598971366882324, 'learning_rate': 3.105190629306385e-05, 'epoch': 1.33}\n",
      "{'loss': 0.5003, 'grad_norm': 2.8805785179138184, 'learning_rate': 3.093706936150666e-05, 'epoch': 1.33}\n",
      "{'loss': 0.6041, 'grad_norm': 3.118939161300659, 'learning_rate': 3.082223242994947e-05, 'epoch': 1.34}\n",
      "{'loss': 0.785, 'grad_norm': 6.1806440353393555, 'learning_rate': 3.0707395498392285e-05, 'epoch': 1.35}\n",
      "{'loss': 0.4766, 'grad_norm': 1.6977711915969849, 'learning_rate': 3.059255856683509e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3818, 'grad_norm': 5.2295050621032715, 'learning_rate': 3.0477721635277908e-05, 'epoch': 1.36}\n",
      "{'loss': 0.6502, 'grad_norm': 0.986765444278717, 'learning_rate': 3.036288470372072e-05, 'epoch': 1.37}\n",
      "{'loss': 0.7495, 'grad_norm': 5.216710090637207, 'learning_rate': 3.024804777216353e-05, 'epoch': 1.37}\n",
      "{'loss': 0.9469, 'grad_norm': 9.179948806762695, 'learning_rate': 3.013321084060634e-05, 'epoch': 1.38}\n",
      "{'loss': 0.5139, 'grad_norm': 2.0712714195251465, 'learning_rate': 3.0018373909049152e-05, 'epoch': 1.38}\n",
      "{'loss': 0.642, 'grad_norm': 4.142796516418457, 'learning_rate': 2.9903536977491963e-05, 'epoch': 1.39}\n",
      "{'loss': 0.5817, 'grad_norm': 3.4240238666534424, 'learning_rate': 2.9788700045934775e-05, 'epoch': 1.4}\n",
      "{'loss': 0.5156, 'grad_norm': 1.5455315113067627, 'learning_rate': 2.9673863114377586e-05, 'epoch': 1.4}\n",
      "{'loss': 0.5855, 'grad_norm': 5.9463982582092285, 'learning_rate': 2.9559026182820397e-05, 'epoch': 1.41}\n",
      "{'loss': 0.4839, 'grad_norm': 2.1549227237701416, 'learning_rate': 2.944418925126321e-05, 'epoch': 1.42}\n",
      "{'loss': 0.5272, 'grad_norm': 2.8167715072631836, 'learning_rate': 2.9329352319706023e-05, 'epoch': 1.42}\n",
      "{'loss': 0.4472, 'grad_norm': 2.4403951168060303, 'learning_rate': 2.9214515388148827e-05, 'epoch': 1.43}\n",
      "{'loss': 0.6546, 'grad_norm': 3.537670373916626, 'learning_rate': 2.9099678456591638e-05, 'epoch': 1.43}\n",
      "{'loss': 0.4625, 'grad_norm': 2.6148815155029297, 'learning_rate': 2.898484152503445e-05, 'epoch': 1.44}\n",
      "{'loss': 0.346, 'grad_norm': 1.8090379238128662, 'learning_rate': 2.8870004593477264e-05, 'epoch': 1.45}\n",
      "{'loss': 0.5973, 'grad_norm': 1.5634783506393433, 'learning_rate': 2.8755167661920075e-05, 'epoch': 1.45}\n",
      "{'loss': 0.6948, 'grad_norm': 7.183749675750732, 'learning_rate': 2.8640330730362886e-05, 'epoch': 1.46}\n",
      "{'loss': 0.4975, 'grad_norm': 3.071213722229004, 'learning_rate': 2.8525493798805697e-05, 'epoch': 1.46}\n",
      "{'loss': 0.5022, 'grad_norm': 4.622086048126221, 'learning_rate': 2.841065686724851e-05, 'epoch': 1.47}\n",
      "{'loss': 0.5756, 'grad_norm': 3.06603741645813, 'learning_rate': 2.829581993569132e-05, 'epoch': 1.48}\n",
      "{'loss': 0.6544, 'grad_norm': 4.850578784942627, 'learning_rate': 2.818098300413413e-05, 'epoch': 1.48}\n",
      "{'loss': 0.3691, 'grad_norm': 5.598288059234619, 'learning_rate': 2.8066146072576942e-05, 'epoch': 1.49}\n",
      "{'loss': 0.4858, 'grad_norm': 6.0303778648376465, 'learning_rate': 2.7951309141019753e-05, 'epoch': 1.5}\n",
      "{'loss': 0.3869, 'grad_norm': 1.6687467098236084, 'learning_rate': 2.7836472209462568e-05, 'epoch': 1.5}\n",
      "{'loss': 0.5173, 'grad_norm': 2.849900245666504, 'learning_rate': 2.772163527790538e-05, 'epoch': 1.51}\n",
      "{'loss': 0.3779, 'grad_norm': 2.149721622467041, 'learning_rate': 2.760679834634819e-05, 'epoch': 1.51}\n",
      "{'loss': 0.6833, 'grad_norm': 2.7585949897766113, 'learning_rate': 2.7491961414790994e-05, 'epoch': 1.52}\n",
      "{'loss': 0.5889, 'grad_norm': 1.7867218255996704, 'learning_rate': 2.7377124483233806e-05, 'epoch': 1.53}\n",
      "{'loss': 0.7562, 'grad_norm': 1.82767653465271, 'learning_rate': 2.726228755167662e-05, 'epoch': 1.53}\n",
      "{'loss': 0.5781, 'grad_norm': 352.08099365234375, 'learning_rate': 2.714745062011943e-05, 'epoch': 1.54}\n",
      "{'loss': 0.6713, 'grad_norm': 2.2885074615478516, 'learning_rate': 2.7032613688562243e-05, 'epoch': 1.55}\n",
      "{'loss': 0.3905, 'grad_norm': 0.96664959192276, 'learning_rate': 2.6917776757005054e-05, 'epoch': 1.55}\n",
      "{'loss': 0.6084, 'grad_norm': 0.9507688283920288, 'learning_rate': 2.6802939825447865e-05, 'epoch': 1.56}\n",
      "{'loss': 0.6675, 'grad_norm': 5.0518364906311035, 'learning_rate': 2.6688102893890676e-05, 'epoch': 1.56}\n",
      "{'loss': 0.6396, 'grad_norm': 1.5066168308258057, 'learning_rate': 2.6573265962333487e-05, 'epoch': 1.57}\n",
      "{'loss': 0.5636, 'grad_norm': 2.853142261505127, 'learning_rate': 2.64584290307763e-05, 'epoch': 1.58}\n",
      "{'loss': 0.4858, 'grad_norm': 0.9428592920303345, 'learning_rate': 2.634359209921911e-05, 'epoch': 1.58}\n",
      "{'loss': 0.5101, 'grad_norm': 1.325206995010376, 'learning_rate': 2.6228755167661924e-05, 'epoch': 1.59}\n",
      "{'loss': 0.474, 'grad_norm': 5.623714447021484, 'learning_rate': 2.6113918236104735e-05, 'epoch': 1.59}\n",
      "{'loss': 0.609, 'grad_norm': 1.042033314704895, 'learning_rate': 2.5999081304547546e-05, 'epoch': 1.6}\n",
      "{'loss': 0.6265, 'grad_norm': 6.489346981048584, 'learning_rate': 2.5884244372990358e-05, 'epoch': 1.61}\n",
      "{'loss': 0.5222, 'grad_norm': 0.8095769882202148, 'learning_rate': 2.576940744143317e-05, 'epoch': 1.61}\n",
      "{'loss': 0.7614, 'grad_norm': 5.918344020843506, 'learning_rate': 2.5654570509875973e-05, 'epoch': 1.62}\n",
      "{'loss': 0.3439, 'grad_norm': 1.2178478240966797, 'learning_rate': 2.5539733578318788e-05, 'epoch': 1.63}\n",
      "{'loss': 0.5115, 'grad_norm': 2.445251941680908, 'learning_rate': 2.54248966467616e-05, 'epoch': 1.63}\n",
      "{'loss': 0.705, 'grad_norm': 1.4528728723526, 'learning_rate': 2.531005971520441e-05, 'epoch': 1.64}\n",
      "{'loss': 0.3849, 'grad_norm': 1.5815943479537964, 'learning_rate': 2.519522278364722e-05, 'epoch': 1.64}\n",
      "{'loss': 0.4014, 'grad_norm': 0.6893630623817444, 'learning_rate': 2.5080385852090032e-05, 'epoch': 1.65}\n",
      "{'loss': 0.6119, 'grad_norm': 5.698859214782715, 'learning_rate': 2.4965548920532844e-05, 'epoch': 1.66}\n",
      "{'loss': 0.7186, 'grad_norm': 1.1970711946487427, 'learning_rate': 2.4850711988975655e-05, 'epoch': 1.66}\n",
      "{'loss': 0.6015, 'grad_norm': 2.6881089210510254, 'learning_rate': 2.4735875057418466e-05, 'epoch': 1.67}\n",
      "{'loss': 0.7703, 'grad_norm': 0.9506152868270874, 'learning_rate': 2.462103812586128e-05, 'epoch': 1.67}\n",
      "{'loss': 0.7465, 'grad_norm': 1.189959168434143, 'learning_rate': 2.450620119430409e-05, 'epoch': 1.68}\n",
      "{'loss': 0.5626, 'grad_norm': 1.4279518127441406, 'learning_rate': 2.4391364262746903e-05, 'epoch': 1.69}\n",
      "{'loss': 0.8409, 'grad_norm': 2.4569735527038574, 'learning_rate': 2.427652733118971e-05, 'epoch': 1.69}\n",
      "{'loss': 0.6011, 'grad_norm': 1.3282456398010254, 'learning_rate': 2.4161690399632522e-05, 'epoch': 1.7}\n",
      "{'loss': 0.539, 'grad_norm': 1.9523417949676514, 'learning_rate': 2.4046853468075333e-05, 'epoch': 1.71}\n",
      "{'loss': 0.5852, 'grad_norm': 2.5733821392059326, 'learning_rate': 2.3932016536518144e-05, 'epoch': 1.71}\n",
      "{'loss': 0.4971, 'grad_norm': 2.102234363555908, 'learning_rate': 2.381717960496096e-05, 'epoch': 1.72}\n",
      "{'loss': 0.7931, 'grad_norm': 1.2520323991775513, 'learning_rate': 2.370234267340377e-05, 'epoch': 1.72}\n",
      "{'loss': 0.5782, 'grad_norm': 2.260246515274048, 'learning_rate': 2.358750574184658e-05, 'epoch': 1.73}\n",
      "{'loss': 0.578, 'grad_norm': 1.4175808429718018, 'learning_rate': 2.3472668810289392e-05, 'epoch': 1.74}\n",
      "{'loss': 0.6572, 'grad_norm': 5.204119682312012, 'learning_rate': 2.33578318787322e-05, 'epoch': 1.74}\n",
      "{'loss': 0.4702, 'grad_norm': 3.4042365550994873, 'learning_rate': 2.324299494717501e-05, 'epoch': 1.75}\n",
      "{'loss': 0.5755, 'grad_norm': 1.08058762550354, 'learning_rate': 2.3128158015617822e-05, 'epoch': 1.76}\n",
      "{'loss': 0.6117, 'grad_norm': 1.2390851974487305, 'learning_rate': 2.3013321084060633e-05, 'epoch': 1.76}\n",
      "{'loss': 0.5822, 'grad_norm': 3.7176709175109863, 'learning_rate': 2.2898484152503448e-05, 'epoch': 1.77}\n",
      "{'loss': 0.451, 'grad_norm': 2.494399070739746, 'learning_rate': 2.278364722094626e-05, 'epoch': 1.77}\n",
      "{'loss': 0.665, 'grad_norm': 10.080405235290527, 'learning_rate': 2.266881028938907e-05, 'epoch': 1.78}\n",
      "{'loss': 0.5669, 'grad_norm': 1.2622681856155396, 'learning_rate': 2.2553973357831878e-05, 'epoch': 1.79}\n",
      "{'loss': 0.5387, 'grad_norm': 1.4673314094543457, 'learning_rate': 2.243913642627469e-05, 'epoch': 1.79}\n",
      "{'loss': 0.569, 'grad_norm': 3.9451332092285156, 'learning_rate': 2.23242994947175e-05, 'epoch': 1.8}\n",
      "{'loss': 0.5961, 'grad_norm': 1.3812108039855957, 'learning_rate': 2.220946256316031e-05, 'epoch': 1.8}\n",
      "{'loss': 0.6633, 'grad_norm': 4.6939802169799805, 'learning_rate': 2.2094625631603126e-05, 'epoch': 1.81}\n",
      "{'loss': 0.4177, 'grad_norm': 2.0615620613098145, 'learning_rate': 2.1979788700045937e-05, 'epoch': 1.82}\n",
      "{'loss': 0.5126, 'grad_norm': 8.449957847595215, 'learning_rate': 2.186495176848875e-05, 'epoch': 1.82}\n",
      "{'loss': 0.7248, 'grad_norm': 3.4065070152282715, 'learning_rate': 2.175011483693156e-05, 'epoch': 1.83}\n",
      "{'loss': 0.3948, 'grad_norm': 2.64239501953125, 'learning_rate': 2.1635277905374367e-05, 'epoch': 1.84}\n",
      "{'loss': 0.4139, 'grad_norm': 2.7086191177368164, 'learning_rate': 2.152044097381718e-05, 'epoch': 1.84}\n",
      "{'loss': 0.7756, 'grad_norm': 2.0638928413391113, 'learning_rate': 2.140560404225999e-05, 'epoch': 1.85}\n",
      "{'loss': 0.3929, 'grad_norm': 2.6598329544067383, 'learning_rate': 2.1290767110702804e-05, 'epoch': 1.85}\n",
      "{'loss': 0.5981, 'grad_norm': 4.926570892333984, 'learning_rate': 2.1175930179145615e-05, 'epoch': 1.86}\n",
      "{'loss': 0.4222, 'grad_norm': 5.343914031982422, 'learning_rate': 2.1061093247588427e-05, 'epoch': 1.87}\n",
      "{'loss': 0.4334, 'grad_norm': 2.7401645183563232, 'learning_rate': 2.0946256316031238e-05, 'epoch': 1.87}\n",
      "{'loss': 0.5082, 'grad_norm': 1.1362532377243042, 'learning_rate': 2.083141938447405e-05, 'epoch': 1.88}\n",
      "{'loss': 0.4626, 'grad_norm': 2.421107292175293, 'learning_rate': 2.0716582452916857e-05, 'epoch': 1.89}\n",
      "{'loss': 0.8822, 'grad_norm': 3.5558032989501953, 'learning_rate': 2.0601745521359668e-05, 'epoch': 1.89}\n",
      "{'loss': 0.3806, 'grad_norm': 0.9599409699440002, 'learning_rate': 2.0486908589802482e-05, 'epoch': 1.9}\n",
      "{'loss': 0.3602, 'grad_norm': 4.24758243560791, 'learning_rate': 2.0372071658245294e-05, 'epoch': 1.9}\n",
      "{'loss': 0.5866, 'grad_norm': 1.7785874605178833, 'learning_rate': 2.0257234726688105e-05, 'epoch': 1.91}\n",
      "{'loss': 0.6704, 'grad_norm': 1.607174038887024, 'learning_rate': 2.0142397795130916e-05, 'epoch': 1.92}\n",
      "{'loss': 0.9218, 'grad_norm': 5.059096336364746, 'learning_rate': 2.0027560863573727e-05, 'epoch': 1.92}\n",
      "{'loss': 0.6738, 'grad_norm': 1.2936302423477173, 'learning_rate': 1.9912723932016538e-05, 'epoch': 1.93}\n",
      "{'loss': 0.9666, 'grad_norm': 9.037571907043457, 'learning_rate': 1.9797887000459346e-05, 'epoch': 1.93}\n",
      "{'loss': 0.7683, 'grad_norm': 2.079143762588501, 'learning_rate': 1.968305006890216e-05, 'epoch': 1.94}\n",
      "{'loss': 0.6355, 'grad_norm': 1.3975504636764526, 'learning_rate': 1.9568213137344972e-05, 'epoch': 1.95}\n",
      "{'loss': 0.6706, 'grad_norm': 1.3599708080291748, 'learning_rate': 1.9453376205787783e-05, 'epoch': 1.95}\n",
      "{'loss': 0.7308, 'grad_norm': 6.434340953826904, 'learning_rate': 1.9338539274230594e-05, 'epoch': 1.96}\n",
      "{'loss': 0.6365, 'grad_norm': 4.6140241622924805, 'learning_rate': 1.9223702342673405e-05, 'epoch': 1.97}\n",
      "{'loss': 0.6675, 'grad_norm': 1.6378146409988403, 'learning_rate': 1.9108865411116216e-05, 'epoch': 1.97}\n",
      "{'loss': 0.7406, 'grad_norm': 4.98366641998291, 'learning_rate': 1.8994028479559028e-05, 'epoch': 1.98}\n",
      "{'loss': 0.7415, 'grad_norm': 1.422119140625, 'learning_rate': 1.887919154800184e-05, 'epoch': 1.98}\n",
      "{'loss': 0.6652, 'grad_norm': 3.4496591091156006, 'learning_rate': 1.876435461644465e-05, 'epoch': 1.99}\n",
      "{'loss': 0.7116, 'grad_norm': 3.4403305053710938, 'learning_rate': 1.864951768488746e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d17a5dfb6eb4a168f88fd0daf56d33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6862745881080627, 'eval_runtime': 98.827, 'eval_samples_per_second': 11.556, 'eval_steps_per_second': 2.894, 'epoch': 2.0}\n",
      "{'loss': 0.6874, 'grad_norm': 4.795597076416016, 'learning_rate': 1.8534680753330272e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6945, 'grad_norm': 5.555529594421387, 'learning_rate': 1.8419843821773083e-05, 'epoch': 2.01}\n",
      "{'loss': 0.7007, 'grad_norm': 8.224658012390137, 'learning_rate': 1.8305006890215895e-05, 'epoch': 2.01}\n",
      "{'loss': 0.659, 'grad_norm': 1.527021050453186, 'learning_rate': 1.8190169958658706e-05, 'epoch': 2.02}\n",
      "{'loss': 0.6779, 'grad_norm': 1.772902488708496, 'learning_rate': 1.8075333027101517e-05, 'epoch': 2.03}\n",
      "{'loss': 0.6894, 'grad_norm': 2.663522481918335, 'learning_rate': 1.7960496095544328e-05, 'epoch': 2.03}\n",
      "{'loss': 0.718, 'grad_norm': 3.940786600112915, 'learning_rate': 1.784565916398714e-05, 'epoch': 2.04}\n",
      "{'loss': 0.7046, 'grad_norm': 1.3531405925750732, 'learning_rate': 1.773082223242995e-05, 'epoch': 2.05}\n",
      "{'loss': 0.6842, 'grad_norm': 4.962005138397217, 'learning_rate': 1.761598530087276e-05, 'epoch': 2.05}\n",
      "{'loss': 0.7344, 'grad_norm': 2.907127618789673, 'learning_rate': 1.7501148369315573e-05, 'epoch': 2.06}\n",
      "{'loss': 0.7163, 'grad_norm': 5.401495456695557, 'learning_rate': 1.7386311437758384e-05, 'epoch': 2.06}\n",
      "{'loss': 0.7131, 'grad_norm': 1.2088680267333984, 'learning_rate': 1.7271474506201195e-05, 'epoch': 2.07}\n",
      "{'loss': 0.714, 'grad_norm': 3.8529298305511475, 'learning_rate': 1.7156637574644006e-05, 'epoch': 2.08}\n",
      "{'loss': 0.7015, 'grad_norm': 5.604833126068115, 'learning_rate': 1.7041800643086817e-05, 'epoch': 2.08}\n",
      "{'loss': 0.7181, 'grad_norm': 3.531564950942993, 'learning_rate': 1.692696371152963e-05, 'epoch': 2.09}\n",
      "{'loss': 0.6802, 'grad_norm': 2.9993882179260254, 'learning_rate': 1.681212677997244e-05, 'epoch': 2.1}\n",
      "{'loss': 0.6895, 'grad_norm': 1.7158782482147217, 'learning_rate': 1.669728984841525e-05, 'epoch': 2.1}\n",
      "{'loss': 0.6704, 'grad_norm': 4.8871660232543945, 'learning_rate': 1.6582452916858062e-05, 'epoch': 2.11}\n",
      "{'loss': 0.6912, 'grad_norm': 2.7784159183502197, 'learning_rate': 1.6467615985300873e-05, 'epoch': 2.11}\n",
      "{'loss': 0.6982, 'grad_norm': 3.978593587875366, 'learning_rate': 1.6352779053743684e-05, 'epoch': 2.12}\n",
      "{'loss': 0.6972, 'grad_norm': 2.829268217086792, 'learning_rate': 1.6237942122186496e-05, 'epoch': 2.13}\n",
      "{'loss': 0.6925, 'grad_norm': 3.5576653480529785, 'learning_rate': 1.6123105190629307e-05, 'epoch': 2.13}\n",
      "{'loss': 0.6547, 'grad_norm': 4.870845317840576, 'learning_rate': 1.6008268259072118e-05, 'epoch': 2.14}\n",
      "{'loss': 0.7142, 'grad_norm': 8.243358612060547, 'learning_rate': 1.589343132751493e-05, 'epoch': 2.14}\n",
      "{'loss': 0.7924, 'grad_norm': 1.5761659145355225, 'learning_rate': 1.577859439595774e-05, 'epoch': 2.15}\n",
      "{'loss': 0.6689, 'grad_norm': 0.9319784045219421, 'learning_rate': 1.566375746440055e-05, 'epoch': 2.16}\n",
      "{'loss': 0.673, 'grad_norm': 2.459503173828125, 'learning_rate': 1.5548920532843363e-05, 'epoch': 2.16}\n",
      "{'loss': 0.6586, 'grad_norm': 3.850724458694458, 'learning_rate': 1.5434083601286177e-05, 'epoch': 2.17}\n",
      "{'loss': 0.7228, 'grad_norm': 2.61251163482666, 'learning_rate': 1.5319246669728985e-05, 'epoch': 2.18}\n",
      "{'loss': 0.6309, 'grad_norm': 1.647085189819336, 'learning_rate': 1.5204409738171796e-05, 'epoch': 2.18}\n",
      "{'loss': 0.6234, 'grad_norm': 1.8037829399108887, 'learning_rate': 1.5089572806614607e-05, 'epoch': 2.19}\n",
      "{'loss': 0.6357, 'grad_norm': 1.866224765777588, 'learning_rate': 1.4974735875057418e-05, 'epoch': 2.19}\n",
      "{'loss': 0.8125, 'grad_norm': 0.9769414067268372, 'learning_rate': 1.4859898943500231e-05, 'epoch': 2.2}\n",
      "{'loss': 0.7223, 'grad_norm': 1.3728307485580444, 'learning_rate': 1.4745062011943042e-05, 'epoch': 2.21}\n",
      "{'loss': 0.6861, 'grad_norm': 1.5031388998031616, 'learning_rate': 1.4630225080385854e-05, 'epoch': 2.21}\n",
      "{'loss': 0.6732, 'grad_norm': 19.10064125061035, 'learning_rate': 1.4515388148828665e-05, 'epoch': 2.22}\n",
      "{'loss': 0.6411, 'grad_norm': 4.095254898071289, 'learning_rate': 1.4400551217271474e-05, 'epoch': 2.22}\n",
      "{'loss': 0.7369, 'grad_norm': 1.9098209142684937, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.23}\n",
      "{'loss': 0.6516, 'grad_norm': 4.530582904815674, 'learning_rate': 1.4170877354157097e-05, 'epoch': 2.24}\n",
      "{'loss': 0.7308, 'grad_norm': 4.476117134094238, 'learning_rate': 1.405604042259991e-05, 'epoch': 2.24}\n",
      "{'loss': 0.7391, 'grad_norm': 1.688838243484497, 'learning_rate': 1.394120349104272e-05, 'epoch': 2.25}\n",
      "{'loss': 0.7462, 'grad_norm': 29.53993797302246, 'learning_rate': 1.3826366559485532e-05, 'epoch': 2.26}\n",
      "{'loss': 0.7799, 'grad_norm': 2.778505563735962, 'learning_rate': 1.3711529627928343e-05, 'epoch': 2.26}\n",
      "{'loss': 0.9524, 'grad_norm': 10.936604499816895, 'learning_rate': 1.3596692696371154e-05, 'epoch': 2.27}\n",
      "{'loss': 0.7969, 'grad_norm': 8.600296020507812, 'learning_rate': 1.3481855764813964e-05, 'epoch': 2.27}\n",
      "{'loss': 0.6546, 'grad_norm': 2.786973714828491, 'learning_rate': 1.3367018833256775e-05, 'epoch': 2.28}\n",
      "{'loss': 0.7425, 'grad_norm': 14.194987297058105, 'learning_rate': 1.3252181901699588e-05, 'epoch': 2.29}\n",
      "{'loss': 0.6671, 'grad_norm': 4.70331335067749, 'learning_rate': 1.3137344970142399e-05, 'epoch': 2.29}\n",
      "{'loss': 0.7063, 'grad_norm': 6.969564914703369, 'learning_rate': 1.302250803858521e-05, 'epoch': 2.3}\n",
      "{'loss': 0.6075, 'grad_norm': 3.3950533866882324, 'learning_rate': 1.2907671107028021e-05, 'epoch': 2.31}\n",
      "{'loss': 0.6816, 'grad_norm': 1.5159988403320312, 'learning_rate': 1.2792834175470832e-05, 'epoch': 2.31}\n",
      "{'loss': 0.724, 'grad_norm': 1.460237979888916, 'learning_rate': 1.2677997243913642e-05, 'epoch': 2.32}\n",
      "{'loss': 0.6585, 'grad_norm': 4.865129470825195, 'learning_rate': 1.2563160312356453e-05, 'epoch': 2.32}\n",
      "{'loss': 0.6999, 'grad_norm': 1.6326875686645508, 'learning_rate': 1.2448323380799266e-05, 'epoch': 2.33}\n",
      "{'loss': 0.6543, 'grad_norm': 2.604276418685913, 'learning_rate': 1.2333486449242077e-05, 'epoch': 2.34}\n",
      "{'loss': 0.6767, 'grad_norm': 5.769055366516113, 'learning_rate': 1.2218649517684888e-05, 'epoch': 2.34}\n",
      "{'loss': 0.5853, 'grad_norm': 2.614396095275879, 'learning_rate': 1.21038125861277e-05, 'epoch': 2.35}\n",
      "{'loss': 0.6709, 'grad_norm': 2.471950054168701, 'learning_rate': 1.198897565457051e-05, 'epoch': 2.35}\n",
      "{'loss': 0.6613, 'grad_norm': 2.795929193496704, 'learning_rate': 1.1874138723013322e-05, 'epoch': 2.36}\n",
      "{'loss': 0.6472, 'grad_norm': 4.899538516998291, 'learning_rate': 1.1759301791456133e-05, 'epoch': 2.37}\n",
      "{'loss': 0.5888, 'grad_norm': 9.267370223999023, 'learning_rate': 1.1644464859898944e-05, 'epoch': 2.37}\n",
      "{'loss': 0.6217, 'grad_norm': 4.785529613494873, 'learning_rate': 1.1529627928341755e-05, 'epoch': 2.38}\n",
      "{'loss': 0.5841, 'grad_norm': 1.9222320318222046, 'learning_rate': 1.1414790996784566e-05, 'epoch': 2.39}\n",
      "{'loss': 0.7043, 'grad_norm': 3.702148199081421, 'learning_rate': 1.1299954065227377e-05, 'epoch': 2.39}\n",
      "{'loss': 0.6194, 'grad_norm': 1.6656285524368286, 'learning_rate': 1.1185117133670189e-05, 'epoch': 2.4}\n",
      "{'loss': 0.7367, 'grad_norm': 4.640748500823975, 'learning_rate': 1.1070280202113e-05, 'epoch': 2.4}\n",
      "{'loss': 0.5364, 'grad_norm': 1.5633223056793213, 'learning_rate': 1.0955443270555811e-05, 'epoch': 2.41}\n",
      "{'loss': 0.5605, 'grad_norm': 5.725704669952393, 'learning_rate': 1.0840606338998622e-05, 'epoch': 2.42}\n",
      "{'loss': 0.5602, 'grad_norm': 1.9874402284622192, 'learning_rate': 1.0725769407441435e-05, 'epoch': 2.42}\n",
      "{'loss': 0.511, 'grad_norm': 6.785548210144043, 'learning_rate': 1.0610932475884244e-05, 'epoch': 2.43}\n",
      "{'loss': 0.688, 'grad_norm': 5.959900856018066, 'learning_rate': 1.0496095544327056e-05, 'epoch': 2.44}\n",
      "{'loss': 0.7518, 'grad_norm': 4.9149169921875, 'learning_rate': 1.0381258612769867e-05, 'epoch': 2.44}\n",
      "{'loss': 0.7086, 'grad_norm': 7.226240634918213, 'learning_rate': 1.026642168121268e-05, 'epoch': 2.45}\n",
      "{'loss': 0.775, 'grad_norm': 9.653576850891113, 'learning_rate': 1.0151584749655489e-05, 'epoch': 2.45}\n",
      "{'loss': 0.5329, 'grad_norm': 3.7048044204711914, 'learning_rate': 1.00367478180983e-05, 'epoch': 2.46}\n",
      "{'loss': 0.5104, 'grad_norm': 6.808679103851318, 'learning_rate': 9.921910886541111e-06, 'epoch': 2.47}\n",
      "{'loss': 0.4593, 'grad_norm': 4.437983512878418, 'learning_rate': 9.807073954983924e-06, 'epoch': 2.47}\n",
      "{'loss': 0.6549, 'grad_norm': 1.8785250186920166, 'learning_rate': 9.692237023426734e-06, 'epoch': 2.48}\n",
      "{'loss': 0.8177, 'grad_norm': 62.04301834106445, 'learning_rate': 9.577400091869545e-06, 'epoch': 2.48}\n",
      "{'loss': 0.5436, 'grad_norm': 9.3232421875, 'learning_rate': 9.462563160312358e-06, 'epoch': 2.49}\n",
      "{'loss': 0.4004, 'grad_norm': 2.4981391429901123, 'learning_rate': 9.347726228755169e-06, 'epoch': 2.5}\n",
      "{'loss': 0.5376, 'grad_norm': 269.0093078613281, 'learning_rate': 9.232889297197978e-06, 'epoch': 2.5}\n",
      "{'loss': 0.5463, 'grad_norm': 1.9874107837677002, 'learning_rate': 9.11805236564079e-06, 'epoch': 2.51}\n",
      "{'loss': 0.3181, 'grad_norm': 0.9470314383506775, 'learning_rate': 9.003215434083602e-06, 'epoch': 2.52}\n",
      "{'loss': 0.6325, 'grad_norm': 6.377286911010742, 'learning_rate': 8.888378502526414e-06, 'epoch': 2.52}\n",
      "{'loss': 0.7645, 'grad_norm': 241.7396697998047, 'learning_rate': 8.773541570969223e-06, 'epoch': 2.53}\n",
      "{'loss': 0.77, 'grad_norm': 12.803595542907715, 'learning_rate': 8.658704639412036e-06, 'epoch': 2.53}\n",
      "{'loss': 0.4837, 'grad_norm': 3.4656989574432373, 'learning_rate': 8.543867707854847e-06, 'epoch': 2.54}\n",
      "{'loss': 0.8244, 'grad_norm': 5.412765979766846, 'learning_rate': 8.429030776297658e-06, 'epoch': 2.55}\n",
      "{'loss': 0.5961, 'grad_norm': 3.421461582183838, 'learning_rate': 8.314193844740468e-06, 'epoch': 2.55}\n",
      "{'loss': 0.7696, 'grad_norm': 2.682884454727173, 'learning_rate': 8.19935691318328e-06, 'epoch': 2.56}\n",
      "{'loss': 0.7205, 'grad_norm': 7.679965972900391, 'learning_rate': 8.084519981626092e-06, 'epoch': 2.56}\n",
      "{'loss': 0.5845, 'grad_norm': 2.486987352371216, 'learning_rate': 7.969683050068903e-06, 'epoch': 2.57}\n",
      "{'loss': 0.6278, 'grad_norm': 4.888280391693115, 'learning_rate': 7.854846118511714e-06, 'epoch': 2.58}\n",
      "{'loss': 0.5463, 'grad_norm': 9.810083389282227, 'learning_rate': 7.740009186954525e-06, 'epoch': 2.58}\n",
      "{'loss': 0.8048, 'grad_norm': 4.942939758300781, 'learning_rate': 7.6251722553973365e-06, 'epoch': 2.59}\n",
      "{'loss': 0.4983, 'grad_norm': 3.1870296001434326, 'learning_rate': 7.510335323840148e-06, 'epoch': 2.6}\n",
      "{'loss': 0.5367, 'grad_norm': 3.392711639404297, 'learning_rate': 7.395498392282958e-06, 'epoch': 2.6}\n",
      "{'loss': 0.4751, 'grad_norm': 6.0592803955078125, 'learning_rate': 7.28066146072577e-06, 'epoch': 2.61}\n",
      "{'loss': 0.639, 'grad_norm': 3.4925642013549805, 'learning_rate': 7.165824529168581e-06, 'epoch': 2.61}\n",
      "{'loss': 0.4827, 'grad_norm': 5.111255168914795, 'learning_rate': 7.0509875976113915e-06, 'epoch': 2.62}\n",
      "{'loss': 0.4922, 'grad_norm': 2.7883098125457764, 'learning_rate': 6.936150666054203e-06, 'epoch': 2.63}\n",
      "{'loss': 0.5878, 'grad_norm': 1.2902765274047852, 'learning_rate': 6.821313734497015e-06, 'epoch': 2.63}\n",
      "{'loss': 0.4978, 'grad_norm': 1.9879118204116821, 'learning_rate': 6.706476802939826e-06, 'epoch': 2.64}\n",
      "{'loss': 0.5908, 'grad_norm': 4.066060543060303, 'learning_rate': 6.591639871382636e-06, 'epoch': 2.65}\n",
      "{'loss': 0.5897, 'grad_norm': 2.194607973098755, 'learning_rate': 6.476802939825448e-06, 'epoch': 2.65}\n",
      "{'loss': 0.8627, 'grad_norm': 2.762378215789795, 'learning_rate': 6.361966008268259e-06, 'epoch': 2.66}\n",
      "{'loss': 0.506, 'grad_norm': 1.9194495677947998, 'learning_rate': 6.2471290767110705e-06, 'epoch': 2.66}\n",
      "{'loss': 0.5228, 'grad_norm': 1.674240231513977, 'learning_rate': 6.132292145153882e-06, 'epoch': 2.67}\n",
      "{'loss': 0.2932, 'grad_norm': 2.2746503353118896, 'learning_rate': 6.017455213596693e-06, 'epoch': 2.68}\n",
      "{'loss': 0.4951, 'grad_norm': 2.310432195663452, 'learning_rate': 5.902618282039504e-06, 'epoch': 2.68}\n",
      "{'loss': 0.5588, 'grad_norm': 5.74340295791626, 'learning_rate': 5.787781350482315e-06, 'epoch': 2.69}\n",
      "{'loss': 0.6663, 'grad_norm': 2.0931286811828613, 'learning_rate': 5.672944418925126e-06, 'epoch': 2.69}\n",
      "{'loss': 0.4739, 'grad_norm': 2.0856282711029053, 'learning_rate': 5.5581074873679375e-06, 'epoch': 2.7}\n",
      "{'loss': 0.4694, 'grad_norm': 1.755168080329895, 'learning_rate': 5.4432705558107495e-06, 'epoch': 2.71}\n",
      "{'loss': 0.303, 'grad_norm': 2.0167760848999023, 'learning_rate': 5.32843362425356e-06, 'epoch': 2.71}\n",
      "{'loss': 0.3624, 'grad_norm': 5.60518217086792, 'learning_rate': 5.213596692696372e-06, 'epoch': 2.72}\n",
      "{'loss': 0.751, 'grad_norm': 7.960605144500732, 'learning_rate': 5.098759761139182e-06, 'epoch': 2.73}\n",
      "{'loss': 0.674, 'grad_norm': 2.1262264251708984, 'learning_rate': 4.983922829581994e-06, 'epoch': 2.73}\n",
      "{'loss': 0.5023, 'grad_norm': 3.0887084007263184, 'learning_rate': 4.8690858980248045e-06, 'epoch': 2.74}\n",
      "{'loss': 0.6933, 'grad_norm': 3.7903666496276855, 'learning_rate': 4.7542489664676165e-06, 'epoch': 2.74}\n",
      "{'loss': 0.2328, 'grad_norm': 1.4345245361328125, 'learning_rate': 4.639412034910427e-06, 'epoch': 2.75}\n",
      "{'loss': 0.6765, 'grad_norm': 10.64908504486084, 'learning_rate': 4.524575103353239e-06, 'epoch': 2.76}\n",
      "{'loss': 0.4073, 'grad_norm': 11.097371101379395, 'learning_rate': 4.40973817179605e-06, 'epoch': 2.76}\n",
      "{'loss': 0.5623, 'grad_norm': 4.469881057739258, 'learning_rate': 4.294901240238861e-06, 'epoch': 2.77}\n",
      "{'loss': 0.8794, 'grad_norm': 12.141721725463867, 'learning_rate': 4.180064308681672e-06, 'epoch': 2.78}\n",
      "{'loss': 0.4406, 'grad_norm': 24.219133377075195, 'learning_rate': 4.0652273771244835e-06, 'epoch': 2.78}\n",
      "{'loss': 0.5203, 'grad_norm': 7.599961757659912, 'learning_rate': 3.950390445567295e-06, 'epoch': 2.79}\n",
      "{'loss': 0.6539, 'grad_norm': 1.5084586143493652, 'learning_rate': 3.835553514010106e-06, 'epoch': 2.79}\n",
      "{'loss': 0.3619, 'grad_norm': 6.362109661102295, 'learning_rate': 3.720716582452917e-06, 'epoch': 2.8}\n",
      "{'loss': 0.3973, 'grad_norm': 3.9385671615600586, 'learning_rate': 3.6058796508957286e-06, 'epoch': 2.81}\n",
      "{'loss': 0.7281, 'grad_norm': 5.611250400543213, 'learning_rate': 3.4910427193385393e-06, 'epoch': 2.81}\n",
      "{'loss': 0.7601, 'grad_norm': 6.783329010009766, 'learning_rate': 3.376205787781351e-06, 'epoch': 2.82}\n",
      "{'loss': 0.7297, 'grad_norm': 7.577439308166504, 'learning_rate': 3.2613688562241617e-06, 'epoch': 2.82}\n",
      "{'loss': 0.4675, 'grad_norm': 4.217441082000732, 'learning_rate': 3.1465319246669733e-06, 'epoch': 2.83}\n",
      "{'loss': 0.4183, 'grad_norm': 1.026295781135559, 'learning_rate': 3.031694993109784e-06, 'epoch': 2.84}\n",
      "{'loss': 0.367, 'grad_norm': 2.5644636154174805, 'learning_rate': 2.9168580615525956e-06, 'epoch': 2.84}\n",
      "{'loss': 0.4656, 'grad_norm': 97.54178619384766, 'learning_rate': 2.8020211299954068e-06, 'epoch': 2.85}\n",
      "{'loss': 0.7764, 'grad_norm': 5.642171859741211, 'learning_rate': 2.687184198438218e-06, 'epoch': 2.86}\n",
      "{'loss': 0.577, 'grad_norm': 8.74290657043457, 'learning_rate': 2.572347266881029e-06, 'epoch': 2.86}\n",
      "{'loss': 0.4717, 'grad_norm': 1.4747987985610962, 'learning_rate': 2.4575103353238403e-06, 'epoch': 2.87}\n",
      "{'loss': 0.6261, 'grad_norm': 7.814080715179443, 'learning_rate': 2.3426734037666514e-06, 'epoch': 2.87}\n",
      "{'loss': 0.5548, 'grad_norm': 32.6170654296875, 'learning_rate': 2.2278364722094626e-06, 'epoch': 2.88}\n",
      "{'loss': 0.5851, 'grad_norm': 5.5907883644104, 'learning_rate': 2.1129995406522738e-06, 'epoch': 2.89}\n",
      "{'loss': 0.5869, 'grad_norm': 9.245261192321777, 'learning_rate': 1.998162609095085e-06, 'epoch': 2.89}\n",
      "{'loss': 0.485, 'grad_norm': 1.4762859344482422, 'learning_rate': 1.8833256775378963e-06, 'epoch': 2.9}\n",
      "{'loss': 0.4756, 'grad_norm': 6.399489879608154, 'learning_rate': 1.7684887459807075e-06, 'epoch': 2.9}\n",
      "{'loss': 0.5671, 'grad_norm': 5.342720985412598, 'learning_rate': 1.6536518144235186e-06, 'epoch': 2.91}\n",
      "{'loss': 0.6503, 'grad_norm': 1.498968243598938, 'learning_rate': 1.53881488286633e-06, 'epoch': 2.92}\n",
      "{'loss': 0.8196, 'grad_norm': 8.132417678833008, 'learning_rate': 1.4239779513091412e-06, 'epoch': 2.92}\n",
      "{'loss': 0.4886, 'grad_norm': 9.859734535217285, 'learning_rate': 1.3091410197519524e-06, 'epoch': 2.93}\n",
      "{'loss': 0.4953, 'grad_norm': 5.965121746063232, 'learning_rate': 1.1943040881947635e-06, 'epoch': 2.94}\n",
      "{'loss': 0.322, 'grad_norm': 31.348825454711914, 'learning_rate': 1.0794671566375747e-06, 'epoch': 2.94}\n",
      "{'loss': 0.4065, 'grad_norm': 64.9743881225586, 'learning_rate': 9.64630225080386e-07, 'epoch': 2.95}\n",
      "{'loss': 0.5316, 'grad_norm': 0.8430985808372498, 'learning_rate': 8.497932935231971e-07, 'epoch': 2.95}\n",
      "{'loss': 0.2705, 'grad_norm': 0.8922926783561707, 'learning_rate': 7.349563619660083e-07, 'epoch': 2.96}\n",
      "{'loss': 0.5363, 'grad_norm': 14.199111938476562, 'learning_rate': 6.201194304088195e-07, 'epoch': 2.97}\n",
      "{'loss': 0.5239, 'grad_norm': 1.4132895469665527, 'learning_rate': 5.052824988516307e-07, 'epoch': 2.97}\n",
      "{'loss': 0.5158, 'grad_norm': 13.823080062866211, 'learning_rate': 3.904455672944419e-07, 'epoch': 2.98}\n",
      "{'loss': 0.7579, 'grad_norm': 1.079782247543335, 'learning_rate': 2.756086357372531e-07, 'epoch': 2.99}\n",
      "{'loss': 0.3224, 'grad_norm': 1.2903193235397339, 'learning_rate': 1.6077170418006432e-07, 'epoch': 2.99}\n",
      "{'loss': 0.6625, 'grad_norm': 1.033770203590393, 'learning_rate': 4.593477262287552e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08835860133b46028913a8a1a122ed4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5774235725402832, 'eval_runtime': 98.8622, 'eval_samples_per_second': 11.551, 'eval_steps_per_second': 2.893, 'epoch': 3.0}\n",
      "{'train_runtime': 9690.0691, 'train_samples_per_second': 2.003, 'train_steps_per_second': 0.501, 'train_loss': 0.6187225948845746, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85226b38bf34b9595d57df3d6213090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894abd254ec3411e9940ef1e64666483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677e9789ce0b4933b6cd6c7768b91366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb24487e4e8b4cc1aa656ef1f0eb8880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90160ad2c5a745e086765caf025aa6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/yanncauchepin/kaggle_disastertweets_robert_submission_df/commit/f6416953df4cb11f0adc0f798db5431b90e714e3', commit_message='Upload dataset', commit_description='', oid='f6416953df4cb11f0adc0f798db5431b90e714e3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/yanncauchepin/kaggle_disastertweets_robert_submission_df', endpoint='https://huggingface.co', repo_type='dataset', repo_id='yanncauchepin/kaggle_disastertweets_robert_submission_df'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "robert_tokenizer_full = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "robert_model_full = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "robert_encodings_full = robert_tokenizer_full(list(df_train_full['text']), truncation=True, padding=True, max_length=256)\n",
    "robert_labels_full = torch.tensor(list(df_train_full['target']))\n",
    "\n",
    "robert_input_ids_train_full, robert_input_ids_valid_full, \\\n",
    "robert_attention_mask_train_full, robert_attention_mask_valid_full, robert_y_train_full, robert_y_valid_full = train_test_split(\n",
    "    robert_encodings_full['input_ids'], \n",
    "    robert_encodings_full['attention_mask'], \n",
    "    robert_labels_full, \n",
    "    test_size=0.15, \n",
    "    stratify=robert_labels_full, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "robert_train_encodings_full = {\n",
    "    'input_ids': torch.tensor(robert_input_ids_train_full),\n",
    "    'attention_mask': torch.tensor(robert_attention_mask_train_full)\n",
    "}\n",
    "\n",
    "robert_valid_encodings_full = {\n",
    "    'input_ids': torch.tensor(robert_input_ids_valid_full),\n",
    "    'attention_mask': torch.tensor(robert_attention_mask_valid_full)\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True \n",
    ")\n",
    "\n",
    "robert_train_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": robert_train_encodings_full['input_ids'],\n",
    "    \"attention_mask\": robert_train_encodings_full['attention_mask'],\n",
    "    \"labels\": robert_y_train_full\n",
    "})\n",
    "\n",
    "robert_valid_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": robert_valid_encodings_full['input_ids'],\n",
    "    \"attention_mask\": robert_valid_encodings_full['attention_mask'],\n",
    "    \"labels\": robert_y_valid_full\n",
    "})\n",
    "\n",
    "robert_trainer_full = Trainer(\n",
    "    model=robert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=robert_train_dataset_full,\n",
    "    eval_dataset=robert_valid_dataset_full\n",
    ")\n",
    "\n",
    "robert_trainer_full.train()\n",
    "\n",
    "robert_predictions_full = robert_trainer_full.predict(robert_valid_dataset_full)\n",
    "robert_logits_full = robert_predictions_full.predictions\n",
    "robert_y_pred_full = np.argmax(robert_logits_full, axis=1)\n",
    "\n",
    "robert_trainer_full_assessement = evaluate_classifier(robert_y_valid_full.numpy(), robert_y_pred_full)\n",
    "\n",
    "robert_test_encodings_full = robert_tokenizer_full(list(df_test_full['text']), truncation=True, padding=True, max_length=256)\n",
    "robert_test_encodings_full = {key: torch.tensor(val) for key, val in robert_test_encodings_full.items()}\n",
    "robert_test_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": robert_test_encodings_full['input_ids'],\n",
    "    \"attention_mask\": robert_test_encodings_full['attention_mask']\n",
    "})\n",
    "\n",
    "robert_test_predictions_full = robert_trainer_full.predict(robert_test_dataset_full)\n",
    "robert_test_logits_full = robert_test_predictions_full.predictions\n",
    "robert_test_y_pred_full = np.argmax(robert_test_logits_full, axis=1)\n",
    "\n",
    "robert_test_submission_full = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': robert_test_y_pred_full.flatten()\n",
    "})\n",
    "\n",
    "robert_trainer_full.model.save_pretrained(\"kaggle_disastertweets_robert_model\")\n",
    "robert_tokenizer_full.save_pretrained(\"kaggle_disastertweets_robert_tokenizer\")\n",
    "\n",
    "robert_trainer_full.model.push_to_hub(\"yanncauchepin/kaggle_disastertweets_robert_model\")\n",
    "robert_tokenizer_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_robert_tokenizer\")\n",
    "\n",
    "hf_robert_test_submission_full = Dataset.from_pandas(robert_test_submission_full)\n",
    "hf_robert_test_submission_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_robert_submission_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/transformers/training_args.py:1574: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac260be8d5ec4ce8a24ac493940d2454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7353, 'grad_norm': 29.566905975341797, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 0.7345, 'grad_norm': 30.92800521850586, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 0.776, 'grad_norm': 34.36389923095703, 'learning_rate': 3e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6873, 'grad_norm': 20.674179077148438, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 0.7031, 'grad_norm': 21.924238204956055, 'learning_rate': 5e-06, 'epoch': 0.03}\n",
      "{'loss': 0.643, 'grad_norm': 21.9400691986084, 'learning_rate': 6e-06, 'epoch': 0.04}\n",
      "{'loss': 0.6528, 'grad_norm': 122.38347625732422, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 0.63, 'grad_norm': 65.72347259521484, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 0.5723, 'grad_norm': 21.250648498535156, 'learning_rate': 9e-06, 'epoch': 0.06}\n",
      "{'loss': 0.7273, 'grad_norm': 66.90182495117188, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5421, 'grad_norm': 37.23288345336914, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6159, 'grad_norm': 51.154029846191406, 'learning_rate': 1.2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5109, 'grad_norm': 6.846164703369141, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.4978, 'grad_norm': 15.703771591186523, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5629, 'grad_norm': 59.38235092163086, 'learning_rate': 1.5e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5046, 'grad_norm': 21.07247543334961, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5399, 'grad_norm': 142.09808349609375, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5645, 'grad_norm': 37.24563980102539, 'learning_rate': 1.8e-05, 'epoch': 0.11}\n",
      "{'loss': 0.4117, 'grad_norm': 13.045910835266113, 'learning_rate': 1.9e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4224, 'grad_norm': 5.6498332023620605, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5855, 'grad_norm': 33.104652404785156, 'learning_rate': 2.1e-05, 'epoch': 0.13}\n",
      "{'loss': 0.6956, 'grad_norm': 224.84304809570312, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.8003, 'grad_norm': 99.22398376464844, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.6006, 'grad_norm': 22.904434204101562, 'learning_rate': 2.4e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5341, 'grad_norm': 94.76133728027344, 'learning_rate': 2.5e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5093, 'grad_norm': 26.240009307861328, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6182, 'grad_norm': 43.43098068237305, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4941, 'grad_norm': 93.78580474853516, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5795, 'grad_norm': 229.84524536132812, 'learning_rate': 2.9e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4746, 'grad_norm': 74.99047088623047, 'learning_rate': 3e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7369, 'grad_norm': 28.427967071533203, 'learning_rate': 3.1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6226, 'grad_norm': 24.43445587158203, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4432, 'grad_norm': 10.809443473815918, 'learning_rate': 3.3e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5564, 'grad_norm': 32.932193756103516, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.21}\n",
      "{'loss': 0.846, 'grad_norm': 72.29618072509766, 'learning_rate': 3.5e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7475, 'grad_norm': 100.27558135986328, 'learning_rate': 3.6e-05, 'epoch': 0.22}\n",
      "{'loss': 0.8459, 'grad_norm': 14.231947898864746, 'learning_rate': 3.7e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6077, 'grad_norm': 11.098379135131836, 'learning_rate': 3.8e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6199, 'grad_norm': 38.06221008300781, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.24}\n",
      "{'loss': 0.8257, 'grad_norm': 85.57180786132812, 'learning_rate': 4e-05, 'epoch': 0.25}\n",
      "{'loss': 0.6577, 'grad_norm': 26.226612091064453, 'learning_rate': 4.1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9351, 'grad_norm': 8.865840911865234, 'learning_rate': 4.2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.7164, 'grad_norm': 102.7816162109375, 'learning_rate': 4.3e-05, 'epoch': 0.27}\n",
      "{'loss': 0.7988, 'grad_norm': 70.26097869873047, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.27}\n",
      "{'loss': 0.7294, 'grad_norm': 14.215441703796387, 'learning_rate': 4.5e-05, 'epoch': 0.28}\n",
      "{'loss': 0.8983, 'grad_norm': 27.688011169433594, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.764, 'grad_norm': 14.256537437438965, 'learning_rate': 4.7e-05, 'epoch': 0.29}\n",
      "{'loss': 0.6706, 'grad_norm': 9.972084045410156, 'learning_rate': 4.8e-05, 'epoch': 0.3}\n",
      "{'loss': 0.7694, 'grad_norm': 5.1652655601501465, 'learning_rate': 4.9e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6778, 'grad_norm': 3.193838357925415, 'learning_rate': 5e-05, 'epoch': 0.31}\n",
      "{'loss': 0.7049, 'grad_norm': 18.668054580688477, 'learning_rate': 4.988516306844282e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7052, 'grad_norm': 13.057449340820312, 'learning_rate': 4.9770326136885625e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7028, 'grad_norm': 3.8641183376312256, 'learning_rate': 4.965548920532844e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6985, 'grad_norm': 2.8733456134796143, 'learning_rate': 4.954065227377125e-05, 'epoch': 0.33}\n",
      "{'loss': 0.68, 'grad_norm': 5.632270812988281, 'learning_rate': 4.942581534221406e-05, 'epoch': 0.34}\n",
      "{'loss': 0.6936, 'grad_norm': 4.188185691833496, 'learning_rate': 4.931097841065687e-05, 'epoch': 0.35}\n",
      "{'loss': 0.6206, 'grad_norm': 2.6025867462158203, 'learning_rate': 4.9196141479099684e-05, 'epoch': 0.35}\n",
      "{'loss': 0.7729, 'grad_norm': 4.270860195159912, 'learning_rate': 4.908130454754249e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7312, 'grad_norm': 3.623110771179199, 'learning_rate': 4.89664676159853e-05, 'epoch': 0.36}\n",
      "{'loss': 0.681, 'grad_norm': 4.773443698883057, 'learning_rate': 4.8851630684428114e-05, 'epoch': 0.37}\n",
      "{'loss': 0.7163, 'grad_norm': 6.929572105407715, 'learning_rate': 4.873679375287092e-05, 'epoch': 0.38}\n",
      "{'loss': 0.5912, 'grad_norm': 5.47383975982666, 'learning_rate': 4.8621956821313736e-05, 'epoch': 0.38}\n",
      "{'loss': 0.7372, 'grad_norm': 7.775313377380371, 'learning_rate': 4.8507119889756544e-05, 'epoch': 0.39}\n",
      "{'loss': 0.7188, 'grad_norm': 3.3681790828704834, 'learning_rate': 4.839228295819936e-05, 'epoch': 0.4}\n",
      "{'loss': 0.6227, 'grad_norm': 4.293003082275391, 'learning_rate': 4.827744602664217e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7238, 'grad_norm': 6.350200653076172, 'learning_rate': 4.816260909508498e-05, 'epoch': 0.41}\n",
      "{'loss': 0.7594, 'grad_norm': 26.048288345336914, 'learning_rate': 4.8047772163527796e-05, 'epoch': 0.41}\n",
      "{'loss': 0.9049, 'grad_norm': 13.598098754882812, 'learning_rate': 4.7932935231970603e-05, 'epoch': 0.42}\n",
      "{'loss': 0.617, 'grad_norm': 9.389867782592773, 'learning_rate': 4.781809830041342e-05, 'epoch': 0.43}\n",
      "{'loss': 0.7532, 'grad_norm': 8.066390037536621, 'learning_rate': 4.7703261368856226e-05, 'epoch': 0.43}\n",
      "{'loss': 0.7297, 'grad_norm': 11.331262588500977, 'learning_rate': 4.758842443729904e-05, 'epoch': 0.44}\n",
      "{'loss': 0.7361, 'grad_norm': 10.565041542053223, 'learning_rate': 4.747358750574185e-05, 'epoch': 0.44}\n",
      "{'loss': 0.7241, 'grad_norm': 5.54034948348999, 'learning_rate': 4.735875057418466e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6449, 'grad_norm': 11.367257118225098, 'learning_rate': 4.724391364262747e-05, 'epoch': 0.46}\n",
      "{'loss': 0.9169, 'grad_norm': 7.054190635681152, 'learning_rate': 4.712907671107028e-05, 'epoch': 0.46}\n",
      "{'loss': 0.7979, 'grad_norm': 3.9206721782684326, 'learning_rate': 4.701423977951309e-05, 'epoch': 0.47}\n",
      "{'loss': 0.7128, 'grad_norm': 10.464218139648438, 'learning_rate': 4.68994028479559e-05, 'epoch': 0.48}\n",
      "{'loss': 0.7038, 'grad_norm': 8.389322280883789, 'learning_rate': 4.6784565916398715e-05, 'epoch': 0.48}\n",
      "{'loss': 0.7429, 'grad_norm': 9.634214401245117, 'learning_rate': 4.666972898484153e-05, 'epoch': 0.49}\n",
      "{'loss': 0.7327, 'grad_norm': 5.768658638000488, 'learning_rate': 4.655489205328434e-05, 'epoch': 0.49}\n",
      "{'loss': 0.6466, 'grad_norm': 2.9134795665740967, 'learning_rate': 4.644005512172715e-05, 'epoch': 0.5}\n",
      "{'loss': 0.7755, 'grad_norm': 6.680934429168701, 'learning_rate': 4.632521819016996e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6814, 'grad_norm': 9.324677467346191, 'learning_rate': 4.6210381258612774e-05, 'epoch': 0.51}\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "\n",
    "albert_tokenizer_full = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "albert_model_full = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2)\n",
    "\n",
    "albert_encodings_full = albert_tokenizer_full(list(df_train_full['text']), truncation=True, padding=True, max_length=256)\n",
    "albert_labels_full = torch.tensor(list(df_train_full['target']))\n",
    "\n",
    "albert_input_ids_train_full, albert_input_ids_valid_full, albert_token_type_ids_train_full, albert_token_type_ids_valid_full, \\\n",
    "albert_attention_mask_train_full, albert_attention_mask_valid_full, albert_y_train_full, albert_y_valid_full = train_test_split(\n",
    "    albert_encodings_full['input_ids'], \n",
    "    albert_encodings_full['token_type_ids'], \n",
    "    albert_encodings_full['attention_mask'], \n",
    "    albert_labels_full, \n",
    "    test_size=0.15, \n",
    "    stratify=albert_labels_full, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "albert_train_encodings_full = {\n",
    "    'input_ids': torch.tensor(albert_input_ids_train_full),\n",
    "    'token_type_ids': torch.tensor(albert_token_type_ids_train_full),\n",
    "    'attention_mask': torch.tensor(albert_attention_mask_train_full)\n",
    "}\n",
    "\n",
    "albert_valid_encodings_full = {\n",
    "    'input_ids': torch.tensor(albert_input_ids_valid_full),\n",
    "    'token_type_ids': torch.tensor(albert_token_type_ids_valid_full),\n",
    "    'attention_mask': torch.tensor(albert_attention_mask_valid_full)\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True \n",
    ")\n",
    "\n",
    "albert_train_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": albert_train_encodings_full['input_ids'],\n",
    "    \"attention_mask\": albert_train_encodings_full['attention_mask'],\n",
    "    \"labels\": albert_y_train_full\n",
    "})\n",
    "\n",
    "albert_valid_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": albert_valid_encodings_full['input_ids'],\n",
    "    \"attention_mask\": albert_valid_encodings_full['attention_mask'],\n",
    "    \"labels\": albert_y_valid_full\n",
    "})\n",
    "\n",
    "albert_trainer_full = Trainer(\n",
    "    model=albert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=albert_train_dataset_full,\n",
    "    eval_dataset=albert_valid_dataset_full\n",
    ")\n",
    "\n",
    "albert_trainer_full.train()\n",
    "\n",
    "albert_predictions_full = albert_trainer_full.predict(albert_valid_dataset_full)\n",
    "albert_logits_full = albert_predictions_full.predictions\n",
    "albert_y_pred_full = np.argmax(albert_logits_full, axis=1)\n",
    "\n",
    "albert_trainer_full_assessement = evaluate_classifier(albert_y_valid_full.numpy(), albert_y_pred_full)\n",
    "\n",
    "albert_test_encodings_full = albert_tokenizer_full(list(df_test_full['text']), truncation=True, padding=True, max_length=256)\n",
    "albert_test_encodings_full = {key: torch.tensor(val) for key, val in albert_test_encodings_full.items()}\n",
    "albert_test_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": albert_test_encodings_full['input_ids'],\n",
    "    \"attention_mask\": albert_test_encodings_full['attention_mask']\n",
    "})\n",
    "\n",
    "albert_test_predictions_full = albert_trainer_full.predict(albert_test_dataset_full)\n",
    "albert_test_logits_full = albert_test_predictions_full.predictions\n",
    "albert_test_y_pred_full = np.argmax(albert_test_logits_full, axis=1)\n",
    "\n",
    "albert_test_submission_full = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': albert_test_y_pred_full.flatten()\n",
    "})\n",
    "\n",
    "albert_trainer_full.model.save_pretrained(\"kaggle_disastertweets_albert_model\")\n",
    "albert_tokenizer_full.save_pretrained(\"kaggle_disastertweets_albert_tokenizer\")\n",
    "\n",
    "albert_trainer_full.model.push_to_hub(\"yanncauchepin/kaggle_disastertweets_albert_model\")\n",
    "albert_tokenizer_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_albert_tokenizer\")\n",
    "\n",
    "hf_albert_test_submission_full = Dataset.from_pandas(albert_test_submission_full)\n",
    "hf_albert_test_submission_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_albert_submission_df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
