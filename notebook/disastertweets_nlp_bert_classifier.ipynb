{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cd04af5a2442608a3fd80ee89b55d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", index_col=0)\\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", index_col=0)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", index_col=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/media/yanncauchepin/ExternalDisk/Datasets/NaturalLanguageProcessing/disaster_tweets/train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"/media/yanncauchepin/ExternalDisk/Datasets/NaturalLanguageProcessing/disaster_tweets/test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length - train 7613 - test 3263\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length - train {len(df_train)} - test {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fatalities</th>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deluge</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>armageddon</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sinking</th>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damage</th>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train  test\n",
       "keyword                \n",
       "fatalities     45     5\n",
       "deluge         42     8\n",
       "armageddon     42     8\n",
       "sinking        41     9\n",
       "damage         41     9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = pd.concat([\n",
    "    pd.DataFrame(df_train[\"keyword\"].value_counts()).rename(columns={\"count\":\"train\"}),\n",
    "    pd.DataFrame(df_test[\"keyword\"].value_counts()).rename(columns={\"count\":\"test\"})\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>104.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>71.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>London</th>\n",
       "      <td>45.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train  test\n",
       "location                  \n",
       "USA            104.0  37.0\n",
       "New York        71.0  38.0\n",
       "United States   50.0  15.0\n",
       "London          45.0  13.0\n",
       "Canada          29.0  13.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = pd.concat([\n",
    "    pd.DataFrame(df_train[\"location\"].value_counts()).rename(columns={\"count\":\"train\"}),\n",
    "    pd.DataFrame(df_test[\"location\"].value_counts()).rename(columns={\"count\":\"test\"})\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = df_train.copy()\n",
    "df_train_full['text'] = df_train_full.apply(lambda row: f\"{row['location'] or ''} {row['keyword'] or ''} {row['text']}\".strip(), axis=1)\n",
    "df_test_full = df_test.copy()\n",
    "df_test_full['text'] = df_test_full.apply(lambda row: f\"{row['location'] or ''} {row['keyword'] or ''} {row['text']}\".strip(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_texts_vec = vectorizer.fit_transform(df_train['text'])\n",
    "train_texts_vec.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_texts_tfidf = tfidf.fit_transform(train_texts_vec)\n",
    "train_texts_tfidf = train_texts_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_vec = vectorizer.transform(df_test['text'])\n",
    "test_texts_vec.todense()\n",
    "test_texts_tfdif = tfidf.transform(test_texts_vec)\n",
    "test_texts_tfdif = test_texts_tfdif.todense()\n",
    "X_test = np.asarray(test_texts_tfdif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_texts_tfidf, df_train['target'], test_size = 0.2, stratify=df_train['target'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_valid = np.asarray(X_valid)\n",
    "y_valid = np.asarray(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_full = CountVectorizer(stop_words='english')\n",
    "train_full_texts_vec = vectorizer_full.fit_transform(df_train_full['text'])\n",
    "train_full_texts_vec.todense()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_full = TfidfTransformer()\n",
    "train_full_texts_tfidf = tfidf_full.fit_transform(train_full_texts_vec)\n",
    "train_full_texts_tfidf = train_full_texts_tfidf.todense()\n",
    "\n",
    "X_full_train = np.asarray(train_full_texts_tfidf)\n",
    "y_full_train = np.asarray(df_train_full['target'])\n",
    "\n",
    "test_full_texts_vec = vectorizer_full.transform(df_test_full['text'])\n",
    "test_full_texts_vec.todense()\n",
    "test_full_texts_tfdif = tfidf_full.transform(test_full_texts_vec)\n",
    "test_full_texts_tfdif = test_full_texts_tfdif.todense()\n",
    "X_full_test = np.asarray(test_full_texts_tfdif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def evaluate_classifier(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "        'Value': [f1, precision, recall]\n",
    "    })\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
    "    \n",
    "    return metrics_df, cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.796308\n",
      "1  Precision  0.807306\n",
      "2     Recall  0.801051,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 790                  79\n",
      "Actual Positive                 224                 430)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_valid)\n",
    "\n",
    "naive_bayes_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(naive_bayes_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier_full = MultinomialNB()\n",
    "naive_bayes_classifier_full.fit(X_full_train, y_full_train)\n",
    "y_full_pred = naive_bayes_classifier_full.predict(X_full_test)\n",
    "\n",
    "naive_bayes_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.svm import SVC\\nsvc_classifier = SVC(kernel='linear')\\nsvc_classifier.fit(X_train, y_train)\\ny_pred = svc_classifier.predict(X_valid)\\n\\nsvc_classifier_assessement = evaluate_classifier(y_valid, y_pred)\\nprint(svc_classifier_assessement)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.svm import SVC\n",
    "svc_classifier = SVC(kernel='linear')\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "y_pred = svc_classifier.predict(X_valid)\n",
    "\n",
    "svc_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(svc_classifier_assessement)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.794483\n",
      "1  Precision  0.810064\n",
      "2     Recall  0.800394,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 801                  68\n",
      "Actual Positive                 236                 418)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression_classifier = LogisticRegression()\n",
    "logistic_regression_classifier.fit(X_train, y_train)\n",
    "y_pred = logistic_regression_classifier.predict(X_valid)\n",
    "\n",
    "logistic_regression_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(logistic_regression_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier_full = LogisticRegression()\n",
    "logistic_regression_classifier_full.fit(X_full_train, y_full_train)\n",
    "y_full_pred = logistic_regression_classifier_full.predict(X_full_test)\n",
    "\n",
    "logistic_regression_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.775101\n",
      "1  Precision  0.802528\n",
      "2     Recall  0.784636,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 814                  55\n",
      "Actual Positive                 273                 381)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "y_pred = random_forest_classifier.predict(X_valid)\n",
    "\n",
    "random_forest_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(random_forest_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier_full = RandomForestClassifier()\n",
    "random_forest_classifier_full.fit(X_full_train, y_full_train)\n",
    "y_full_pred = random_forest_classifier_full.predict(X_full_test)\n",
    "\n",
    "random_forest_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-10-31 20:49:37.365278: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2024-10-31 20:49:37.365795: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:135] retrieving CUDA diagnostic information for host: yanncauchepincomputing\n",
      "2024-10-31 20:49:37.365813: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:142] hostname: yanncauchepincomputing\n",
      "2024-10-31 20:49:37.366105: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:166] libcuda reported version is: 550.120.0\n",
      "2024-10-31 20:49:37.366144: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] kernel reported version is: 550.107.2\n",
      "2024-10-31 20:49:37.366157: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:252] kernel version 550.107.2 does not match DSO version 550.120.0 -- cannot find working devices in this configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 20:49:40.182417: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 520402680 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m762/762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.6765 - loss: 0.5968 - val_accuracy: 0.7984 - val_loss: 0.4524\n",
      "Epoch 2/10\n",
      "\u001b[1m762/762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8905 - loss: 0.2714 - val_accuracy: 0.7682 - val_loss: 0.5765\n",
      "Epoch 3/10\n",
      "\u001b[1m762/762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9630 - loss: 0.0922 - val_accuracy: 0.7630 - val_loss: 0.7429\n",
      "Epoch 4/10\n",
      "\u001b[1m762/762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9837 - loss: 0.0401 - val_accuracy: 0.7525 - val_loss: 0.8938\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "(      Metric     Value\n",
      "0   F1 Score  0.792614\n",
      "1  Precision  0.807421\n",
      "2     Recall  0.798424,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 798                  71\n",
      "Actual Positive                 236                 418)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "neural_network_classifier = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "neural_network_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = neural_network_classifier.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "y_pred = neural_network_classifier.predict(X_valid)\n",
    "\n",
    "neural_network_classifier_assessment = evaluate_classifier(y_valid, np.round(y_pred))\n",
    "print(neural_network_classifier_assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.6790 - loss: 0.5842 - val_accuracy: 0.8082 - val_loss: 0.4394\n",
      "Epoch 2/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8946 - loss: 0.2580 - val_accuracy: 0.7723 - val_loss: 0.5667\n",
      "Epoch 3/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9689 - loss: 0.0890 - val_accuracy: 0.7627 - val_loss: 0.7224\n",
      "Epoch 4/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9848 - loss: 0.0359 - val_accuracy: 0.7434 - val_loss: 0.9126\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "X_full_train_, X_full_valid, y_full_train_, y_full_valid = train_test_split(X_full_train, y_full_train, test_size = 0.15, stratify=y_full_train, random_state = 0)\n",
    "\n",
    "neural_network_classifier_full = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_full_train_.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "neural_network_classifier_full.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping_full = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history_full = neural_network_classifier_full.fit(\n",
    "    X_full_train_, y_full_train_,\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_full_valid, y_full_valid),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "y_full_pred = neural_network_classifier_full.predict(X_full_test)\n",
    "\n",
    "neural_network_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': np.round(y_full_pred).flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [20:58:09] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.785225\n",
      "1  Precision  0.794514\n",
      "2     Recall  0.789888,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 778                  91\n",
      "Actual Positive                 229                 425)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier(n_estimators=1000, max_depth=4, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_classifier.fit(X_train, y_train, verbose=1)\n",
    "\n",
    "y_pred = xgb_classifier.predict(X_valid)\n",
    "\n",
    "xgb_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(xgb_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:00:38] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier_full = XGBClassifier(n_estimators=1000, max_depth=4, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_classifier_full.fit(X_full_train, y_full_train, verbose=1)\n",
    "\n",
    "y_full_pred = xgb_classifier_full.predict(X_full_test)\n",
    "\n",
    "xgb_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6797045\ttotal: 74.8ms\tremaining: 1m 14s\n",
      "2:\tlearn: 0.6640753\ttotal: 127ms\tremaining: 42.2s\n",
      "4:\tlearn: 0.6527238\ttotal: 177ms\tremaining: 35.3s\n",
      "6:\tlearn: 0.6418404\ttotal: 225ms\tremaining: 31.9s\n",
      "8:\tlearn: 0.6339999\ttotal: 272ms\tremaining: 30s\n",
      "10:\tlearn: 0.6280628\ttotal: 324ms\tremaining: 29.1s\n",
      "12:\tlearn: 0.6209378\ttotal: 389ms\tremaining: 29.5s\n",
      "14:\tlearn: 0.6162400\ttotal: 437ms\tremaining: 28.7s\n",
      "16:\tlearn: 0.6115203\ttotal: 486ms\tremaining: 28.1s\n",
      "18:\tlearn: 0.6076645\ttotal: 538ms\tremaining: 27.8s\n",
      "20:\tlearn: 0.6030091\ttotal: 586ms\tremaining: 27.3s\n",
      "22:\tlearn: 0.5995534\ttotal: 636ms\tremaining: 27s\n",
      "24:\tlearn: 0.5964546\ttotal: 683ms\tremaining: 26.7s\n",
      "26:\tlearn: 0.5930054\ttotal: 732ms\tremaining: 26.4s\n",
      "28:\tlearn: 0.5902966\ttotal: 781ms\tremaining: 26.2s\n",
      "30:\tlearn: 0.5873470\ttotal: 828ms\tremaining: 25.9s\n",
      "32:\tlearn: 0.5849357\ttotal: 877ms\tremaining: 25.7s\n",
      "34:\tlearn: 0.5827788\ttotal: 926ms\tremaining: 25.5s\n",
      "36:\tlearn: 0.5798956\ttotal: 975ms\tremaining: 25.4s\n",
      "38:\tlearn: 0.5778346\ttotal: 1.02s\tremaining: 25.2s\n",
      "40:\tlearn: 0.5759892\ttotal: 1.07s\tremaining: 25.1s\n",
      "42:\tlearn: 0.5736391\ttotal: 1.12s\tremaining: 25s\n",
      "44:\tlearn: 0.5715019\ttotal: 1.17s\tremaining: 24.8s\n",
      "46:\tlearn: 0.5694379\ttotal: 1.22s\tremaining: 24.7s\n",
      "48:\tlearn: 0.5674322\ttotal: 1.26s\tremaining: 24.6s\n",
      "50:\tlearn: 0.5658421\ttotal: 1.31s\tremaining: 24.5s\n",
      "52:\tlearn: 0.5638582\ttotal: 1.36s\tremaining: 24.4s\n",
      "54:\tlearn: 0.5613693\ttotal: 1.41s\tremaining: 24.3s\n",
      "56:\tlearn: 0.5600778\ttotal: 1.46s\tremaining: 24.2s\n",
      "58:\tlearn: 0.5587724\ttotal: 1.51s\tremaining: 24.1s\n",
      "60:\tlearn: 0.5571517\ttotal: 1.56s\tremaining: 24s\n",
      "62:\tlearn: 0.5556529\ttotal: 1.6s\tremaining: 23.9s\n",
      "64:\tlearn: 0.5540510\ttotal: 1.65s\tremaining: 23.8s\n",
      "66:\tlearn: 0.5523827\ttotal: 1.7s\tremaining: 23.7s\n",
      "68:\tlearn: 0.5505863\ttotal: 1.75s\tremaining: 23.6s\n",
      "70:\tlearn: 0.5490264\ttotal: 1.8s\tremaining: 23.5s\n",
      "72:\tlearn: 0.5476734\ttotal: 1.84s\tremaining: 23.4s\n",
      "74:\tlearn: 0.5461642\ttotal: 1.89s\tremaining: 23.3s\n",
      "76:\tlearn: 0.5442920\ttotal: 1.94s\tremaining: 23.2s\n",
      "78:\tlearn: 0.5426018\ttotal: 1.98s\tremaining: 23.1s\n",
      "80:\tlearn: 0.5411093\ttotal: 2.03s\tremaining: 23s\n",
      "82:\tlearn: 0.5394318\ttotal: 2.08s\tremaining: 23s\n",
      "84:\tlearn: 0.5378561\ttotal: 2.13s\tremaining: 22.9s\n",
      "86:\tlearn: 0.5362890\ttotal: 2.17s\tremaining: 22.8s\n",
      "88:\tlearn: 0.5347047\ttotal: 2.22s\tremaining: 22.8s\n",
      "90:\tlearn: 0.5330016\ttotal: 2.27s\tremaining: 22.7s\n",
      "92:\tlearn: 0.5310676\ttotal: 2.32s\tremaining: 22.6s\n",
      "94:\tlearn: 0.5297311\ttotal: 2.37s\tremaining: 22.5s\n",
      "96:\tlearn: 0.5277773\ttotal: 2.42s\tremaining: 22.6s\n",
      "98:\tlearn: 0.5261703\ttotal: 2.47s\tremaining: 22.5s\n",
      "100:\tlearn: 0.5247077\ttotal: 2.52s\tremaining: 22.4s\n",
      "102:\tlearn: 0.5230015\ttotal: 2.57s\tremaining: 22.4s\n",
      "104:\tlearn: 0.5213563\ttotal: 2.62s\tremaining: 22.3s\n",
      "106:\tlearn: 0.5201779\ttotal: 2.67s\tremaining: 22.3s\n",
      "108:\tlearn: 0.5184250\ttotal: 2.72s\tremaining: 22.2s\n",
      "110:\tlearn: 0.5169502\ttotal: 2.77s\tremaining: 22.2s\n",
      "112:\tlearn: 0.5149358\ttotal: 2.81s\tremaining: 22.1s\n",
      "114:\tlearn: 0.5129840\ttotal: 2.86s\tremaining: 22s\n",
      "116:\tlearn: 0.5114098\ttotal: 2.91s\tremaining: 22s\n",
      "118:\tlearn: 0.5100370\ttotal: 2.96s\tremaining: 21.9s\n",
      "120:\tlearn: 0.5087392\ttotal: 3.01s\tremaining: 21.9s\n",
      "122:\tlearn: 0.5074549\ttotal: 3.06s\tremaining: 21.8s\n",
      "124:\tlearn: 0.5061063\ttotal: 3.11s\tremaining: 21.8s\n",
      "126:\tlearn: 0.5048079\ttotal: 3.16s\tremaining: 21.7s\n",
      "128:\tlearn: 0.5037329\ttotal: 3.21s\tremaining: 21.7s\n",
      "130:\tlearn: 0.5023032\ttotal: 3.26s\tremaining: 21.7s\n",
      "132:\tlearn: 0.5011450\ttotal: 3.31s\tremaining: 21.6s\n",
      "134:\tlearn: 0.4996258\ttotal: 3.36s\tremaining: 21.6s\n",
      "136:\tlearn: 0.4979305\ttotal: 3.41s\tremaining: 21.5s\n",
      "138:\tlearn: 0.4962446\ttotal: 3.46s\tremaining: 21.4s\n",
      "140:\tlearn: 0.4953408\ttotal: 3.51s\tremaining: 21.4s\n",
      "142:\tlearn: 0.4938285\ttotal: 3.57s\tremaining: 21.4s\n",
      "144:\tlearn: 0.4923820\ttotal: 3.62s\tremaining: 21.3s\n",
      "146:\tlearn: 0.4911069\ttotal: 3.67s\tremaining: 21.3s\n",
      "148:\tlearn: 0.4895391\ttotal: 3.73s\tremaining: 21.3s\n",
      "150:\tlearn: 0.4881495\ttotal: 3.78s\tremaining: 21.3s\n",
      "152:\tlearn: 0.4869094\ttotal: 3.84s\tremaining: 21.3s\n",
      "154:\tlearn: 0.4855590\ttotal: 3.89s\tremaining: 21.2s\n",
      "156:\tlearn: 0.4844663\ttotal: 3.95s\tremaining: 21.2s\n",
      "158:\tlearn: 0.4832558\ttotal: 4s\tremaining: 21.2s\n",
      "160:\tlearn: 0.4823963\ttotal: 4.06s\tremaining: 21.1s\n",
      "162:\tlearn: 0.4812555\ttotal: 4.11s\tremaining: 21.1s\n",
      "164:\tlearn: 0.4800440\ttotal: 4.16s\tremaining: 21.1s\n",
      "166:\tlearn: 0.4789399\ttotal: 4.21s\tremaining: 21s\n",
      "168:\tlearn: 0.4781308\ttotal: 4.26s\tremaining: 21s\n",
      "170:\tlearn: 0.4771756\ttotal: 4.31s\tremaining: 20.9s\n",
      "172:\tlearn: 0.4759461\ttotal: 4.36s\tremaining: 20.8s\n",
      "174:\tlearn: 0.4747981\ttotal: 4.41s\tremaining: 20.8s\n",
      "176:\tlearn: 0.4736382\ttotal: 4.47s\tremaining: 20.8s\n",
      "178:\tlearn: 0.4725821\ttotal: 4.52s\tremaining: 20.7s\n",
      "180:\tlearn: 0.4716533\ttotal: 4.57s\tremaining: 20.7s\n",
      "182:\tlearn: 0.4705809\ttotal: 4.62s\tremaining: 20.6s\n",
      "184:\tlearn: 0.4696018\ttotal: 4.67s\tremaining: 20.6s\n",
      "186:\tlearn: 0.4688866\ttotal: 4.71s\tremaining: 20.5s\n",
      "188:\tlearn: 0.4678496\ttotal: 4.76s\tremaining: 20.4s\n",
      "190:\tlearn: 0.4670510\ttotal: 4.81s\tremaining: 20.4s\n",
      "192:\tlearn: 0.4658619\ttotal: 4.86s\tremaining: 20.3s\n",
      "194:\tlearn: 0.4652432\ttotal: 4.91s\tremaining: 20.3s\n",
      "196:\tlearn: 0.4645378\ttotal: 4.96s\tremaining: 20.2s\n",
      "198:\tlearn: 0.4634856\ttotal: 5.01s\tremaining: 20.2s\n",
      "200:\tlearn: 0.4627012\ttotal: 5.06s\tremaining: 20.1s\n",
      "202:\tlearn: 0.4617999\ttotal: 5.12s\tremaining: 20.1s\n",
      "204:\tlearn: 0.4607078\ttotal: 5.17s\tremaining: 20s\n",
      "206:\tlearn: 0.4598261\ttotal: 5.22s\tremaining: 20s\n",
      "208:\tlearn: 0.4589594\ttotal: 5.27s\tremaining: 19.9s\n",
      "210:\tlearn: 0.4580456\ttotal: 5.32s\tremaining: 19.9s\n",
      "212:\tlearn: 0.4571112\ttotal: 5.37s\tremaining: 19.9s\n",
      "214:\tlearn: 0.4560788\ttotal: 5.43s\tremaining: 19.8s\n",
      "216:\tlearn: 0.4551481\ttotal: 5.47s\tremaining: 19.7s\n",
      "218:\tlearn: 0.4540412\ttotal: 5.52s\tremaining: 19.7s\n",
      "220:\tlearn: 0.4531933\ttotal: 5.57s\tremaining: 19.6s\n",
      "222:\tlearn: 0.4523048\ttotal: 5.62s\tremaining: 19.6s\n",
      "224:\tlearn: 0.4516181\ttotal: 5.67s\tremaining: 19.5s\n",
      "226:\tlearn: 0.4510290\ttotal: 5.72s\tremaining: 19.5s\n",
      "228:\tlearn: 0.4502631\ttotal: 5.76s\tremaining: 19.4s\n",
      "230:\tlearn: 0.4495500\ttotal: 5.81s\tremaining: 19.4s\n",
      "232:\tlearn: 0.4488495\ttotal: 5.86s\tremaining: 19.3s\n",
      "234:\tlearn: 0.4482463\ttotal: 5.91s\tremaining: 19.3s\n",
      "236:\tlearn: 0.4474210\ttotal: 5.96s\tremaining: 19.2s\n",
      "238:\tlearn: 0.4466364\ttotal: 6.01s\tremaining: 19.1s\n",
      "240:\tlearn: 0.4460738\ttotal: 6.06s\tremaining: 19.1s\n",
      "242:\tlearn: 0.4452302\ttotal: 6.11s\tremaining: 19s\n",
      "244:\tlearn: 0.4447196\ttotal: 6.18s\tremaining: 19s\n",
      "246:\tlearn: 0.4435851\ttotal: 6.23s\tremaining: 19s\n",
      "248:\tlearn: 0.4429541\ttotal: 6.28s\tremaining: 18.9s\n",
      "250:\tlearn: 0.4423128\ttotal: 6.33s\tremaining: 18.9s\n",
      "252:\tlearn: 0.4417453\ttotal: 6.39s\tremaining: 18.9s\n",
      "254:\tlearn: 0.4409863\ttotal: 6.44s\tremaining: 18.8s\n",
      "256:\tlearn: 0.4401960\ttotal: 6.5s\tremaining: 18.8s\n",
      "258:\tlearn: 0.4396794\ttotal: 6.55s\tremaining: 18.8s\n",
      "260:\tlearn: 0.4384640\ttotal: 6.61s\tremaining: 18.7s\n",
      "262:\tlearn: 0.4376213\ttotal: 6.65s\tremaining: 18.6s\n",
      "264:\tlearn: 0.4371905\ttotal: 6.7s\tremaining: 18.6s\n",
      "266:\tlearn: 0.4363084\ttotal: 6.75s\tremaining: 18.5s\n",
      "268:\tlearn: 0.4356339\ttotal: 6.8s\tremaining: 18.5s\n",
      "270:\tlearn: 0.4344576\ttotal: 6.86s\tremaining: 18.4s\n",
      "272:\tlearn: 0.4337178\ttotal: 6.93s\tremaining: 18.4s\n",
      "274:\tlearn: 0.4330944\ttotal: 6.99s\tremaining: 18.4s\n",
      "276:\tlearn: 0.4327276\ttotal: 7.06s\tremaining: 18.4s\n",
      "278:\tlearn: 0.4321933\ttotal: 7.12s\tremaining: 18.4s\n",
      "280:\tlearn: 0.4316327\ttotal: 7.17s\tremaining: 18.3s\n",
      "282:\tlearn: 0.4311392\ttotal: 7.23s\tremaining: 18.3s\n",
      "284:\tlearn: 0.4299317\ttotal: 7.29s\tremaining: 18.3s\n",
      "286:\tlearn: 0.4293200\ttotal: 7.34s\tremaining: 18.2s\n",
      "288:\tlearn: 0.4286959\ttotal: 7.4s\tremaining: 18.2s\n",
      "290:\tlearn: 0.4279969\ttotal: 7.45s\tremaining: 18.2s\n",
      "292:\tlearn: 0.4269520\ttotal: 7.51s\tremaining: 18.1s\n",
      "294:\tlearn: 0.4260668\ttotal: 7.56s\tremaining: 18.1s\n",
      "296:\tlearn: 0.4252018\ttotal: 7.62s\tremaining: 18s\n",
      "298:\tlearn: 0.4247962\ttotal: 7.67s\tremaining: 18s\n",
      "300:\tlearn: 0.4242414\ttotal: 7.72s\tremaining: 17.9s\n",
      "302:\tlearn: 0.4239036\ttotal: 7.76s\tremaining: 17.9s\n",
      "304:\tlearn: 0.4234882\ttotal: 7.82s\tremaining: 17.8s\n",
      "306:\tlearn: 0.4228410\ttotal: 7.87s\tremaining: 17.8s\n",
      "308:\tlearn: 0.4218682\ttotal: 7.92s\tremaining: 17.7s\n",
      "310:\tlearn: 0.4213773\ttotal: 7.96s\tremaining: 17.6s\n",
      "312:\tlearn: 0.4207828\ttotal: 8.02s\tremaining: 17.6s\n",
      "314:\tlearn: 0.4199281\ttotal: 8.07s\tremaining: 17.6s\n",
      "316:\tlearn: 0.4191391\ttotal: 8.12s\tremaining: 17.5s\n",
      "318:\tlearn: 0.4187259\ttotal: 8.17s\tremaining: 17.4s\n",
      "320:\tlearn: 0.4181838\ttotal: 8.22s\tremaining: 17.4s\n",
      "322:\tlearn: 0.4175316\ttotal: 8.27s\tremaining: 17.3s\n",
      "324:\tlearn: 0.4168182\ttotal: 8.32s\tremaining: 17.3s\n",
      "326:\tlearn: 0.4163884\ttotal: 8.37s\tremaining: 17.2s\n",
      "328:\tlearn: 0.4156723\ttotal: 8.41s\tremaining: 17.2s\n",
      "330:\tlearn: 0.4153925\ttotal: 8.46s\tremaining: 17.1s\n",
      "332:\tlearn: 0.4150304\ttotal: 8.52s\tremaining: 17.1s\n",
      "334:\tlearn: 0.4145712\ttotal: 8.57s\tremaining: 17s\n",
      "336:\tlearn: 0.4139094\ttotal: 8.62s\tremaining: 17s\n",
      "338:\tlearn: 0.4134165\ttotal: 8.67s\tremaining: 16.9s\n",
      "340:\tlearn: 0.4125950\ttotal: 8.72s\tremaining: 16.8s\n",
      "342:\tlearn: 0.4118877\ttotal: 8.77s\tremaining: 16.8s\n",
      "344:\tlearn: 0.4112381\ttotal: 8.82s\tremaining: 16.8s\n",
      "346:\tlearn: 0.4107081\ttotal: 8.87s\tremaining: 16.7s\n",
      "348:\tlearn: 0.4101342\ttotal: 8.92s\tremaining: 16.6s\n",
      "350:\tlearn: 0.4095287\ttotal: 8.97s\tremaining: 16.6s\n",
      "352:\tlearn: 0.4089310\ttotal: 9.02s\tremaining: 16.5s\n",
      "354:\tlearn: 0.4084248\ttotal: 9.07s\tremaining: 16.5s\n",
      "356:\tlearn: 0.4077483\ttotal: 9.13s\tremaining: 16.4s\n",
      "358:\tlearn: 0.4071450\ttotal: 9.18s\tremaining: 16.4s\n",
      "360:\tlearn: 0.4065326\ttotal: 9.23s\tremaining: 16.3s\n",
      "362:\tlearn: 0.4062688\ttotal: 9.28s\tremaining: 16.3s\n",
      "364:\tlearn: 0.4058553\ttotal: 9.34s\tremaining: 16.2s\n",
      "366:\tlearn: 0.4050856\ttotal: 9.39s\tremaining: 16.2s\n",
      "368:\tlearn: 0.4048899\ttotal: 9.44s\tremaining: 16.1s\n",
      "370:\tlearn: 0.4043667\ttotal: 9.49s\tremaining: 16.1s\n",
      "372:\tlearn: 0.4038234\ttotal: 9.54s\tremaining: 16s\n",
      "374:\tlearn: 0.4033705\ttotal: 9.59s\tremaining: 16s\n",
      "376:\tlearn: 0.4024821\ttotal: 9.64s\tremaining: 15.9s\n",
      "378:\tlearn: 0.4016857\ttotal: 9.69s\tremaining: 15.9s\n",
      "380:\tlearn: 0.4011507\ttotal: 9.74s\tremaining: 15.8s\n",
      "382:\tlearn: 0.4004868\ttotal: 9.79s\tremaining: 15.8s\n",
      "384:\tlearn: 0.4001387\ttotal: 9.84s\tremaining: 15.7s\n",
      "386:\tlearn: 0.3993614\ttotal: 9.89s\tremaining: 15.7s\n",
      "388:\tlearn: 0.3990022\ttotal: 9.93s\tremaining: 15.6s\n",
      "390:\tlearn: 0.3986671\ttotal: 9.98s\tremaining: 15.5s\n",
      "392:\tlearn: 0.3980145\ttotal: 10s\tremaining: 15.5s\n",
      "394:\tlearn: 0.3975518\ttotal: 10.1s\tremaining: 15.4s\n",
      "396:\tlearn: 0.3970795\ttotal: 10.1s\tremaining: 15.4s\n",
      "398:\tlearn: 0.3968297\ttotal: 10.2s\tremaining: 15.3s\n",
      "400:\tlearn: 0.3961947\ttotal: 10.2s\tremaining: 15.3s\n",
      "402:\tlearn: 0.3955686\ttotal: 10.3s\tremaining: 15.3s\n",
      "404:\tlearn: 0.3949838\ttotal: 10.4s\tremaining: 15.2s\n",
      "406:\tlearn: 0.3942833\ttotal: 10.4s\tremaining: 15.2s\n",
      "408:\tlearn: 0.3938282\ttotal: 10.5s\tremaining: 15.1s\n",
      "410:\tlearn: 0.3929693\ttotal: 10.5s\tremaining: 15.1s\n",
      "412:\tlearn: 0.3926530\ttotal: 10.6s\tremaining: 15s\n",
      "414:\tlearn: 0.3922304\ttotal: 10.6s\tremaining: 15s\n",
      "416:\tlearn: 0.3919727\ttotal: 10.7s\tremaining: 14.9s\n",
      "418:\tlearn: 0.3914102\ttotal: 10.7s\tremaining: 14.9s\n",
      "420:\tlearn: 0.3910425\ttotal: 10.8s\tremaining: 14.8s\n",
      "422:\tlearn: 0.3905410\ttotal: 10.8s\tremaining: 14.8s\n",
      "424:\tlearn: 0.3902443\ttotal: 10.9s\tremaining: 14.7s\n",
      "426:\tlearn: 0.3898271\ttotal: 10.9s\tremaining: 14.7s\n",
      "428:\tlearn: 0.3889481\ttotal: 11s\tremaining: 14.6s\n",
      "430:\tlearn: 0.3885709\ttotal: 11s\tremaining: 14.6s\n",
      "432:\tlearn: 0.3880220\ttotal: 11.1s\tremaining: 14.5s\n",
      "434:\tlearn: 0.3877116\ttotal: 11.1s\tremaining: 14.5s\n",
      "436:\tlearn: 0.3873411\ttotal: 11.2s\tremaining: 14.4s\n",
      "438:\tlearn: 0.3868701\ttotal: 11.3s\tremaining: 14.4s\n",
      "440:\tlearn: 0.3863635\ttotal: 11.3s\tremaining: 14.3s\n",
      "442:\tlearn: 0.3859407\ttotal: 11.4s\tremaining: 14.3s\n",
      "444:\tlearn: 0.3856447\ttotal: 11.4s\tremaining: 14.2s\n",
      "446:\tlearn: 0.3850862\ttotal: 11.5s\tremaining: 14.2s\n",
      "448:\tlearn: 0.3847454\ttotal: 11.5s\tremaining: 14.1s\n",
      "450:\tlearn: 0.3841863\ttotal: 11.6s\tremaining: 14.1s\n",
      "452:\tlearn: 0.3836996\ttotal: 11.6s\tremaining: 14s\n",
      "454:\tlearn: 0.3832219\ttotal: 11.7s\tremaining: 14s\n",
      "456:\tlearn: 0.3829250\ttotal: 11.8s\tremaining: 14s\n",
      "458:\tlearn: 0.3823833\ttotal: 11.8s\tremaining: 13.9s\n",
      "460:\tlearn: 0.3820568\ttotal: 11.9s\tremaining: 13.9s\n",
      "462:\tlearn: 0.3816029\ttotal: 12s\tremaining: 13.9s\n",
      "464:\tlearn: 0.3810058\ttotal: 12s\tremaining: 13.9s\n",
      "466:\tlearn: 0.3802184\ttotal: 12.1s\tremaining: 13.8s\n",
      "468:\tlearn: 0.3796452\ttotal: 12.2s\tremaining: 13.8s\n",
      "470:\tlearn: 0.3790642\ttotal: 12.2s\tremaining: 13.7s\n",
      "472:\tlearn: 0.3785171\ttotal: 12.3s\tremaining: 13.7s\n",
      "474:\tlearn: 0.3778159\ttotal: 12.3s\tremaining: 13.6s\n",
      "476:\tlearn: 0.3773601\ttotal: 12.4s\tremaining: 13.6s\n",
      "478:\tlearn: 0.3769030\ttotal: 12.4s\tremaining: 13.5s\n",
      "480:\tlearn: 0.3765387\ttotal: 12.5s\tremaining: 13.5s\n",
      "482:\tlearn: 0.3760985\ttotal: 12.5s\tremaining: 13.4s\n",
      "484:\tlearn: 0.3756708\ttotal: 12.6s\tremaining: 13.4s\n",
      "486:\tlearn: 0.3753843\ttotal: 12.6s\tremaining: 13.3s\n",
      "488:\tlearn: 0.3749032\ttotal: 12.7s\tremaining: 13.3s\n",
      "490:\tlearn: 0.3742710\ttotal: 12.8s\tremaining: 13.2s\n",
      "492:\tlearn: 0.3740540\ttotal: 12.8s\tremaining: 13.2s\n",
      "494:\tlearn: 0.3735199\ttotal: 12.8s\tremaining: 13.1s\n",
      "496:\tlearn: 0.3731156\ttotal: 12.9s\tremaining: 13.1s\n",
      "498:\tlearn: 0.3726980\ttotal: 12.9s\tremaining: 13s\n",
      "500:\tlearn: 0.3722371\ttotal: 13s\tremaining: 12.9s\n",
      "502:\tlearn: 0.3719259\ttotal: 13s\tremaining: 12.9s\n",
      "504:\tlearn: 0.3714509\ttotal: 13.1s\tremaining: 12.8s\n",
      "506:\tlearn: 0.3712383\ttotal: 13.1s\tremaining: 12.8s\n",
      "508:\tlearn: 0.3706264\ttotal: 13.2s\tremaining: 12.7s\n",
      "510:\tlearn: 0.3704458\ttotal: 13.2s\tremaining: 12.7s\n",
      "512:\tlearn: 0.3700118\ttotal: 13.3s\tremaining: 12.6s\n",
      "514:\tlearn: 0.3693146\ttotal: 13.3s\tremaining: 12.6s\n",
      "516:\tlearn: 0.3691178\ttotal: 13.4s\tremaining: 12.5s\n",
      "518:\tlearn: 0.3687245\ttotal: 13.4s\tremaining: 12.4s\n",
      "520:\tlearn: 0.3683605\ttotal: 13.5s\tremaining: 12.4s\n",
      "522:\tlearn: 0.3679832\ttotal: 13.5s\tremaining: 12.3s\n",
      "524:\tlearn: 0.3675131\ttotal: 13.6s\tremaining: 12.3s\n",
      "526:\tlearn: 0.3672720\ttotal: 13.6s\tremaining: 12.2s\n",
      "528:\tlearn: 0.3670651\ttotal: 13.7s\tremaining: 12.2s\n",
      "530:\tlearn: 0.3667500\ttotal: 13.7s\tremaining: 12.1s\n",
      "532:\tlearn: 0.3665142\ttotal: 13.8s\tremaining: 12.1s\n",
      "534:\tlearn: 0.3656974\ttotal: 13.8s\tremaining: 12s\n",
      "536:\tlearn: 0.3653853\ttotal: 13.9s\tremaining: 12s\n",
      "538:\tlearn: 0.3649804\ttotal: 13.9s\tremaining: 11.9s\n",
      "540:\tlearn: 0.3644864\ttotal: 14s\tremaining: 11.9s\n",
      "542:\tlearn: 0.3640036\ttotal: 14s\tremaining: 11.8s\n",
      "544:\tlearn: 0.3633866\ttotal: 14.1s\tremaining: 11.8s\n",
      "546:\tlearn: 0.3631059\ttotal: 14.1s\tremaining: 11.7s\n",
      "548:\tlearn: 0.3627114\ttotal: 14.2s\tremaining: 11.7s\n",
      "550:\tlearn: 0.3620214\ttotal: 14.2s\tremaining: 11.6s\n",
      "552:\tlearn: 0.3615709\ttotal: 14.3s\tremaining: 11.5s\n",
      "554:\tlearn: 0.3611750\ttotal: 14.3s\tremaining: 11.5s\n",
      "556:\tlearn: 0.3607983\ttotal: 14.4s\tremaining: 11.4s\n",
      "558:\tlearn: 0.3603221\ttotal: 14.4s\tremaining: 11.4s\n",
      "560:\tlearn: 0.3600701\ttotal: 14.5s\tremaining: 11.3s\n",
      "562:\tlearn: 0.3597542\ttotal: 14.5s\tremaining: 11.3s\n",
      "564:\tlearn: 0.3594164\ttotal: 14.6s\tremaining: 11.2s\n",
      "566:\tlearn: 0.3590706\ttotal: 14.6s\tremaining: 11.2s\n",
      "568:\tlearn: 0.3587357\ttotal: 14.7s\tremaining: 11.1s\n",
      "570:\tlearn: 0.3584035\ttotal: 14.7s\tremaining: 11.1s\n",
      "572:\tlearn: 0.3579952\ttotal: 14.8s\tremaining: 11s\n",
      "574:\tlearn: 0.3574336\ttotal: 14.8s\tremaining: 11s\n",
      "576:\tlearn: 0.3567832\ttotal: 14.9s\tremaining: 10.9s\n",
      "578:\tlearn: 0.3563742\ttotal: 14.9s\tremaining: 10.8s\n",
      "580:\tlearn: 0.3560771\ttotal: 15s\tremaining: 10.8s\n",
      "582:\tlearn: 0.3556635\ttotal: 15s\tremaining: 10.7s\n",
      "584:\tlearn: 0.3554854\ttotal: 15.1s\tremaining: 10.7s\n",
      "586:\tlearn: 0.3552355\ttotal: 15.1s\tremaining: 10.6s\n",
      "588:\tlearn: 0.3547520\ttotal: 15.2s\tremaining: 10.6s\n",
      "590:\tlearn: 0.3543196\ttotal: 15.2s\tremaining: 10.5s\n",
      "592:\tlearn: 0.3538094\ttotal: 15.3s\tremaining: 10.5s\n",
      "594:\tlearn: 0.3533993\ttotal: 15.3s\tremaining: 10.4s\n",
      "596:\tlearn: 0.3531503\ttotal: 15.4s\tremaining: 10.4s\n",
      "598:\tlearn: 0.3527857\ttotal: 15.4s\tremaining: 10.3s\n",
      "600:\tlearn: 0.3523302\ttotal: 15.5s\tremaining: 10.3s\n",
      "602:\tlearn: 0.3520125\ttotal: 15.5s\tremaining: 10.2s\n",
      "604:\tlearn: 0.3515213\ttotal: 15.6s\tremaining: 10.2s\n",
      "606:\tlearn: 0.3512257\ttotal: 15.6s\tremaining: 10.1s\n",
      "608:\tlearn: 0.3507667\ttotal: 15.7s\tremaining: 10.1s\n",
      "610:\tlearn: 0.3503367\ttotal: 15.7s\tremaining: 10s\n",
      "612:\tlearn: 0.3501135\ttotal: 15.8s\tremaining: 9.96s\n",
      "614:\tlearn: 0.3494748\ttotal: 15.8s\tremaining: 9.9s\n",
      "616:\tlearn: 0.3492959\ttotal: 15.9s\tremaining: 9.85s\n",
      "618:\tlearn: 0.3488917\ttotal: 15.9s\tremaining: 9.8s\n",
      "620:\tlearn: 0.3485412\ttotal: 16s\tremaining: 9.75s\n",
      "622:\tlearn: 0.3481091\ttotal: 16s\tremaining: 9.7s\n",
      "624:\tlearn: 0.3475918\ttotal: 16.1s\tremaining: 9.64s\n",
      "626:\tlearn: 0.3474395\ttotal: 16.1s\tremaining: 9.59s\n",
      "628:\tlearn: 0.3470363\ttotal: 16.2s\tremaining: 9.54s\n",
      "630:\tlearn: 0.3467218\ttotal: 16.2s\tremaining: 9.49s\n",
      "632:\tlearn: 0.3463769\ttotal: 16.3s\tremaining: 9.44s\n",
      "634:\tlearn: 0.3459143\ttotal: 16.3s\tremaining: 9.39s\n",
      "636:\tlearn: 0.3455431\ttotal: 16.4s\tremaining: 9.34s\n",
      "638:\tlearn: 0.3451400\ttotal: 16.4s\tremaining: 9.29s\n",
      "640:\tlearn: 0.3448651\ttotal: 16.5s\tremaining: 9.24s\n",
      "642:\tlearn: 0.3445939\ttotal: 16.5s\tremaining: 9.19s\n",
      "644:\tlearn: 0.3441331\ttotal: 16.6s\tremaining: 9.13s\n",
      "646:\tlearn: 0.3435666\ttotal: 16.7s\tremaining: 9.09s\n",
      "648:\tlearn: 0.3434018\ttotal: 16.8s\tremaining: 9.06s\n",
      "650:\tlearn: 0.3431516\ttotal: 16.8s\tremaining: 9.02s\n",
      "652:\tlearn: 0.3427715\ttotal: 16.9s\tremaining: 8.98s\n",
      "654:\tlearn: 0.3424495\ttotal: 17s\tremaining: 8.94s\n",
      "656:\tlearn: 0.3422177\ttotal: 17s\tremaining: 8.9s\n",
      "658:\tlearn: 0.3419466\ttotal: 17.1s\tremaining: 8.86s\n",
      "660:\tlearn: 0.3413374\ttotal: 17.2s\tremaining: 8.81s\n",
      "662:\tlearn: 0.3411706\ttotal: 17.2s\tremaining: 8.76s\n",
      "664:\tlearn: 0.3410090\ttotal: 17.3s\tremaining: 8.71s\n",
      "666:\tlearn: 0.3406728\ttotal: 17.3s\tremaining: 8.66s\n",
      "668:\tlearn: 0.3404609\ttotal: 17.4s\tremaining: 8.6s\n",
      "670:\tlearn: 0.3399327\ttotal: 17.4s\tremaining: 8.55s\n",
      "672:\tlearn: 0.3397584\ttotal: 17.5s\tremaining: 8.5s\n",
      "674:\tlearn: 0.3396057\ttotal: 17.5s\tremaining: 8.45s\n",
      "676:\tlearn: 0.3392844\ttotal: 17.6s\tremaining: 8.39s\n",
      "678:\tlearn: 0.3389060\ttotal: 17.6s\tremaining: 8.34s\n",
      "680:\tlearn: 0.3385440\ttotal: 17.7s\tremaining: 8.29s\n",
      "682:\tlearn: 0.3381031\ttotal: 17.7s\tremaining: 8.24s\n",
      "684:\tlearn: 0.3378332\ttotal: 17.8s\tremaining: 8.18s\n",
      "686:\tlearn: 0.3374284\ttotal: 17.8s\tremaining: 8.13s\n",
      "688:\tlearn: 0.3371233\ttotal: 17.9s\tremaining: 8.08s\n",
      "690:\tlearn: 0.3369000\ttotal: 17.9s\tremaining: 8.03s\n",
      "692:\tlearn: 0.3363059\ttotal: 18s\tremaining: 7.97s\n",
      "694:\tlearn: 0.3361083\ttotal: 18s\tremaining: 7.92s\n",
      "696:\tlearn: 0.3359546\ttotal: 18.1s\tremaining: 7.87s\n",
      "698:\tlearn: 0.3357795\ttotal: 18.1s\tremaining: 7.81s\n",
      "700:\tlearn: 0.3355921\ttotal: 18.2s\tremaining: 7.76s\n",
      "702:\tlearn: 0.3354482\ttotal: 18.2s\tremaining: 7.71s\n",
      "704:\tlearn: 0.3351271\ttotal: 18.3s\tremaining: 7.66s\n",
      "706:\tlearn: 0.3348493\ttotal: 18.3s\tremaining: 7.6s\n",
      "708:\tlearn: 0.3345872\ttotal: 18.4s\tremaining: 7.55s\n",
      "710:\tlearn: 0.3342521\ttotal: 18.4s\tremaining: 7.5s\n",
      "712:\tlearn: 0.3339186\ttotal: 18.5s\tremaining: 7.45s\n",
      "714:\tlearn: 0.3335102\ttotal: 18.5s\tremaining: 7.39s\n",
      "716:\tlearn: 0.3327611\ttotal: 18.6s\tremaining: 7.34s\n",
      "718:\tlearn: 0.3325097\ttotal: 18.6s\tremaining: 7.29s\n",
      "720:\tlearn: 0.3322119\ttotal: 18.7s\tremaining: 7.24s\n",
      "722:\tlearn: 0.3319047\ttotal: 18.8s\tremaining: 7.18s\n",
      "724:\tlearn: 0.3315558\ttotal: 18.8s\tremaining: 7.13s\n",
      "726:\tlearn: 0.3312819\ttotal: 18.9s\tremaining: 7.08s\n",
      "728:\tlearn: 0.3307901\ttotal: 18.9s\tremaining: 7.03s\n",
      "730:\tlearn: 0.3306045\ttotal: 19s\tremaining: 6.98s\n",
      "732:\tlearn: 0.3302758\ttotal: 19s\tremaining: 6.92s\n",
      "734:\tlearn: 0.3299504\ttotal: 19.1s\tremaining: 6.87s\n",
      "736:\tlearn: 0.3297748\ttotal: 19.1s\tremaining: 6.82s\n",
      "738:\tlearn: 0.3294834\ttotal: 19.2s\tremaining: 6.77s\n",
      "740:\tlearn: 0.3293198\ttotal: 19.2s\tremaining: 6.72s\n",
      "742:\tlearn: 0.3290315\ttotal: 19.3s\tremaining: 6.67s\n",
      "744:\tlearn: 0.3286660\ttotal: 19.3s\tremaining: 6.62s\n",
      "746:\tlearn: 0.3283275\ttotal: 19.4s\tremaining: 6.57s\n",
      "748:\tlearn: 0.3280723\ttotal: 19.4s\tremaining: 6.52s\n",
      "750:\tlearn: 0.3276036\ttotal: 19.5s\tremaining: 6.47s\n",
      "752:\tlearn: 0.3274398\ttotal: 19.6s\tremaining: 6.41s\n",
      "754:\tlearn: 0.3270753\ttotal: 19.6s\tremaining: 6.36s\n",
      "756:\tlearn: 0.3268829\ttotal: 19.7s\tremaining: 6.31s\n",
      "758:\tlearn: 0.3267273\ttotal: 19.7s\tremaining: 6.26s\n",
      "760:\tlearn: 0.3265524\ttotal: 19.8s\tremaining: 6.21s\n",
      "762:\tlearn: 0.3263600\ttotal: 19.8s\tremaining: 6.15s\n",
      "764:\tlearn: 0.3258057\ttotal: 19.9s\tremaining: 6.1s\n",
      "766:\tlearn: 0.3255122\ttotal: 19.9s\tremaining: 6.05s\n",
      "768:\tlearn: 0.3252129\ttotal: 20s\tremaining: 6s\n",
      "770:\tlearn: 0.3249004\ttotal: 20s\tremaining: 5.95s\n",
      "772:\tlearn: 0.3246567\ttotal: 20.1s\tremaining: 5.89s\n",
      "774:\tlearn: 0.3243239\ttotal: 20.1s\tremaining: 5.84s\n",
      "776:\tlearn: 0.3239798\ttotal: 20.2s\tremaining: 5.79s\n",
      "778:\tlearn: 0.3236609\ttotal: 20.2s\tremaining: 5.74s\n",
      "780:\tlearn: 0.3232550\ttotal: 20.3s\tremaining: 5.69s\n",
      "782:\tlearn: 0.3229705\ttotal: 20.3s\tremaining: 5.63s\n",
      "784:\tlearn: 0.3226718\ttotal: 20.4s\tremaining: 5.58s\n",
      "786:\tlearn: 0.3224739\ttotal: 20.4s\tremaining: 5.53s\n",
      "788:\tlearn: 0.3222675\ttotal: 20.5s\tremaining: 5.48s\n",
      "790:\tlearn: 0.3218166\ttotal: 20.5s\tremaining: 5.42s\n",
      "792:\tlearn: 0.3213490\ttotal: 20.6s\tremaining: 5.37s\n",
      "794:\tlearn: 0.3210303\ttotal: 20.6s\tremaining: 5.32s\n",
      "796:\tlearn: 0.3208879\ttotal: 20.7s\tremaining: 5.27s\n",
      "798:\tlearn: 0.3207136\ttotal: 20.7s\tremaining: 5.21s\n",
      "800:\tlearn: 0.3203457\ttotal: 20.8s\tremaining: 5.16s\n",
      "802:\tlearn: 0.3200520\ttotal: 20.8s\tremaining: 5.11s\n",
      "804:\tlearn: 0.3195244\ttotal: 20.9s\tremaining: 5.06s\n",
      "806:\tlearn: 0.3193709\ttotal: 20.9s\tremaining: 5.01s\n",
      "808:\tlearn: 0.3191019\ttotal: 21s\tremaining: 4.96s\n",
      "810:\tlearn: 0.3189099\ttotal: 21s\tremaining: 4.9s\n",
      "812:\tlearn: 0.3187393\ttotal: 21.1s\tremaining: 4.85s\n",
      "814:\tlearn: 0.3184847\ttotal: 21.2s\tremaining: 4.8s\n",
      "816:\tlearn: 0.3183225\ttotal: 21.2s\tremaining: 4.76s\n",
      "818:\tlearn: 0.3179534\ttotal: 21.3s\tremaining: 4.71s\n",
      "820:\tlearn: 0.3177327\ttotal: 21.4s\tremaining: 4.66s\n",
      "822:\tlearn: 0.3172480\ttotal: 21.4s\tremaining: 4.61s\n",
      "824:\tlearn: 0.3170564\ttotal: 21.5s\tremaining: 4.56s\n",
      "826:\tlearn: 0.3166852\ttotal: 21.6s\tremaining: 4.51s\n",
      "828:\tlearn: 0.3163839\ttotal: 21.6s\tremaining: 4.46s\n",
      "830:\tlearn: 0.3159508\ttotal: 21.7s\tremaining: 4.41s\n",
      "832:\tlearn: 0.3155052\ttotal: 21.7s\tremaining: 4.36s\n",
      "834:\tlearn: 0.3152428\ttotal: 21.8s\tremaining: 4.31s\n",
      "836:\tlearn: 0.3150872\ttotal: 21.8s\tremaining: 4.25s\n",
      "838:\tlearn: 0.3148626\ttotal: 21.9s\tremaining: 4.2s\n",
      "840:\tlearn: 0.3143947\ttotal: 21.9s\tremaining: 4.15s\n",
      "842:\tlearn: 0.3139806\ttotal: 22s\tremaining: 4.1s\n",
      "844:\tlearn: 0.3138392\ttotal: 22.1s\tremaining: 4.05s\n",
      "846:\tlearn: 0.3136786\ttotal: 22.1s\tremaining: 4s\n",
      "848:\tlearn: 0.3135195\ttotal: 22.2s\tremaining: 3.94s\n",
      "850:\tlearn: 0.3132600\ttotal: 22.2s\tremaining: 3.9s\n",
      "852:\tlearn: 0.3128474\ttotal: 22.3s\tremaining: 3.85s\n",
      "854:\tlearn: 0.3127157\ttotal: 22.4s\tremaining: 3.8s\n",
      "856:\tlearn: 0.3125239\ttotal: 22.5s\tremaining: 3.75s\n",
      "858:\tlearn: 0.3123184\ttotal: 22.5s\tremaining: 3.7s\n",
      "860:\tlearn: 0.3120531\ttotal: 22.6s\tremaining: 3.65s\n",
      "862:\tlearn: 0.3119095\ttotal: 22.7s\tremaining: 3.6s\n",
      "864:\tlearn: 0.3114871\ttotal: 22.7s\tremaining: 3.54s\n",
      "866:\tlearn: 0.3111806\ttotal: 22.8s\tremaining: 3.49s\n",
      "868:\tlearn: 0.3108591\ttotal: 22.8s\tremaining: 3.44s\n",
      "870:\tlearn: 0.3105429\ttotal: 22.9s\tremaining: 3.39s\n",
      "872:\tlearn: 0.3099754\ttotal: 23s\tremaining: 3.34s\n",
      "874:\tlearn: 0.3096689\ttotal: 23s\tremaining: 3.29s\n",
      "876:\tlearn: 0.3092522\ttotal: 23.1s\tremaining: 3.24s\n",
      "878:\tlearn: 0.3091028\ttotal: 23.2s\tremaining: 3.19s\n",
      "880:\tlearn: 0.3088393\ttotal: 23.2s\tremaining: 3.14s\n",
      "882:\tlearn: 0.3086566\ttotal: 23.3s\tremaining: 3.09s\n",
      "884:\tlearn: 0.3083735\ttotal: 23.4s\tremaining: 3.04s\n",
      "886:\tlearn: 0.3082480\ttotal: 23.5s\tremaining: 2.99s\n",
      "888:\tlearn: 0.3077789\ttotal: 23.5s\tremaining: 2.94s\n",
      "890:\tlearn: 0.3075991\ttotal: 23.6s\tremaining: 2.88s\n",
      "892:\tlearn: 0.3074787\ttotal: 23.6s\tremaining: 2.83s\n",
      "894:\tlearn: 0.3073516\ttotal: 23.7s\tremaining: 2.78s\n",
      "896:\tlearn: 0.3069637\ttotal: 23.7s\tremaining: 2.73s\n",
      "898:\tlearn: 0.3067219\ttotal: 23.8s\tremaining: 2.67s\n",
      "900:\tlearn: 0.3065737\ttotal: 23.8s\tremaining: 2.62s\n",
      "902:\tlearn: 0.3064335\ttotal: 23.9s\tremaining: 2.57s\n",
      "904:\tlearn: 0.3062795\ttotal: 23.9s\tremaining: 2.51s\n",
      "906:\tlearn: 0.3061049\ttotal: 24s\tremaining: 2.46s\n",
      "908:\tlearn: 0.3057701\ttotal: 24.1s\tremaining: 2.41s\n",
      "910:\tlearn: 0.3055759\ttotal: 24.1s\tremaining: 2.36s\n",
      "912:\tlearn: 0.3052570\ttotal: 24.2s\tremaining: 2.31s\n",
      "914:\tlearn: 0.3049471\ttotal: 24.3s\tremaining: 2.25s\n",
      "916:\tlearn: 0.3046149\ttotal: 24.3s\tremaining: 2.2s\n",
      "918:\tlearn: 0.3044270\ttotal: 24.4s\tremaining: 2.15s\n",
      "920:\tlearn: 0.3041018\ttotal: 24.5s\tremaining: 2.1s\n",
      "922:\tlearn: 0.3038295\ttotal: 24.6s\tremaining: 2.05s\n",
      "924:\tlearn: 0.3035613\ttotal: 24.6s\tremaining: 2s\n",
      "926:\tlearn: 0.3031499\ttotal: 24.7s\tremaining: 1.94s\n",
      "928:\tlearn: 0.3029998\ttotal: 24.8s\tremaining: 1.89s\n",
      "930:\tlearn: 0.3028384\ttotal: 24.8s\tremaining: 1.84s\n",
      "932:\tlearn: 0.3024240\ttotal: 24.9s\tremaining: 1.79s\n",
      "934:\tlearn: 0.3021882\ttotal: 25s\tremaining: 1.74s\n",
      "936:\tlearn: 0.3018578\ttotal: 25s\tremaining: 1.68s\n",
      "938:\tlearn: 0.3015503\ttotal: 25.1s\tremaining: 1.63s\n",
      "940:\tlearn: 0.3014222\ttotal: 25.2s\tremaining: 1.58s\n",
      "942:\tlearn: 0.3012375\ttotal: 25.3s\tremaining: 1.53s\n",
      "944:\tlearn: 0.3008973\ttotal: 25.4s\tremaining: 1.48s\n",
      "946:\tlearn: 0.3007791\ttotal: 25.4s\tremaining: 1.42s\n",
      "948:\tlearn: 0.3004913\ttotal: 25.5s\tremaining: 1.37s\n",
      "950:\tlearn: 0.3002397\ttotal: 25.6s\tremaining: 1.32s\n",
      "952:\tlearn: 0.3000818\ttotal: 25.7s\tremaining: 1.27s\n",
      "954:\tlearn: 0.2998012\ttotal: 25.7s\tremaining: 1.21s\n",
      "956:\tlearn: 0.2995426\ttotal: 25.8s\tremaining: 1.16s\n",
      "958:\tlearn: 0.2993134\ttotal: 25.8s\tremaining: 1.1s\n",
      "960:\tlearn: 0.2991208\ttotal: 25.9s\tremaining: 1.05s\n",
      "962:\tlearn: 0.2988175\ttotal: 26s\tremaining: 997ms\n",
      "964:\tlearn: 0.2986750\ttotal: 26s\tremaining: 944ms\n",
      "966:\tlearn: 0.2983009\ttotal: 26.1s\tremaining: 891ms\n",
      "968:\tlearn: 0.2981454\ttotal: 26.2s\tremaining: 838ms\n",
      "970:\tlearn: 0.2980126\ttotal: 26.2s\tremaining: 784ms\n",
      "972:\tlearn: 0.2978250\ttotal: 26.3s\tremaining: 730ms\n",
      "974:\tlearn: 0.2975932\ttotal: 26.4s\tremaining: 676ms\n",
      "976:\tlearn: 0.2973354\ttotal: 26.4s\tremaining: 622ms\n",
      "978:\tlearn: 0.2971365\ttotal: 26.5s\tremaining: 568ms\n",
      "980:\tlearn: 0.2969176\ttotal: 26.5s\tremaining: 514ms\n",
      "982:\tlearn: 0.2967223\ttotal: 26.6s\tremaining: 460ms\n",
      "984:\tlearn: 0.2964706\ttotal: 26.6s\tremaining: 406ms\n",
      "986:\tlearn: 0.2960170\ttotal: 26.7s\tremaining: 352ms\n",
      "988:\tlearn: 0.2957428\ttotal: 26.7s\tremaining: 298ms\n",
      "990:\tlearn: 0.2954974\ttotal: 26.8s\tremaining: 244ms\n",
      "992:\tlearn: 0.2952252\ttotal: 26.9s\tremaining: 190ms\n",
      "994:\tlearn: 0.2951031\ttotal: 27s\tremaining: 135ms\n",
      "996:\tlearn: 0.2949103\ttotal: 27s\tremaining: 81.3ms\n",
      "998:\tlearn: 0.2947807\ttotal: 27.1s\tremaining: 27.1ms\n",
      "999:\tlearn: 0.2946474\ttotal: 27.1s\tremaining: 0us\n",
      "(      Metric     Value\n",
      "0   F1 Score  0.787287\n",
      "1  Precision  0.796497\n",
      "2     Recall  0.791858,                  Predicted Negative  Predicted Positive\n",
      "Actual Negative                 779                  90\n",
      "Actual Positive                 227                 427)\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost_classifier = CatBoostClassifier(iterations=1000, depth=4, learning_rate=0.1, verbose=2)\n",
    "catboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = catboost_classifier.predict(X_valid)\n",
    "\n",
    "catboost_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(catboost_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6830062\ttotal: 28.3ms\tremaining: 28.3s\n",
      "2:\tlearn: 0.6635779\ttotal: 83.1ms\tremaining: 27.6s\n",
      "4:\tlearn: 0.6506161\ttotal: 136ms\tremaining: 27s\n",
      "6:\tlearn: 0.6415673\ttotal: 188ms\tremaining: 26.7s\n",
      "8:\tlearn: 0.6337147\ttotal: 240ms\tremaining: 26.5s\n",
      "10:\tlearn: 0.6258401\ttotal: 295ms\tremaining: 26.6s\n",
      "12:\tlearn: 0.6205170\ttotal: 347ms\tremaining: 26.3s\n",
      "14:\tlearn: 0.6152465\ttotal: 400ms\tremaining: 26.3s\n",
      "16:\tlearn: 0.6112210\ttotal: 453ms\tremaining: 26.2s\n",
      "18:\tlearn: 0.6066602\ttotal: 507ms\tremaining: 26.2s\n",
      "20:\tlearn: 0.6036622\ttotal: 559ms\tremaining: 26.1s\n",
      "22:\tlearn: 0.6004812\ttotal: 612ms\tremaining: 26s\n",
      "24:\tlearn: 0.5975389\ttotal: 665ms\tremaining: 25.9s\n",
      "26:\tlearn: 0.5946601\ttotal: 718ms\tremaining: 25.9s\n",
      "28:\tlearn: 0.5914057\ttotal: 771ms\tremaining: 25.8s\n",
      "30:\tlearn: 0.5874459\ttotal: 824ms\tremaining: 25.8s\n",
      "32:\tlearn: 0.5840149\ttotal: 880ms\tremaining: 25.8s\n",
      "34:\tlearn: 0.5819688\ttotal: 936ms\tremaining: 25.8s\n",
      "36:\tlearn: 0.5796179\ttotal: 989ms\tremaining: 25.7s\n",
      "38:\tlearn: 0.5774566\ttotal: 1.04s\tremaining: 25.7s\n",
      "40:\tlearn: 0.5752932\ttotal: 1.09s\tremaining: 25.6s\n",
      "42:\tlearn: 0.5733644\ttotal: 1.15s\tremaining: 25.5s\n",
      "44:\tlearn: 0.5713063\ttotal: 1.2s\tremaining: 25.5s\n",
      "46:\tlearn: 0.5696551\ttotal: 1.25s\tremaining: 25.5s\n",
      "48:\tlearn: 0.5676224\ttotal: 1.31s\tremaining: 25.4s\n",
      "50:\tlearn: 0.5653741\ttotal: 1.36s\tremaining: 25.4s\n",
      "52:\tlearn: 0.5636199\ttotal: 1.42s\tremaining: 25.4s\n",
      "54:\tlearn: 0.5615193\ttotal: 1.47s\tremaining: 25.3s\n",
      "56:\tlearn: 0.5595586\ttotal: 1.52s\tremaining: 25.2s\n",
      "58:\tlearn: 0.5579757\ttotal: 1.58s\tremaining: 25.2s\n",
      "60:\tlearn: 0.5567404\ttotal: 1.63s\tremaining: 25.1s\n",
      "62:\tlearn: 0.5552066\ttotal: 1.68s\tremaining: 25s\n",
      "64:\tlearn: 0.5531169\ttotal: 1.74s\tremaining: 25s\n",
      "66:\tlearn: 0.5517165\ttotal: 1.79s\tremaining: 25s\n",
      "68:\tlearn: 0.5501875\ttotal: 1.85s\tremaining: 25s\n",
      "70:\tlearn: 0.5488514\ttotal: 1.91s\tremaining: 24.9s\n",
      "72:\tlearn: 0.5472511\ttotal: 1.96s\tremaining: 24.9s\n",
      "74:\tlearn: 0.5452319\ttotal: 2.02s\tremaining: 24.9s\n",
      "76:\tlearn: 0.5439056\ttotal: 2.07s\tremaining: 24.8s\n",
      "78:\tlearn: 0.5422229\ttotal: 2.12s\tremaining: 24.7s\n",
      "80:\tlearn: 0.5405010\ttotal: 2.17s\tremaining: 24.7s\n",
      "82:\tlearn: 0.5391290\ttotal: 2.23s\tremaining: 24.6s\n",
      "84:\tlearn: 0.5376498\ttotal: 2.28s\tremaining: 24.5s\n",
      "86:\tlearn: 0.5363120\ttotal: 2.33s\tremaining: 24.5s\n",
      "88:\tlearn: 0.5346387\ttotal: 2.38s\tremaining: 24.4s\n",
      "90:\tlearn: 0.5331878\ttotal: 2.44s\tremaining: 24.3s\n",
      "92:\tlearn: 0.5314834\ttotal: 2.5s\tremaining: 24.4s\n",
      "94:\tlearn: 0.5297902\ttotal: 2.56s\tremaining: 24.4s\n",
      "96:\tlearn: 0.5284523\ttotal: 2.62s\tremaining: 24.4s\n",
      "98:\tlearn: 0.5271477\ttotal: 2.69s\tremaining: 24.5s\n",
      "100:\tlearn: 0.5255963\ttotal: 2.75s\tremaining: 24.5s\n",
      "102:\tlearn: 0.5240034\ttotal: 2.82s\tremaining: 24.5s\n",
      "104:\tlearn: 0.5227641\ttotal: 2.88s\tremaining: 24.6s\n",
      "106:\tlearn: 0.5214401\ttotal: 2.94s\tremaining: 24.5s\n",
      "108:\tlearn: 0.5201072\ttotal: 3s\tremaining: 24.5s\n",
      "110:\tlearn: 0.5185683\ttotal: 3.06s\tremaining: 24.5s\n",
      "112:\tlearn: 0.5169913\ttotal: 3.12s\tremaining: 24.5s\n",
      "114:\tlearn: 0.5156173\ttotal: 3.17s\tremaining: 24.4s\n",
      "116:\tlearn: 0.5141129\ttotal: 3.23s\tremaining: 24.4s\n",
      "118:\tlearn: 0.5127791\ttotal: 3.29s\tremaining: 24.4s\n",
      "120:\tlearn: 0.5115450\ttotal: 3.35s\tremaining: 24.4s\n",
      "122:\tlearn: 0.5101394\ttotal: 3.44s\tremaining: 24.5s\n",
      "124:\tlearn: 0.5090032\ttotal: 3.53s\tremaining: 24.7s\n",
      "126:\tlearn: 0.5078224\ttotal: 3.6s\tremaining: 24.7s\n",
      "128:\tlearn: 0.5065476\ttotal: 3.66s\tremaining: 24.7s\n",
      "130:\tlearn: 0.5052482\ttotal: 3.72s\tremaining: 24.7s\n",
      "132:\tlearn: 0.5036573\ttotal: 3.78s\tremaining: 24.6s\n",
      "134:\tlearn: 0.5023829\ttotal: 3.84s\tremaining: 24.6s\n",
      "136:\tlearn: 0.5008425\ttotal: 3.89s\tremaining: 24.5s\n",
      "138:\tlearn: 0.4995065\ttotal: 3.95s\tremaining: 24.4s\n",
      "140:\tlearn: 0.4984241\ttotal: 4.01s\tremaining: 24.4s\n",
      "142:\tlearn: 0.4973358\ttotal: 4.08s\tremaining: 24.5s\n",
      "144:\tlearn: 0.4960726\ttotal: 4.16s\tremaining: 24.5s\n",
      "146:\tlearn: 0.4951145\ttotal: 4.23s\tremaining: 24.5s\n",
      "148:\tlearn: 0.4936506\ttotal: 4.29s\tremaining: 24.5s\n",
      "150:\tlearn: 0.4924485\ttotal: 4.35s\tremaining: 24.4s\n",
      "152:\tlearn: 0.4910990\ttotal: 4.41s\tremaining: 24.4s\n",
      "154:\tlearn: 0.4898199\ttotal: 4.47s\tremaining: 24.4s\n",
      "156:\tlearn: 0.4885409\ttotal: 4.52s\tremaining: 24.3s\n",
      "158:\tlearn: 0.4874118\ttotal: 4.58s\tremaining: 24.2s\n",
      "160:\tlearn: 0.4863282\ttotal: 4.64s\tremaining: 24.2s\n",
      "162:\tlearn: 0.4853862\ttotal: 4.72s\tremaining: 24.2s\n",
      "164:\tlearn: 0.4842359\ttotal: 4.78s\tremaining: 24.2s\n",
      "166:\tlearn: 0.4829805\ttotal: 4.84s\tremaining: 24.2s\n",
      "168:\tlearn: 0.4820366\ttotal: 4.9s\tremaining: 24.1s\n",
      "170:\tlearn: 0.4810091\ttotal: 4.95s\tremaining: 24s\n",
      "172:\tlearn: 0.4797289\ttotal: 5.01s\tremaining: 23.9s\n",
      "174:\tlearn: 0.4788531\ttotal: 5.07s\tremaining: 23.9s\n",
      "176:\tlearn: 0.4779995\ttotal: 5.13s\tremaining: 23.8s\n",
      "178:\tlearn: 0.4772875\ttotal: 5.19s\tremaining: 23.8s\n",
      "180:\tlearn: 0.4761554\ttotal: 5.26s\tremaining: 23.8s\n",
      "182:\tlearn: 0.4752220\ttotal: 5.32s\tremaining: 23.7s\n",
      "184:\tlearn: 0.4742562\ttotal: 5.38s\tremaining: 23.7s\n",
      "186:\tlearn: 0.4735663\ttotal: 5.43s\tremaining: 23.6s\n",
      "188:\tlearn: 0.4725228\ttotal: 5.49s\tremaining: 23.6s\n",
      "190:\tlearn: 0.4718600\ttotal: 5.54s\tremaining: 23.5s\n",
      "192:\tlearn: 0.4709865\ttotal: 5.6s\tremaining: 23.4s\n",
      "194:\tlearn: 0.4701243\ttotal: 5.65s\tremaining: 23.3s\n",
      "196:\tlearn: 0.4689553\ttotal: 5.7s\tremaining: 23.2s\n",
      "198:\tlearn: 0.4683115\ttotal: 5.76s\tremaining: 23.2s\n",
      "200:\tlearn: 0.4675148\ttotal: 5.82s\tremaining: 23.1s\n",
      "202:\tlearn: 0.4668943\ttotal: 5.89s\tremaining: 23.1s\n",
      "204:\tlearn: 0.4658377\ttotal: 5.96s\tremaining: 23.1s\n",
      "206:\tlearn: 0.4649449\ttotal: 6.01s\tremaining: 23s\n",
      "208:\tlearn: 0.4639209\ttotal: 6.07s\tremaining: 23s\n",
      "210:\tlearn: 0.4632730\ttotal: 6.12s\tremaining: 22.9s\n",
      "212:\tlearn: 0.4620014\ttotal: 6.18s\tremaining: 22.8s\n",
      "214:\tlearn: 0.4608762\ttotal: 6.24s\tremaining: 22.8s\n",
      "216:\tlearn: 0.4601006\ttotal: 6.3s\tremaining: 22.7s\n",
      "218:\tlearn: 0.4591792\ttotal: 6.36s\tremaining: 22.7s\n",
      "220:\tlearn: 0.4584828\ttotal: 6.41s\tremaining: 22.6s\n",
      "222:\tlearn: 0.4576762\ttotal: 6.47s\tremaining: 22.5s\n",
      "224:\tlearn: 0.4568199\ttotal: 6.52s\tremaining: 22.5s\n",
      "226:\tlearn: 0.4561735\ttotal: 6.57s\tremaining: 22.4s\n",
      "228:\tlearn: 0.4554194\ttotal: 6.63s\tremaining: 22.3s\n",
      "230:\tlearn: 0.4546983\ttotal: 6.69s\tremaining: 22.3s\n",
      "232:\tlearn: 0.4536641\ttotal: 6.75s\tremaining: 22.2s\n",
      "234:\tlearn: 0.4529739\ttotal: 6.82s\tremaining: 22.2s\n",
      "236:\tlearn: 0.4521224\ttotal: 6.88s\tremaining: 22.1s\n",
      "238:\tlearn: 0.4514866\ttotal: 6.94s\tremaining: 22.1s\n",
      "240:\tlearn: 0.4504480\ttotal: 7s\tremaining: 22s\n",
      "242:\tlearn: 0.4499077\ttotal: 7.06s\tremaining: 22s\n",
      "244:\tlearn: 0.4489527\ttotal: 7.12s\tremaining: 22s\n",
      "246:\tlearn: 0.4481287\ttotal: 7.18s\tremaining: 21.9s\n",
      "248:\tlearn: 0.4474195\ttotal: 7.24s\tremaining: 21.8s\n",
      "250:\tlearn: 0.4465809\ttotal: 7.3s\tremaining: 21.8s\n",
      "252:\tlearn: 0.4459812\ttotal: 7.35s\tremaining: 21.7s\n",
      "254:\tlearn: 0.4453923\ttotal: 7.41s\tremaining: 21.7s\n",
      "256:\tlearn: 0.4446102\ttotal: 7.47s\tremaining: 21.6s\n",
      "258:\tlearn: 0.4439586\ttotal: 7.57s\tremaining: 21.7s\n",
      "260:\tlearn: 0.4431073\ttotal: 7.65s\tremaining: 21.7s\n",
      "262:\tlearn: 0.4424471\ttotal: 7.71s\tremaining: 21.6s\n",
      "264:\tlearn: 0.4419935\ttotal: 7.78s\tremaining: 21.6s\n",
      "266:\tlearn: 0.4412930\ttotal: 7.85s\tremaining: 21.6s\n",
      "268:\tlearn: 0.4402421\ttotal: 7.92s\tremaining: 21.5s\n",
      "270:\tlearn: 0.4395484\ttotal: 7.99s\tremaining: 21.5s\n",
      "272:\tlearn: 0.4389262\ttotal: 8.07s\tremaining: 21.5s\n",
      "274:\tlearn: 0.4382703\ttotal: 8.16s\tremaining: 21.5s\n",
      "276:\tlearn: 0.4376189\ttotal: 8.22s\tremaining: 21.5s\n",
      "278:\tlearn: 0.4368907\ttotal: 8.28s\tremaining: 21.4s\n",
      "280:\tlearn: 0.4362656\ttotal: 8.35s\tremaining: 21.4s\n",
      "282:\tlearn: 0.4355398\ttotal: 8.41s\tremaining: 21.3s\n",
      "284:\tlearn: 0.4350651\ttotal: 8.48s\tremaining: 21.3s\n",
      "286:\tlearn: 0.4344972\ttotal: 8.55s\tremaining: 21.2s\n",
      "288:\tlearn: 0.4335987\ttotal: 8.61s\tremaining: 21.2s\n",
      "290:\tlearn: 0.4330020\ttotal: 8.67s\tremaining: 21.1s\n",
      "292:\tlearn: 0.4321646\ttotal: 8.74s\tremaining: 21.1s\n",
      "294:\tlearn: 0.4315625\ttotal: 8.8s\tremaining: 21s\n",
      "296:\tlearn: 0.4309242\ttotal: 8.86s\tremaining: 21s\n",
      "298:\tlearn: 0.4301708\ttotal: 8.92s\tremaining: 20.9s\n",
      "300:\tlearn: 0.4293989\ttotal: 8.99s\tremaining: 20.9s\n",
      "302:\tlearn: 0.4287094\ttotal: 9.05s\tremaining: 20.8s\n",
      "304:\tlearn: 0.4283646\ttotal: 9.11s\tremaining: 20.7s\n",
      "306:\tlearn: 0.4273997\ttotal: 9.16s\tremaining: 20.7s\n",
      "308:\tlearn: 0.4265669\ttotal: 9.23s\tremaining: 20.6s\n",
      "310:\tlearn: 0.4261030\ttotal: 9.29s\tremaining: 20.6s\n",
      "312:\tlearn: 0.4255086\ttotal: 9.35s\tremaining: 20.5s\n",
      "314:\tlearn: 0.4249051\ttotal: 9.41s\tremaining: 20.5s\n",
      "316:\tlearn: 0.4242406\ttotal: 9.47s\tremaining: 20.4s\n",
      "318:\tlearn: 0.4238149\ttotal: 9.53s\tremaining: 20.4s\n",
      "320:\tlearn: 0.4235271\ttotal: 9.61s\tremaining: 20.3s\n",
      "322:\tlearn: 0.4228477\ttotal: 9.68s\tremaining: 20.3s\n",
      "324:\tlearn: 0.4221748\ttotal: 9.73s\tremaining: 20.2s\n",
      "326:\tlearn: 0.4215443\ttotal: 9.79s\tremaining: 20.1s\n",
      "328:\tlearn: 0.4212147\ttotal: 9.85s\tremaining: 20.1s\n",
      "330:\tlearn: 0.4205366\ttotal: 9.9s\tremaining: 20s\n",
      "332:\tlearn: 0.4198106\ttotal: 9.96s\tremaining: 19.9s\n",
      "334:\tlearn: 0.4195034\ttotal: 10s\tremaining: 19.9s\n",
      "336:\tlearn: 0.4190382\ttotal: 10.1s\tremaining: 19.8s\n",
      "338:\tlearn: 0.4186900\ttotal: 10.1s\tremaining: 19.7s\n",
      "340:\tlearn: 0.4180054\ttotal: 10.2s\tremaining: 19.7s\n",
      "342:\tlearn: 0.4175559\ttotal: 10.2s\tremaining: 19.6s\n",
      "344:\tlearn: 0.4169507\ttotal: 10.3s\tremaining: 19.6s\n",
      "346:\tlearn: 0.4167330\ttotal: 10.4s\tremaining: 19.5s\n",
      "348:\tlearn: 0.4162334\ttotal: 10.4s\tremaining: 19.4s\n",
      "350:\tlearn: 0.4156883\ttotal: 10.5s\tremaining: 19.4s\n",
      "352:\tlearn: 0.4148163\ttotal: 10.5s\tremaining: 19.3s\n",
      "354:\tlearn: 0.4141782\ttotal: 10.6s\tremaining: 19.3s\n",
      "356:\tlearn: 0.4136416\ttotal: 10.7s\tremaining: 19.2s\n",
      "358:\tlearn: 0.4132241\ttotal: 10.7s\tremaining: 19.2s\n",
      "360:\tlearn: 0.4129053\ttotal: 10.8s\tremaining: 19.1s\n",
      "362:\tlearn: 0.4124418\ttotal: 10.9s\tremaining: 19.1s\n",
      "364:\tlearn: 0.4118699\ttotal: 11s\tremaining: 19.1s\n",
      "366:\tlearn: 0.4114425\ttotal: 11s\tremaining: 19s\n",
      "368:\tlearn: 0.4107915\ttotal: 11.1s\tremaining: 18.9s\n",
      "370:\tlearn: 0.4104171\ttotal: 11.1s\tremaining: 18.9s\n",
      "372:\tlearn: 0.4100582\ttotal: 11.2s\tremaining: 18.8s\n",
      "374:\tlearn: 0.4095818\ttotal: 11.2s\tremaining: 18.7s\n",
      "376:\tlearn: 0.4091576\ttotal: 11.3s\tremaining: 18.7s\n",
      "378:\tlearn: 0.4086278\ttotal: 11.3s\tremaining: 18.6s\n",
      "380:\tlearn: 0.4081349\ttotal: 11.4s\tremaining: 18.5s\n",
      "382:\tlearn: 0.4076389\ttotal: 11.5s\tremaining: 18.5s\n",
      "384:\tlearn: 0.4068788\ttotal: 11.5s\tremaining: 18.4s\n",
      "386:\tlearn: 0.4063378\ttotal: 11.6s\tremaining: 18.3s\n",
      "388:\tlearn: 0.4058247\ttotal: 11.6s\tremaining: 18.3s\n",
      "390:\tlearn: 0.4055030\ttotal: 11.7s\tremaining: 18.2s\n",
      "392:\tlearn: 0.4048369\ttotal: 11.7s\tremaining: 18.1s\n",
      "394:\tlearn: 0.4042509\ttotal: 11.8s\tremaining: 18.1s\n",
      "396:\tlearn: 0.4033215\ttotal: 11.8s\tremaining: 18s\n",
      "398:\tlearn: 0.4026069\ttotal: 11.9s\tremaining: 17.9s\n",
      "400:\tlearn: 0.4020069\ttotal: 12s\tremaining: 17.9s\n",
      "402:\tlearn: 0.4015757\ttotal: 12s\tremaining: 17.8s\n",
      "404:\tlearn: 0.4011340\ttotal: 12.1s\tremaining: 17.7s\n",
      "406:\tlearn: 0.4006782\ttotal: 12.1s\tremaining: 17.7s\n",
      "408:\tlearn: 0.4003245\ttotal: 12.2s\tremaining: 17.6s\n",
      "410:\tlearn: 0.3998219\ttotal: 12.3s\tremaining: 17.6s\n",
      "412:\tlearn: 0.3991100\ttotal: 12.3s\tremaining: 17.5s\n",
      "414:\tlearn: 0.3984094\ttotal: 12.4s\tremaining: 17.5s\n",
      "416:\tlearn: 0.3981452\ttotal: 12.5s\tremaining: 17.4s\n",
      "418:\tlearn: 0.3979272\ttotal: 12.5s\tremaining: 17.4s\n",
      "420:\tlearn: 0.3972665\ttotal: 12.6s\tremaining: 17.3s\n",
      "422:\tlearn: 0.3966901\ttotal: 12.6s\tremaining: 17.2s\n",
      "424:\tlearn: 0.3961348\ttotal: 12.7s\tremaining: 17.2s\n",
      "426:\tlearn: 0.3957229\ttotal: 12.8s\tremaining: 17.1s\n",
      "428:\tlearn: 0.3950565\ttotal: 12.8s\tremaining: 17.1s\n",
      "430:\tlearn: 0.3947985\ttotal: 12.9s\tremaining: 17s\n",
      "432:\tlearn: 0.3942231\ttotal: 12.9s\tremaining: 16.9s\n",
      "434:\tlearn: 0.3940271\ttotal: 13s\tremaining: 16.9s\n",
      "436:\tlearn: 0.3936444\ttotal: 13.1s\tremaining: 16.8s\n",
      "438:\tlearn: 0.3930319\ttotal: 13.1s\tremaining: 16.8s\n",
      "440:\tlearn: 0.3924702\ttotal: 13.2s\tremaining: 16.7s\n",
      "442:\tlearn: 0.3918970\ttotal: 13.2s\tremaining: 16.6s\n",
      "444:\tlearn: 0.3915172\ttotal: 13.3s\tremaining: 16.6s\n",
      "446:\tlearn: 0.3910999\ttotal: 13.3s\tremaining: 16.5s\n",
      "448:\tlearn: 0.3906982\ttotal: 13.4s\tremaining: 16.4s\n",
      "450:\tlearn: 0.3902286\ttotal: 13.5s\tremaining: 16.4s\n",
      "452:\tlearn: 0.3897003\ttotal: 13.5s\tremaining: 16.3s\n",
      "454:\tlearn: 0.3892326\ttotal: 13.6s\tremaining: 16.3s\n",
      "456:\tlearn: 0.3884355\ttotal: 13.6s\tremaining: 16.2s\n",
      "458:\tlearn: 0.3880538\ttotal: 13.7s\tremaining: 16.2s\n",
      "460:\tlearn: 0.3876361\ttotal: 13.8s\tremaining: 16.1s\n",
      "462:\tlearn: 0.3871891\ttotal: 13.8s\tremaining: 16s\n",
      "464:\tlearn: 0.3867423\ttotal: 13.9s\tremaining: 16s\n",
      "466:\tlearn: 0.3863187\ttotal: 13.9s\tremaining: 15.9s\n",
      "468:\tlearn: 0.3861417\ttotal: 14s\tremaining: 15.8s\n",
      "470:\tlearn: 0.3855966\ttotal: 14.1s\tremaining: 15.8s\n",
      "472:\tlearn: 0.3850937\ttotal: 14.1s\tremaining: 15.7s\n",
      "474:\tlearn: 0.3844877\ttotal: 14.2s\tremaining: 15.7s\n",
      "476:\tlearn: 0.3840530\ttotal: 14.2s\tremaining: 15.6s\n",
      "478:\tlearn: 0.3837806\ttotal: 14.3s\tremaining: 15.5s\n",
      "480:\tlearn: 0.3831453\ttotal: 14.3s\tremaining: 15.5s\n",
      "482:\tlearn: 0.3828587\ttotal: 14.4s\tremaining: 15.4s\n",
      "484:\tlearn: 0.3826378\ttotal: 14.5s\tremaining: 15.3s\n",
      "486:\tlearn: 0.3821613\ttotal: 14.5s\tremaining: 15.3s\n",
      "488:\tlearn: 0.3814133\ttotal: 14.6s\tremaining: 15.2s\n",
      "490:\tlearn: 0.3810008\ttotal: 14.6s\tremaining: 15.2s\n",
      "492:\tlearn: 0.3806524\ttotal: 14.7s\tremaining: 15.1s\n",
      "494:\tlearn: 0.3802146\ttotal: 14.7s\tremaining: 15s\n",
      "496:\tlearn: 0.3797326\ttotal: 14.8s\tremaining: 15s\n",
      "498:\tlearn: 0.3795368\ttotal: 14.8s\tremaining: 14.9s\n",
      "500:\tlearn: 0.3789619\ttotal: 14.9s\tremaining: 14.8s\n",
      "502:\tlearn: 0.3787556\ttotal: 14.9s\tremaining: 14.8s\n",
      "504:\tlearn: 0.3782305\ttotal: 15s\tremaining: 14.7s\n",
      "506:\tlearn: 0.3778112\ttotal: 15.1s\tremaining: 14.6s\n",
      "508:\tlearn: 0.3776318\ttotal: 15.1s\tremaining: 14.6s\n",
      "510:\tlearn: 0.3770908\ttotal: 15.2s\tremaining: 14.5s\n",
      "512:\tlearn: 0.3765355\ttotal: 15.2s\tremaining: 14.5s\n",
      "514:\tlearn: 0.3762163\ttotal: 15.3s\tremaining: 14.4s\n",
      "516:\tlearn: 0.3757305\ttotal: 15.4s\tremaining: 14.4s\n",
      "518:\tlearn: 0.3753779\ttotal: 15.5s\tremaining: 14.4s\n",
      "520:\tlearn: 0.3749696\ttotal: 15.6s\tremaining: 14.3s\n",
      "522:\tlearn: 0.3744814\ttotal: 15.6s\tremaining: 14.3s\n",
      "524:\tlearn: 0.3742592\ttotal: 15.7s\tremaining: 14.2s\n",
      "526:\tlearn: 0.3736332\ttotal: 15.8s\tremaining: 14.2s\n",
      "528:\tlearn: 0.3732326\ttotal: 15.9s\tremaining: 14.1s\n",
      "530:\tlearn: 0.3728985\ttotal: 16s\tremaining: 14.1s\n",
      "532:\tlearn: 0.3723871\ttotal: 16.1s\tremaining: 14.1s\n",
      "534:\tlearn: 0.3720064\ttotal: 16.2s\tremaining: 14.1s\n",
      "536:\tlearn: 0.3717858\ttotal: 16.3s\tremaining: 14.1s\n",
      "538:\tlearn: 0.3713035\ttotal: 16.4s\tremaining: 14s\n",
      "540:\tlearn: 0.3707687\ttotal: 16.5s\tremaining: 14s\n",
      "542:\tlearn: 0.3703272\ttotal: 16.6s\tremaining: 13.9s\n",
      "544:\tlearn: 0.3700160\ttotal: 16.6s\tremaining: 13.9s\n",
      "546:\tlearn: 0.3695107\ttotal: 16.7s\tremaining: 13.8s\n",
      "548:\tlearn: 0.3691661\ttotal: 16.8s\tremaining: 13.8s\n",
      "550:\tlearn: 0.3688165\ttotal: 16.8s\tremaining: 13.7s\n",
      "552:\tlearn: 0.3683668\ttotal: 16.9s\tremaining: 13.7s\n",
      "554:\tlearn: 0.3678741\ttotal: 17s\tremaining: 13.6s\n",
      "556:\tlearn: 0.3675322\ttotal: 17s\tremaining: 13.5s\n",
      "558:\tlearn: 0.3672491\ttotal: 17.1s\tremaining: 13.5s\n",
      "560:\tlearn: 0.3668954\ttotal: 17.2s\tremaining: 13.4s\n",
      "562:\tlearn: 0.3663726\ttotal: 17.2s\tremaining: 13.4s\n",
      "564:\tlearn: 0.3658980\ttotal: 17.3s\tremaining: 13.3s\n",
      "566:\tlearn: 0.3656767\ttotal: 17.4s\tremaining: 13.3s\n",
      "568:\tlearn: 0.3651381\ttotal: 17.4s\tremaining: 13.2s\n",
      "570:\tlearn: 0.3646603\ttotal: 17.5s\tremaining: 13.1s\n",
      "572:\tlearn: 0.3644867\ttotal: 17.5s\tremaining: 13.1s\n",
      "574:\tlearn: 0.3642111\ttotal: 17.6s\tremaining: 13s\n",
      "576:\tlearn: 0.3638465\ttotal: 17.6s\tremaining: 12.9s\n",
      "578:\tlearn: 0.3634989\ttotal: 17.7s\tremaining: 12.9s\n",
      "580:\tlearn: 0.3631448\ttotal: 17.7s\tremaining: 12.8s\n",
      "582:\tlearn: 0.3627603\ttotal: 17.8s\tremaining: 12.7s\n",
      "584:\tlearn: 0.3623469\ttotal: 17.9s\tremaining: 12.7s\n",
      "586:\tlearn: 0.3621793\ttotal: 17.9s\tremaining: 12.6s\n",
      "588:\tlearn: 0.3618029\ttotal: 18s\tremaining: 12.5s\n",
      "590:\tlearn: 0.3614512\ttotal: 18s\tremaining: 12.5s\n",
      "592:\tlearn: 0.3611105\ttotal: 18.1s\tremaining: 12.4s\n",
      "594:\tlearn: 0.3606999\ttotal: 18.2s\tremaining: 12.4s\n",
      "596:\tlearn: 0.3603413\ttotal: 18.2s\tremaining: 12.3s\n",
      "598:\tlearn: 0.3599943\ttotal: 18.3s\tremaining: 12.3s\n",
      "600:\tlearn: 0.3596709\ttotal: 18.4s\tremaining: 12.2s\n",
      "602:\tlearn: 0.3592663\ttotal: 18.4s\tremaining: 12.1s\n",
      "604:\tlearn: 0.3588836\ttotal: 18.5s\tremaining: 12.1s\n",
      "606:\tlearn: 0.3585729\ttotal: 18.5s\tremaining: 12s\n",
      "608:\tlearn: 0.3582181\ttotal: 18.6s\tremaining: 11.9s\n",
      "610:\tlearn: 0.3577629\ttotal: 18.7s\tremaining: 11.9s\n",
      "612:\tlearn: 0.3575181\ttotal: 18.7s\tremaining: 11.8s\n",
      "614:\tlearn: 0.3573204\ttotal: 18.8s\tremaining: 11.8s\n",
      "616:\tlearn: 0.3567812\ttotal: 18.8s\tremaining: 11.7s\n",
      "618:\tlearn: 0.3562013\ttotal: 18.9s\tremaining: 11.6s\n",
      "620:\tlearn: 0.3558039\ttotal: 19s\tremaining: 11.6s\n",
      "622:\tlearn: 0.3553492\ttotal: 19.1s\tremaining: 11.5s\n",
      "624:\tlearn: 0.3547253\ttotal: 19.1s\tremaining: 11.5s\n",
      "626:\tlearn: 0.3543313\ttotal: 19.2s\tremaining: 11.4s\n",
      "628:\tlearn: 0.3541467\ttotal: 19.3s\tremaining: 11.4s\n",
      "630:\tlearn: 0.3537820\ttotal: 19.3s\tremaining: 11.3s\n",
      "632:\tlearn: 0.3534922\ttotal: 19.4s\tremaining: 11.2s\n",
      "634:\tlearn: 0.3532195\ttotal: 19.4s\tremaining: 11.2s\n",
      "636:\tlearn: 0.3528648\ttotal: 19.5s\tremaining: 11.1s\n",
      "638:\tlearn: 0.3525429\ttotal: 19.6s\tremaining: 11.1s\n",
      "640:\tlearn: 0.3522058\ttotal: 19.6s\tremaining: 11s\n",
      "642:\tlearn: 0.3520242\ttotal: 19.7s\tremaining: 10.9s\n",
      "644:\tlearn: 0.3518579\ttotal: 19.7s\tremaining: 10.9s\n",
      "646:\tlearn: 0.3515877\ttotal: 19.8s\tremaining: 10.8s\n",
      "648:\tlearn: 0.3511270\ttotal: 19.9s\tremaining: 10.7s\n",
      "650:\tlearn: 0.3508579\ttotal: 19.9s\tremaining: 10.7s\n",
      "652:\tlearn: 0.3504470\ttotal: 20s\tremaining: 10.6s\n",
      "654:\tlearn: 0.3501057\ttotal: 20s\tremaining: 10.6s\n",
      "656:\tlearn: 0.3497781\ttotal: 20.1s\tremaining: 10.5s\n",
      "658:\tlearn: 0.3494030\ttotal: 20.2s\tremaining: 10.4s\n",
      "660:\tlearn: 0.3488993\ttotal: 20.2s\tremaining: 10.4s\n",
      "662:\tlearn: 0.3486408\ttotal: 20.3s\tremaining: 10.3s\n",
      "664:\tlearn: 0.3484003\ttotal: 20.4s\tremaining: 10.3s\n",
      "666:\tlearn: 0.3482469\ttotal: 20.4s\tremaining: 10.2s\n",
      "668:\tlearn: 0.3479963\ttotal: 20.5s\tremaining: 10.1s\n",
      "670:\tlearn: 0.3476297\ttotal: 20.5s\tremaining: 10.1s\n",
      "672:\tlearn: 0.3473852\ttotal: 20.6s\tremaining: 10s\n",
      "674:\tlearn: 0.3471667\ttotal: 20.7s\tremaining: 9.96s\n",
      "676:\tlearn: 0.3469911\ttotal: 20.8s\tremaining: 9.91s\n",
      "678:\tlearn: 0.3465237\ttotal: 20.8s\tremaining: 9.85s\n",
      "680:\tlearn: 0.3462553\ttotal: 20.9s\tremaining: 9.79s\n",
      "682:\tlearn: 0.3460968\ttotal: 21s\tremaining: 9.73s\n",
      "684:\tlearn: 0.3457154\ttotal: 21s\tremaining: 9.67s\n",
      "686:\tlearn: 0.3454106\ttotal: 21.1s\tremaining: 9.6s\n",
      "688:\tlearn: 0.3451698\ttotal: 21.1s\tremaining: 9.54s\n",
      "690:\tlearn: 0.3447502\ttotal: 21.2s\tremaining: 9.47s\n",
      "692:\tlearn: 0.3443901\ttotal: 21.2s\tremaining: 9.41s\n",
      "694:\tlearn: 0.3438683\ttotal: 21.3s\tremaining: 9.35s\n",
      "696:\tlearn: 0.3434332\ttotal: 21.4s\tremaining: 9.28s\n",
      "698:\tlearn: 0.3429431\ttotal: 21.4s\tremaining: 9.22s\n",
      "700:\tlearn: 0.3427000\ttotal: 21.5s\tremaining: 9.15s\n",
      "702:\tlearn: 0.3423849\ttotal: 21.5s\tremaining: 9.09s\n",
      "704:\tlearn: 0.3421505\ttotal: 21.6s\tremaining: 9.03s\n",
      "706:\tlearn: 0.3418099\ttotal: 21.6s\tremaining: 8.96s\n",
      "708:\tlearn: 0.3415161\ttotal: 21.7s\tremaining: 8.9s\n",
      "710:\tlearn: 0.3411619\ttotal: 21.7s\tremaining: 8.84s\n",
      "712:\tlearn: 0.3409123\ttotal: 21.8s\tremaining: 8.77s\n",
      "714:\tlearn: 0.3408138\ttotal: 21.9s\tremaining: 8.71s\n",
      "716:\tlearn: 0.3404167\ttotal: 21.9s\tremaining: 8.65s\n",
      "718:\tlearn: 0.3402298\ttotal: 22s\tremaining: 8.59s\n",
      "720:\tlearn: 0.3397341\ttotal: 22s\tremaining: 8.53s\n",
      "722:\tlearn: 0.3394655\ttotal: 22.1s\tremaining: 8.46s\n",
      "724:\tlearn: 0.3391736\ttotal: 22.1s\tremaining: 8.4s\n",
      "726:\tlearn: 0.3386640\ttotal: 22.2s\tremaining: 8.34s\n",
      "728:\tlearn: 0.3382375\ttotal: 22.3s\tremaining: 8.28s\n",
      "730:\tlearn: 0.3379758\ttotal: 22.3s\tremaining: 8.21s\n",
      "732:\tlearn: 0.3374752\ttotal: 22.4s\tremaining: 8.15s\n",
      "734:\tlearn: 0.3370763\ttotal: 22.4s\tremaining: 8.09s\n",
      "736:\tlearn: 0.3366733\ttotal: 22.5s\tremaining: 8.03s\n",
      "738:\tlearn: 0.3362495\ttotal: 22.5s\tremaining: 7.96s\n",
      "740:\tlearn: 0.3359716\ttotal: 22.6s\tremaining: 7.9s\n",
      "742:\tlearn: 0.3354968\ttotal: 22.7s\tremaining: 7.83s\n",
      "744:\tlearn: 0.3350625\ttotal: 22.7s\tremaining: 7.77s\n",
      "746:\tlearn: 0.3346145\ttotal: 22.8s\tremaining: 7.71s\n",
      "748:\tlearn: 0.3342860\ttotal: 22.8s\tremaining: 7.65s\n",
      "750:\tlearn: 0.3340251\ttotal: 22.9s\tremaining: 7.58s\n",
      "752:\tlearn: 0.3337560\ttotal: 22.9s\tremaining: 7.52s\n",
      "754:\tlearn: 0.3333828\ttotal: 23s\tremaining: 7.46s\n",
      "756:\tlearn: 0.3331433\ttotal: 23s\tremaining: 7.39s\n",
      "758:\tlearn: 0.3328314\ttotal: 23.1s\tremaining: 7.33s\n",
      "760:\tlearn: 0.3324525\ttotal: 23.1s\tremaining: 7.27s\n",
      "762:\tlearn: 0.3322781\ttotal: 23.2s\tremaining: 7.21s\n",
      "764:\tlearn: 0.3320158\ttotal: 23.3s\tremaining: 7.14s\n",
      "766:\tlearn: 0.3316536\ttotal: 23.3s\tremaining: 7.08s\n",
      "768:\tlearn: 0.3312867\ttotal: 23.4s\tremaining: 7.02s\n",
      "770:\tlearn: 0.3309984\ttotal: 23.4s\tremaining: 6.96s\n",
      "772:\tlearn: 0.3307490\ttotal: 23.5s\tremaining: 6.89s\n",
      "774:\tlearn: 0.3305333\ttotal: 23.5s\tremaining: 6.83s\n",
      "776:\tlearn: 0.3303691\ttotal: 23.6s\tremaining: 6.77s\n",
      "778:\tlearn: 0.3300660\ttotal: 23.6s\tremaining: 6.71s\n",
      "780:\tlearn: 0.3296366\ttotal: 23.7s\tremaining: 6.64s\n",
      "782:\tlearn: 0.3294057\ttotal: 23.7s\tremaining: 6.58s\n",
      "784:\tlearn: 0.3289954\ttotal: 23.8s\tremaining: 6.52s\n",
      "786:\tlearn: 0.3288543\ttotal: 23.9s\tremaining: 6.46s\n",
      "788:\tlearn: 0.3287044\ttotal: 23.9s\tremaining: 6.39s\n",
      "790:\tlearn: 0.3285646\ttotal: 24s\tremaining: 6.33s\n",
      "792:\tlearn: 0.3283838\ttotal: 24s\tremaining: 6.27s\n",
      "794:\tlearn: 0.3280697\ttotal: 24.1s\tremaining: 6.21s\n",
      "796:\tlearn: 0.3278221\ttotal: 24.1s\tremaining: 6.15s\n",
      "798:\tlearn: 0.3276822\ttotal: 24.2s\tremaining: 6.09s\n",
      "800:\tlearn: 0.3273198\ttotal: 24.2s\tremaining: 6.02s\n",
      "802:\tlearn: 0.3270389\ttotal: 24.3s\tremaining: 5.96s\n",
      "804:\tlearn: 0.3267418\ttotal: 24.4s\tremaining: 5.9s\n",
      "806:\tlearn: 0.3265515\ttotal: 24.4s\tremaining: 5.84s\n",
      "808:\tlearn: 0.3263685\ttotal: 24.5s\tremaining: 5.78s\n",
      "810:\tlearn: 0.3261813\ttotal: 24.5s\tremaining: 5.71s\n",
      "812:\tlearn: 0.3257763\ttotal: 24.6s\tremaining: 5.65s\n",
      "814:\tlearn: 0.3256334\ttotal: 24.6s\tremaining: 5.59s\n",
      "816:\tlearn: 0.3254445\ttotal: 24.7s\tremaining: 5.53s\n",
      "818:\tlearn: 0.3252329\ttotal: 24.7s\tremaining: 5.46s\n",
      "820:\tlearn: 0.3248349\ttotal: 24.8s\tremaining: 5.4s\n",
      "822:\tlearn: 0.3244727\ttotal: 24.8s\tremaining: 5.34s\n",
      "824:\tlearn: 0.3241912\ttotal: 24.9s\tremaining: 5.28s\n",
      "826:\tlearn: 0.3240040\ttotal: 24.9s\tremaining: 5.22s\n",
      "828:\tlearn: 0.3237545\ttotal: 25s\tremaining: 5.16s\n",
      "830:\tlearn: 0.3233957\ttotal: 25.1s\tremaining: 5.1s\n",
      "832:\tlearn: 0.3230901\ttotal: 25.1s\tremaining: 5.04s\n",
      "834:\tlearn: 0.3229134\ttotal: 25.2s\tremaining: 4.97s\n",
      "836:\tlearn: 0.3225476\ttotal: 25.2s\tremaining: 4.92s\n",
      "838:\tlearn: 0.3224080\ttotal: 25.3s\tremaining: 4.85s\n",
      "840:\tlearn: 0.3221582\ttotal: 25.4s\tremaining: 4.79s\n",
      "842:\tlearn: 0.3219105\ttotal: 25.4s\tremaining: 4.73s\n",
      "844:\tlearn: 0.3217749\ttotal: 25.5s\tremaining: 4.67s\n",
      "846:\tlearn: 0.3215544\ttotal: 25.5s\tremaining: 4.61s\n",
      "848:\tlearn: 0.3212434\ttotal: 25.6s\tremaining: 4.55s\n",
      "850:\tlearn: 0.3211240\ttotal: 25.6s\tremaining: 4.49s\n",
      "852:\tlearn: 0.3209626\ttotal: 25.7s\tremaining: 4.43s\n",
      "854:\tlearn: 0.3208040\ttotal: 25.8s\tremaining: 4.37s\n",
      "856:\tlearn: 0.3205860\ttotal: 25.8s\tremaining: 4.3s\n",
      "858:\tlearn: 0.3204660\ttotal: 25.9s\tremaining: 4.24s\n",
      "860:\tlearn: 0.3201073\ttotal: 25.9s\tremaining: 4.18s\n",
      "862:\tlearn: 0.3197144\ttotal: 26s\tremaining: 4.12s\n",
      "864:\tlearn: 0.3195719\ttotal: 26s\tremaining: 4.06s\n",
      "866:\tlearn: 0.3192735\ttotal: 26.1s\tremaining: 4s\n",
      "868:\tlearn: 0.3189937\ttotal: 26.1s\tremaining: 3.94s\n",
      "870:\tlearn: 0.3186807\ttotal: 26.2s\tremaining: 3.88s\n",
      "872:\tlearn: 0.3184403\ttotal: 26.2s\tremaining: 3.82s\n",
      "874:\tlearn: 0.3183091\ttotal: 26.3s\tremaining: 3.76s\n",
      "876:\tlearn: 0.3180569\ttotal: 26.4s\tremaining: 3.7s\n",
      "878:\tlearn: 0.3178082\ttotal: 26.4s\tremaining: 3.63s\n",
      "880:\tlearn: 0.3176445\ttotal: 26.5s\tremaining: 3.57s\n",
      "882:\tlearn: 0.3173337\ttotal: 26.5s\tremaining: 3.51s\n",
      "884:\tlearn: 0.3168144\ttotal: 26.6s\tremaining: 3.45s\n",
      "886:\tlearn: 0.3165425\ttotal: 26.6s\tremaining: 3.39s\n",
      "888:\tlearn: 0.3162400\ttotal: 26.7s\tremaining: 3.33s\n",
      "890:\tlearn: 0.3159711\ttotal: 26.7s\tremaining: 3.27s\n",
      "892:\tlearn: 0.3157707\ttotal: 26.8s\tremaining: 3.21s\n",
      "894:\tlearn: 0.3154206\ttotal: 26.9s\tremaining: 3.15s\n",
      "896:\tlearn: 0.3151370\ttotal: 26.9s\tremaining: 3.09s\n",
      "898:\tlearn: 0.3147828\ttotal: 27s\tremaining: 3.03s\n",
      "900:\tlearn: 0.3144623\ttotal: 27s\tremaining: 2.97s\n",
      "902:\tlearn: 0.3143042\ttotal: 27.1s\tremaining: 2.91s\n",
      "904:\tlearn: 0.3138485\ttotal: 27.1s\tremaining: 2.85s\n",
      "906:\tlearn: 0.3135833\ttotal: 27.2s\tremaining: 2.79s\n",
      "908:\tlearn: 0.3132655\ttotal: 27.2s\tremaining: 2.73s\n",
      "910:\tlearn: 0.3130245\ttotal: 27.3s\tremaining: 2.67s\n",
      "912:\tlearn: 0.3128209\ttotal: 27.3s\tremaining: 2.6s\n",
      "914:\tlearn: 0.3126414\ttotal: 27.4s\tremaining: 2.54s\n",
      "916:\tlearn: 0.3123661\ttotal: 27.4s\tremaining: 2.48s\n",
      "918:\tlearn: 0.3122273\ttotal: 27.5s\tremaining: 2.42s\n",
      "920:\tlearn: 0.3120020\ttotal: 27.6s\tremaining: 2.36s\n",
      "922:\tlearn: 0.3117562\ttotal: 27.6s\tremaining: 2.3s\n",
      "924:\tlearn: 0.3115409\ttotal: 27.7s\tremaining: 2.24s\n",
      "926:\tlearn: 0.3111395\ttotal: 27.7s\tremaining: 2.18s\n",
      "928:\tlearn: 0.3109335\ttotal: 27.8s\tremaining: 2.12s\n",
      "930:\tlearn: 0.3108033\ttotal: 27.8s\tremaining: 2.06s\n",
      "932:\tlearn: 0.3105550\ttotal: 27.9s\tremaining: 2s\n",
      "934:\tlearn: 0.3102954\ttotal: 27.9s\tremaining: 1.94s\n",
      "936:\tlearn: 0.3101348\ttotal: 28s\tremaining: 1.88s\n",
      "938:\tlearn: 0.3097618\ttotal: 28s\tremaining: 1.82s\n",
      "940:\tlearn: 0.3095503\ttotal: 28.1s\tremaining: 1.76s\n",
      "942:\tlearn: 0.3093470\ttotal: 28.2s\tremaining: 1.7s\n",
      "944:\tlearn: 0.3090294\ttotal: 28.2s\tremaining: 1.64s\n",
      "946:\tlearn: 0.3087356\ttotal: 28.3s\tremaining: 1.58s\n",
      "948:\tlearn: 0.3085229\ttotal: 28.3s\tremaining: 1.52s\n",
      "950:\tlearn: 0.3082437\ttotal: 28.4s\tremaining: 1.46s\n",
      "952:\tlearn: 0.3080954\ttotal: 28.4s\tremaining: 1.4s\n",
      "954:\tlearn: 0.3077562\ttotal: 28.5s\tremaining: 1.34s\n",
      "956:\tlearn: 0.3074492\ttotal: 28.6s\tremaining: 1.28s\n",
      "958:\tlearn: 0.3071613\ttotal: 28.6s\tremaining: 1.22s\n",
      "960:\tlearn: 0.3070497\ttotal: 28.7s\tremaining: 1.16s\n",
      "962:\tlearn: 0.3068210\ttotal: 28.7s\tremaining: 1.1s\n",
      "964:\tlearn: 0.3067027\ttotal: 28.8s\tremaining: 1.04s\n",
      "966:\tlearn: 0.3065770\ttotal: 28.9s\tremaining: 985ms\n",
      "968:\tlearn: 0.3061756\ttotal: 28.9s\tremaining: 925ms\n",
      "970:\tlearn: 0.3059542\ttotal: 29s\tremaining: 865ms\n",
      "972:\tlearn: 0.3057851\ttotal: 29s\tremaining: 805ms\n",
      "974:\tlearn: 0.3053679\ttotal: 29.1s\tremaining: 745ms\n",
      "976:\tlearn: 0.3052162\ttotal: 29.1s\tremaining: 686ms\n",
      "978:\tlearn: 0.3049764\ttotal: 29.2s\tremaining: 626ms\n",
      "980:\tlearn: 0.3046473\ttotal: 29.2s\tremaining: 566ms\n",
      "982:\tlearn: 0.3043887\ttotal: 29.3s\tremaining: 506ms\n",
      "984:\tlearn: 0.3041407\ttotal: 29.3s\tremaining: 447ms\n",
      "986:\tlearn: 0.3038280\ttotal: 29.4s\tremaining: 387ms\n",
      "988:\tlearn: 0.3034946\ttotal: 29.4s\tremaining: 327ms\n",
      "990:\tlearn: 0.3031034\ttotal: 29.5s\tremaining: 268ms\n",
      "992:\tlearn: 0.3029913\ttotal: 29.5s\tremaining: 208ms\n",
      "994:\tlearn: 0.3028926\ttotal: 29.6s\tremaining: 149ms\n",
      "996:\tlearn: 0.3026011\ttotal: 29.6s\tremaining: 89.2ms\n",
      "998:\tlearn: 0.3024184\ttotal: 29.7s\tremaining: 29.7ms\n",
      "999:\tlearn: 0.3023564\ttotal: 29.7s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "catboost_classifier_full = CatBoostClassifier(iterations=1000, depth=4, learning_rate=0.1, verbose=2)\n",
    "catboost_classifier_full.fit(X_full_train, y_full_train)\n",
    "\n",
    "y_full_pred = catboost_classifier_full.predict(X_full_test)\n",
    "\n",
    "catboost_classifier_full_submission = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': y_full_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hugging_face_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 06:35:16.210948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-03 06:35:16.249496: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-03 06:35:16.264167: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-03 06:35:16.314092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-03 06:35:18.371648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/transformers/training_args.py:1574: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f28660eaf504d59a454c3443550c4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 66\u001b[0m\n\u001b[1;32m     53\u001b[0m bert_valid_dataset_full \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: bert_valid_encodings_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: bert_valid_encodings_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: bert_y_valid_full\n\u001b[1;32m     57\u001b[0m })\n\u001b[1;32m     59\u001b[0m bert_trainer_full \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     60\u001b[0m     model\u001b[38;5;241m=\u001b[39mbert_model_full,\n\u001b[1;32m     61\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     62\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mbert_train_dataset_full,\n\u001b[1;32m     63\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mbert_valid_dataset_full\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[43mbert_trainer_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m bert_predictions_full \u001b[38;5;241m=\u001b[39m bert_trainer_full\u001b[38;5;241m.\u001b[39mpredict(bert_valid_dataset_full)\n\u001b[1;32m     69\u001b[0m bert_logits_full \u001b[38;5;241m=\u001b[39m bert_predictions_full\u001b[38;5;241m.\u001b[39mpredictions\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/trainer.py:2474\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2480\u001b[0m ):\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3572\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3578\u001b[0m ):\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/trainer.py:3625\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3623\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3624\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3625\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3626\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1668\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1668\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1680\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1682\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:524\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    516\u001b[0m         hidden_states,\n\u001b[1;32m    517\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m         output_attentions,\n\u001b[1;32m    523\u001b[0m     )\n\u001b[0;32m--> 524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:467\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "bert_tokenizer_full = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model_full = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "bert_encodings_full = bert_tokenizer_full(list(df_train_full['text']), truncation=True, padding=True, max_length=256)\n",
    "bert_labels_full = torch.tensor(list(df_train_full['target']))\n",
    "\n",
    "bert_input_ids_train_full, bert_input_ids_valid_full, bert_token_type_ids_train_full, bert_token_type_ids_valid_full, \\\n",
    "bert_attention_mask_train_full, bert_attention_mask_valid_full, bert_y_train_full, bert_y_valid_full = train_test_split(\n",
    "    bert_encodings_full['input_ids'], \n",
    "    bert_encodings_full['token_type_ids'], \n",
    "    bert_encodings_full['attention_mask'], \n",
    "    bert_labels_full, \n",
    "    test_size=0.15, \n",
    "    stratify=bert_labels_full, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "bert_train_encodings_full = {\n",
    "    'input_ids': torch.tensor(bert_input_ids_train_full),\n",
    "    'token_type_ids': torch.tensor(bert_token_type_ids_train_full),\n",
    "    'attention_mask': torch.tensor(bert_attention_mask_train_full)\n",
    "}\n",
    "\n",
    "bert_valid_encodings_full = {\n",
    "    'input_ids': torch.tensor(bert_input_ids_valid_full),\n",
    "    'token_type_ids': torch.tensor(bert_token_type_ids_valid_full),\n",
    "    'attention_mask': torch.tensor(bert_attention_mask_valid_full)\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True \n",
    ")\n",
    "\n",
    "bert_train_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": bert_train_encodings_full['input_ids'],\n",
    "    \"attention_mask\": bert_train_encodings_full['attention_mask'],\n",
    "    \"labels\": bert_y_train_full\n",
    "})\n",
    "\n",
    "bert_valid_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": bert_valid_encodings_full['input_ids'],\n",
    "    \"attention_mask\": bert_valid_encodings_full['attention_mask'],\n",
    "    \"labels\": bert_y_valid_full\n",
    "})\n",
    "\n",
    "bert_trainer_full = Trainer(\n",
    "    model=bert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=bert_train_dataset_full,\n",
    "    eval_dataset=bert_valid_dataset_full\n",
    ")\n",
    "\n",
    "bert_trainer_full.train()\n",
    "\n",
    "bert_predictions_full = bert_trainer_full.predict(bert_valid_dataset_full)\n",
    "bert_logits_full = bert_predictions_full.predictions\n",
    "bert_y_pred_full = np.argmax(bert_logits_full, axis=1)\n",
    "\n",
    "bert_trainer_full_assessement = evaluate_classifier(bert_y_valid_full.numpy(), bert_y_pred_full)\n",
    "\n",
    "bert_trainer_full.save_model(\"disastertweets_bert_model\")\n",
    "bert_trainer_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_bert_model\")\n",
    "\n",
    "bert_test_encodings_full = bert_tokenizer_full(list(df_test_full['text']), truncation=True, padding=True, max_length=256)\n",
    "bert_test_encodings_full = {key: torch.tensor(val) for key, val in bert_test_encodings_full.items()}\n",
    "bert_test_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": bert_test_encodings_full['input_ids'],\n",
    "    \"attention_mask\": bert_test_encodings_full['attention_mask']\n",
    "})\n",
    "\n",
    "bert_test_predictions_full = bert_trainer_full.predict(bert_test_dataset_full)\n",
    "bert_test_logits_full = bert_test_predictions_full.predictions\n",
    "bert_test_y_pred_full = np.argmax(bert_test_logits_full, axis=1)\n",
    "\n",
    "bert_test_submission_full = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': bert_test_y_pred_full.flatten()\n",
    "})\n",
    "\n",
    "bert_trainer_full.save_pretrained(\"kaggle_disastertweets_bert_trainer\")\n",
    "bert_tokenizer_full.save_pretrained(\"kaggle_disastertweets_bert_tokenizer\")\n",
    "\n",
    "hf_bert_test_submission_full = Dataset.from_pandas(bert_test_submission_full)\n",
    "hf_bert_test_submission_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_bert_submission_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipywidgets) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
     ]
    }
   ],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599e22c89dc5451d96deaded47778b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee69e4cf2e994386b4f71aa2ed3e04ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c513a50216dd49b883f496e46a3f789a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adde816d11874adca9a9253277370ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc5712aeaf84e6ea7f981d7e3750dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/transformers/training_args.py:1574: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63ae705c8264d289f04e1e74b34be39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6998, 'grad_norm': 2.7322075366973877, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 0.7094, 'grad_norm': 2.089362382888794, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 0.713, 'grad_norm': 5.578409194946289, 'learning_rate': 3e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6946, 'grad_norm': 2.7555131912231445, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6883, 'grad_norm': 3.025681734085083, 'learning_rate': 5e-06, 'epoch': 0.03}\n",
      "{'loss': 0.6786, 'grad_norm': 3.002380609512329, 'learning_rate': 6e-06, 'epoch': 0.04}\n",
      "{'loss': 0.665, 'grad_norm': 3.166919708251953, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 0.6749, 'grad_norm': 2.206883668899536, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 0.6365, 'grad_norm': 3.53364634513855, 'learning_rate': 9e-06, 'epoch': 0.06}\n",
      "{'loss': 0.6403, 'grad_norm': 3.659611701965332, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5894, 'grad_norm': 4.000495433807373, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5711, 'grad_norm': 7.320504188537598, 'learning_rate': 1.2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5655, 'grad_norm': 3.5258562564849854, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.4973, 'grad_norm': 5.650879859924316, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.4776, 'grad_norm': 6.573753833770752, 'learning_rate': 1.5e-05, 'epoch': 0.09}\n",
      "{'loss': 0.4599, 'grad_norm': 4.868785858154297, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.1}\n",
      "{'loss': 0.4219, 'grad_norm': 17.979257583618164, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5707, 'grad_norm': 11.406912803649902, 'learning_rate': 1.8e-05, 'epoch': 0.11}\n",
      "{'loss': 0.4419, 'grad_norm': 4.654411315917969, 'learning_rate': 1.9e-05, 'epoch': 0.12}\n",
      "{'loss': 0.547, 'grad_norm': 6.798267364501953, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4385, 'grad_norm': 5.991165637969971, 'learning_rate': 2.1e-05, 'epoch': 0.13}\n",
      "{'loss': 0.482, 'grad_norm': 38.174766540527344, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.6716, 'grad_norm': 8.642657279968262, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4337, 'grad_norm': 11.874887466430664, 'learning_rate': 2.4e-05, 'epoch': 0.15}\n",
      "{'loss': 0.3256, 'grad_norm': 19.29290199279785, 'learning_rate': 2.5e-05, 'epoch': 0.15}\n",
      "{'loss': 0.8587, 'grad_norm': 1.3290554285049438, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 0.5416, 'grad_norm': 6.245454788208008, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4976, 'grad_norm': 5.457097053527832, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5923, 'grad_norm': 8.012558937072754, 'learning_rate': 2.9e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2599, 'grad_norm': 13.600244522094727, 'learning_rate': 3e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6374, 'grad_norm': 8.85171890258789, 'learning_rate': 3.1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4037, 'grad_norm': 5.064623832702637, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5014, 'grad_norm': 0.7938064336776733, 'learning_rate': 3.3e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5181, 'grad_norm': 1.9360311031341553, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3731, 'grad_norm': 39.1468505859375, 'learning_rate': 3.5e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7384, 'grad_norm': 9.59506607055664, 'learning_rate': 3.6e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7463, 'grad_norm': 45.90456771850586, 'learning_rate': 3.7e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4149, 'grad_norm': 6.682615756988525, 'learning_rate': 3.8e-05, 'epoch': 0.23}\n",
      "{'loss': 0.7331, 'grad_norm': 9.425161361694336, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4921, 'grad_norm': 4.1927618980407715, 'learning_rate': 4e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4547, 'grad_norm': 4.832698822021484, 'learning_rate': 4.1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.7566, 'grad_norm': 14.959028244018555, 'learning_rate': 4.2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5213, 'grad_norm': 8.96088981628418, 'learning_rate': 4.3e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3865, 'grad_norm': 3.2885162830352783, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.27}\n",
      "{'loss': 0.586, 'grad_norm': 19.26353645324707, 'learning_rate': 4.5e-05, 'epoch': 0.28}\n",
      "{'loss': 0.7093, 'grad_norm': 9.945012092590332, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.6965, 'grad_norm': 8.249814987182617, 'learning_rate': 4.7e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5147, 'grad_norm': 3.621894598007202, 'learning_rate': 4.8e-05, 'epoch': 0.3}\n",
      "{'loss': 0.7049, 'grad_norm': 4.232887268066406, 'learning_rate': 4.9e-05, 'epoch': 0.3}\n",
      "{'loss': 0.54, 'grad_norm': 13.29491901397705, 'learning_rate': 5e-05, 'epoch': 0.31}\n",
      "{'loss': 0.6248, 'grad_norm': 5.031297206878662, 'learning_rate': 4.988516306844282e-05, 'epoch': 0.32}\n",
      "{'loss': 0.6507, 'grad_norm': 16.424745559692383, 'learning_rate': 4.9770326136885625e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4977, 'grad_norm': 12.409104347229004, 'learning_rate': 4.965548920532844e-05, 'epoch': 0.33}\n",
      "{'loss': 0.8082, 'grad_norm': 38.25517272949219, 'learning_rate': 4.954065227377125e-05, 'epoch': 0.33}\n",
      "{'loss': 0.5153, 'grad_norm': 15.970413208007812, 'learning_rate': 4.942581534221406e-05, 'epoch': 0.34}\n",
      "{'loss': 0.6786, 'grad_norm': 4.71720552444458, 'learning_rate': 4.931097841065687e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5165, 'grad_norm': 4.996511936187744, 'learning_rate': 4.9196141479099684e-05, 'epoch': 0.35}\n",
      "{'loss': 0.7305, 'grad_norm': 11.894497871398926, 'learning_rate': 4.908130454754249e-05, 'epoch': 0.36}\n",
      "{'loss': 0.6454, 'grad_norm': 5.157669544219971, 'learning_rate': 4.89664676159853e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3865, 'grad_norm': 4.724567890167236, 'learning_rate': 4.8851630684428114e-05, 'epoch': 0.37}\n",
      "{'loss': 0.4293, 'grad_norm': 0.6301004886627197, 'learning_rate': 4.873679375287092e-05, 'epoch': 0.38}\n",
      "{'loss': 0.7196, 'grad_norm': 4.091165542602539, 'learning_rate': 4.8621956821313736e-05, 'epoch': 0.38}\n",
      "{'loss': 0.6088, 'grad_norm': 4.247843265533447, 'learning_rate': 4.8507119889756544e-05, 'epoch': 0.39}\n",
      "{'loss': 0.4334, 'grad_norm': 0.6740701198577881, 'learning_rate': 4.839228295819936e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7851, 'grad_norm': 12.883040428161621, 'learning_rate': 4.827744602664217e-05, 'epoch': 0.4}\n",
      "{'loss': 0.647, 'grad_norm': 5.450213432312012, 'learning_rate': 4.816260909508498e-05, 'epoch': 0.41}\n",
      "{'loss': 0.7182, 'grad_norm': 8.564543724060059, 'learning_rate': 4.8047772163527796e-05, 'epoch': 0.41}\n",
      "{'loss': 0.6335, 'grad_norm': 2.81571626663208, 'learning_rate': 4.7932935231970603e-05, 'epoch': 0.42}\n",
      "{'loss': 0.4388, 'grad_norm': 2.092230796813965, 'learning_rate': 4.781809830041342e-05, 'epoch': 0.43}\n",
      "{'loss': 0.4636, 'grad_norm': 2.2187511920928955, 'learning_rate': 4.7703261368856226e-05, 'epoch': 0.43}\n",
      "{'loss': 0.4653, 'grad_norm': 3.337156057357788, 'learning_rate': 4.758842443729904e-05, 'epoch': 0.44}\n",
      "{'loss': 0.8503, 'grad_norm': 8.123804092407227, 'learning_rate': 4.747358750574185e-05, 'epoch': 0.44}\n",
      "{'loss': 0.6206, 'grad_norm': 2.3243463039398193, 'learning_rate': 4.735875057418466e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3879, 'grad_norm': 5.931573390960693, 'learning_rate': 4.724391364262747e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3482, 'grad_norm': 1.0760871171951294, 'learning_rate': 4.712907671107028e-05, 'epoch': 0.46}\n",
      "{'loss': 0.8516, 'grad_norm': 11.06531047821045, 'learning_rate': 4.701423977951309e-05, 'epoch': 0.47}\n",
      "{'loss': 0.7599, 'grad_norm': 1.8937888145446777, 'learning_rate': 4.68994028479559e-05, 'epoch': 0.48}\n",
      "{'loss': 0.4422, 'grad_norm': 6.43483829498291, 'learning_rate': 4.6784565916398715e-05, 'epoch': 0.48}\n",
      "{'loss': 0.8082, 'grad_norm': 5.063107490539551, 'learning_rate': 4.666972898484153e-05, 'epoch': 0.49}\n",
      "{'loss': 0.6093, 'grad_norm': 3.4680073261260986, 'learning_rate': 4.655489205328434e-05, 'epoch': 0.49}\n",
      "{'loss': 0.4532, 'grad_norm': 3.2792110443115234, 'learning_rate': 4.644005512172715e-05, 'epoch': 0.5}\n",
      "{'loss': 0.4704, 'grad_norm': 19.838476181030273, 'learning_rate': 4.632521819016996e-05, 'epoch': 0.51}\n",
      "{'loss': 0.531, 'grad_norm': 7.388889789581299, 'learning_rate': 4.6210381258612774e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5685, 'grad_norm': 3.7443034648895264, 'learning_rate': 4.609554432705558e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4544, 'grad_norm': 0.6338512897491455, 'learning_rate': 4.59807073954984e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5683, 'grad_norm': 29.69545555114746, 'learning_rate': 4.5865870463941204e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6092, 'grad_norm': 33.74745559692383, 'learning_rate': 4.575103353238402e-05, 'epoch': 0.54}\n",
      "{'loss': 0.4947, 'grad_norm': 41.06812286376953, 'learning_rate': 4.5636196600826834e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7257, 'grad_norm': 5.221243381500244, 'learning_rate': 4.552135966926964e-05, 'epoch': 0.55}\n",
      "{'loss': 0.4364, 'grad_norm': 0.8435624241828918, 'learning_rate': 4.540652273771245e-05, 'epoch': 0.56}\n",
      "{'loss': 0.642, 'grad_norm': 19.99024772644043, 'learning_rate': 4.529168580615526e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2621, 'grad_norm': 3.832592487335205, 'learning_rate': 4.517684887459807e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5739, 'grad_norm': 18.65813446044922, 'learning_rate': 4.5062011943040886e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6407, 'grad_norm': 11.196824073791504, 'learning_rate': 4.4947175011483694e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7167, 'grad_norm': 15.619001388549805, 'learning_rate': 4.483233807992651e-05, 'epoch': 0.59}\n",
      "{'loss': 0.4454, 'grad_norm': 13.950260162353516, 'learning_rate': 4.4717501148369316e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6363, 'grad_norm': 8.491705894470215, 'learning_rate': 4.460266421681213e-05, 'epoch': 0.6}\n",
      "{'loss': 0.594, 'grad_norm': 4.317529678344727, 'learning_rate': 4.448782728525494e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5948, 'grad_norm': 12.666154861450195, 'learning_rate': 4.437299035369775e-05, 'epoch': 0.61}\n",
      "{'loss': 0.4103, 'grad_norm': 1.6672273874282837, 'learning_rate': 4.425815342214056e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2823, 'grad_norm': 0.42015424370765686, 'learning_rate': 4.4143316490583375e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5557, 'grad_norm': 0.422145277261734, 'learning_rate': 4.402847955902619e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5261, 'grad_norm': 14.380684852600098, 'learning_rate': 4.3913642627469e-05, 'epoch': 0.64}\n",
      "{'loss': 0.8507, 'grad_norm': 25.669700622558594, 'learning_rate': 4.379880569591181e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5743, 'grad_norm': 24.78628921508789, 'learning_rate': 4.368396876435461e-05, 'epoch': 0.65}\n",
      "{'loss': 0.4035, 'grad_norm': 0.4758860468864441, 'learning_rate': 4.356913183279743e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7031, 'grad_norm': 18.260852813720703, 'learning_rate': 4.345429490124024e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3919, 'grad_norm': 0.4116712510585785, 'learning_rate': 4.333945796968305e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6689, 'grad_norm': 14.094453811645508, 'learning_rate': 4.3224621038125865e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5814, 'grad_norm': 3.9864749908447266, 'learning_rate': 4.310978410656867e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5191, 'grad_norm': 3.7333056926727295, 'learning_rate': 4.299494717501149e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4645, 'grad_norm': 1.1119650602340698, 'learning_rate': 4.2880110243454295e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3507, 'grad_norm': 0.6946279406547546, 'learning_rate': 4.276527331189711e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3876, 'grad_norm': 0.6106389164924622, 'learning_rate': 4.265043638033992e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6963, 'grad_norm': 0.21120738983154297, 'learning_rate': 4.253559944878273e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3804, 'grad_norm': 4.466864109039307, 'learning_rate': 4.2420762517225546e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6058, 'grad_norm': 18.714599609375, 'learning_rate': 4.2305925585668354e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8638, 'grad_norm': 9.918678283691406, 'learning_rate': 4.219108865411117e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7212, 'grad_norm': 1.589311957359314, 'learning_rate': 4.2076251722553976e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3592, 'grad_norm': 4.978710174560547, 'learning_rate': 4.196141479099679e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5163, 'grad_norm': 13.279086112976074, 'learning_rate': 4.184657785943959e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6158, 'grad_norm': 4.770392894744873, 'learning_rate': 4.1731740927882406e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3594, 'grad_norm': 4.286405563354492, 'learning_rate': 4.161690399632522e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3503, 'grad_norm': 0.6874645352363586, 'learning_rate': 4.150206706476803e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4279, 'grad_norm': 32.87373352050781, 'learning_rate': 4.138723013321084e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4087, 'grad_norm': 18.38919448852539, 'learning_rate': 4.127239320165365e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3595, 'grad_norm': 14.802783966064453, 'learning_rate': 4.1157556270096466e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5243, 'grad_norm': 66.63653564453125, 'learning_rate': 4.1042719338539273e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7362, 'grad_norm': 40.572303771972656, 'learning_rate': 4.092788240698209e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6238, 'grad_norm': 10.630802154541016, 'learning_rate': 4.08130454754249e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3943, 'grad_norm': 35.96887969970703, 'learning_rate': 4.069820854386771e-05, 'epoch': 0.81}\n",
      "{'loss': 0.4358, 'grad_norm': 0.609516441822052, 'learning_rate': 4.0583371612310525e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8912, 'grad_norm': 7.7509541511535645, 'learning_rate': 4.046853468075333e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7376, 'grad_norm': 7.978670120239258, 'learning_rate': 4.035369774919615e-05, 'epoch': 0.83}\n",
      "{'loss': 0.4611, 'grad_norm': 10.26596450805664, 'learning_rate': 4.0238860817638955e-05, 'epoch': 0.83}\n",
      "{'loss': 0.264, 'grad_norm': 4.601107120513916, 'learning_rate': 4.012402388608177e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6327, 'grad_norm': 3.4020488262176514, 'learning_rate': 4.000918695452458e-05, 'epoch': 0.85}\n",
      "{'loss': 0.4342, 'grad_norm': 7.616700172424316, 'learning_rate': 3.9894350022967385e-05, 'epoch': 0.85}\n",
      "{'loss': 0.2821, 'grad_norm': 17.426044464111328, 'learning_rate': 3.97795130914102e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5454, 'grad_norm': 4.719128608703613, 'learning_rate': 3.966467615985301e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6017, 'grad_norm': 0.5053308606147766, 'learning_rate': 3.954983922829582e-05, 'epoch': 0.87}\n",
      "{'loss': 0.4565, 'grad_norm': 4.81946325302124, 'learning_rate': 3.943500229673863e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5062, 'grad_norm': 12.50620174407959, 'learning_rate': 3.9320165365181444e-05, 'epoch': 0.88}\n",
      "{'loss': 0.4842, 'grad_norm': 8.292021751403809, 'learning_rate': 3.920532843362425e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6891, 'grad_norm': 37.003482818603516, 'learning_rate': 3.909049150206707e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3407, 'grad_norm': 0.5422723889350891, 'learning_rate': 3.897565457050988e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5576, 'grad_norm': 2.8148505687713623, 'learning_rate': 3.886081763895269e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5412, 'grad_norm': 1.7800884246826172, 'learning_rate': 3.8745980707395504e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3998, 'grad_norm': 15.408550262451172, 'learning_rate': 3.863114377583831e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4879, 'grad_norm': 3.261014223098755, 'learning_rate': 3.8516306844281126e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7809, 'grad_norm': 6.467398643493652, 'learning_rate': 3.8401469912723934e-05, 'epoch': 0.93}\n",
      "{'loss': 0.4149, 'grad_norm': 1.5497517585754395, 'learning_rate': 3.828663298116674e-05, 'epoch': 0.94}\n",
      "{'loss': 0.4317, 'grad_norm': 25.041399002075195, 'learning_rate': 3.8171796049609556e-05, 'epoch': 0.95}\n",
      "{'loss': 0.4155, 'grad_norm': 0.9818109273910522, 'learning_rate': 3.8056959118052364e-05, 'epoch': 0.95}\n",
      "{'loss': 0.4533, 'grad_norm': 4.9849653244018555, 'learning_rate': 3.794212218649518e-05, 'epoch': 0.96}\n",
      "{'loss': 0.4179, 'grad_norm': 0.275488018989563, 'learning_rate': 3.7827285254937986e-05, 'epoch': 0.96}\n",
      "{'loss': 0.8195, 'grad_norm': 7.962958335876465, 'learning_rate': 3.77124483233808e-05, 'epoch': 0.97}\n",
      "{'loss': 0.2665, 'grad_norm': 0.29865479469299316, 'learning_rate': 3.759761139182361e-05, 'epoch': 0.98}\n",
      "{'loss': 0.6711, 'grad_norm': 5.050051212310791, 'learning_rate': 3.748277446026642e-05, 'epoch': 0.98}\n",
      "{'loss': 0.712, 'grad_norm': 24.03596305847168, 'learning_rate': 3.736793752870924e-05, 'epoch': 0.99}\n",
      "{'loss': 0.6422, 'grad_norm': 6.25531530380249, 'learning_rate': 3.7253100597152045e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0641920c81af4867b26eebc1fccf6813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.490791916847229, 'eval_runtime': 47.699, 'eval_samples_per_second': 23.942, 'eval_steps_per_second': 5.996, 'epoch': 1.0}\n",
      "{'loss': 0.3371, 'grad_norm': 5.415544033050537, 'learning_rate': 3.713826366559486e-05, 'epoch': 1.0}\n",
      "{'loss': 0.4015, 'grad_norm': 13.797213554382324, 'learning_rate': 3.702342673403767e-05, 'epoch': 1.01}\n",
      "{'loss': 0.7149, 'grad_norm': 0.8268038630485535, 'learning_rate': 3.690858980248048e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3742, 'grad_norm': 0.6877231001853943, 'learning_rate': 3.679375287092329e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3724, 'grad_norm': 4.0024003982543945, 'learning_rate': 3.6678915939366105e-05, 'epoch': 1.03}\n",
      "{'loss': 0.2719, 'grad_norm': 10.312480926513672, 'learning_rate': 3.656407900780892e-05, 'epoch': 1.03}\n",
      "{'loss': 0.7755, 'grad_norm': 9.692072868347168, 'learning_rate': 3.644924207625172e-05, 'epoch': 1.04}\n",
      "{'loss': 0.4193, 'grad_norm': 60.62513732910156, 'learning_rate': 3.6334405144694535e-05, 'epoch': 1.04}\n",
      "{'loss': 0.5707, 'grad_norm': 0.4988955855369568, 'learning_rate': 3.621956821313734e-05, 'epoch': 1.05}\n",
      "{'loss': 0.6693, 'grad_norm': 7.251408100128174, 'learning_rate': 3.610473128158016e-05, 'epoch': 1.06}\n",
      "{'loss': 0.215, 'grad_norm': 11.262289047241211, 'learning_rate': 3.5989894350022965e-05, 'epoch': 1.06}\n",
      "{'loss': 0.5056, 'grad_norm': 0.8123831748962402, 'learning_rate': 3.587505741846578e-05, 'epoch': 1.07}\n",
      "{'loss': 0.1835, 'grad_norm': 0.2286108285188675, 'learning_rate': 3.5760220486908594e-05, 'epoch': 1.08}\n",
      "{'loss': 0.2461, 'grad_norm': 0.24842803180217743, 'learning_rate': 3.56453835553514e-05, 'epoch': 1.08}\n",
      "{'loss': 0.3312, 'grad_norm': 29.762439727783203, 'learning_rate': 3.5530546623794216e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3073, 'grad_norm': 4.5777587890625, 'learning_rate': 3.5415709692237024e-05, 'epoch': 1.09}\n",
      "{'loss': 0.7416, 'grad_norm': 15.725464820861816, 'learning_rate': 3.530087276067984e-05, 'epoch': 1.1}\n",
      "{'loss': 0.4719, 'grad_norm': 1.502712368965149, 'learning_rate': 3.5186035829122646e-05, 'epoch': 1.11}\n",
      "{'loss': 0.7362, 'grad_norm': 6.030498027801514, 'learning_rate': 3.507119889756546e-05, 'epoch': 1.11}\n",
      "{'loss': 0.614, 'grad_norm': 5.124351978302002, 'learning_rate': 3.495636196600827e-05, 'epoch': 1.12}\n",
      "{'loss': 0.4386, 'grad_norm': 14.021429061889648, 'learning_rate': 3.484152503445108e-05, 'epoch': 1.12}\n",
      "{'loss': 0.3657, 'grad_norm': 5.827121734619141, 'learning_rate': 3.47266881028939e-05, 'epoch': 1.13}\n",
      "{'loss': 0.1469, 'grad_norm': 0.6417927145957947, 'learning_rate': 3.46118511713367e-05, 'epoch': 1.14}\n",
      "{'loss': 0.5355, 'grad_norm': 0.3291994631290436, 'learning_rate': 3.449701423977951e-05, 'epoch': 1.14}\n",
      "{'loss': 0.2616, 'grad_norm': 29.57989501953125, 'learning_rate': 3.438217730822232e-05, 'epoch': 1.15}\n",
      "{'loss': 0.6482, 'grad_norm': 5.404002666473389, 'learning_rate': 3.4267340376665136e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3861, 'grad_norm': 48.08650207519531, 'learning_rate': 3.415250344510795e-05, 'epoch': 1.16}\n",
      "{'loss': 0.216, 'grad_norm': 0.3410201668739319, 'learning_rate': 3.403766651355076e-05, 'epoch': 1.17}\n",
      "{'loss': 0.4732, 'grad_norm': 0.3536972403526306, 'learning_rate': 3.392282958199357e-05, 'epoch': 1.17}\n",
      "{'loss': 0.5384, 'grad_norm': 37.8001594543457, 'learning_rate': 3.380799265043638e-05, 'epoch': 1.18}\n",
      "{'loss': 0.2795, 'grad_norm': 0.3454577624797821, 'learning_rate': 3.3693155718879195e-05, 'epoch': 1.19}\n",
      "{'loss': 0.1882, 'grad_norm': 2.7917282581329346, 'learning_rate': 3.3578318787322e-05, 'epoch': 1.19}\n",
      "{'loss': 0.6372, 'grad_norm': 6.949123382568359, 'learning_rate': 3.346348185576482e-05, 'epoch': 1.2}\n",
      "{'loss': 0.3675, 'grad_norm': 5.537930011749268, 'learning_rate': 3.3348644924207625e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3389, 'grad_norm': 0.7011213898658752, 'learning_rate': 3.323380799265044e-05, 'epoch': 1.21}\n",
      "{'loss': 0.4313, 'grad_norm': 0.16841141879558563, 'learning_rate': 3.3118971061093254e-05, 'epoch': 1.22}\n",
      "{'loss': 0.6585, 'grad_norm': 5.602109432220459, 'learning_rate': 3.300413412953606e-05, 'epoch': 1.22}\n",
      "{'loss': 0.3105, 'grad_norm': 0.4367905259132385, 'learning_rate': 3.288929719797887e-05, 'epoch': 1.23}\n",
      "{'loss': 0.6369, 'grad_norm': 0.6385950446128845, 'learning_rate': 3.277446026642168e-05, 'epoch': 1.24}\n",
      "{'loss': 0.4109, 'grad_norm': 6.695492267608643, 'learning_rate': 3.265962333486449e-05, 'epoch': 1.24}\n",
      "{'loss': 0.1814, 'grad_norm': 7.254838466644287, 'learning_rate': 3.2544786403307307e-05, 'epoch': 1.25}\n",
      "{'loss': 0.3662, 'grad_norm': 5.688439846038818, 'learning_rate': 3.2429949471750114e-05, 'epoch': 1.25}\n",
      "{'loss': 0.3557, 'grad_norm': 55.54147720336914, 'learning_rate': 3.231511254019293e-05, 'epoch': 1.26}\n",
      "{'loss': 0.526, 'grad_norm': 0.29331427812576294, 'learning_rate': 3.220027560863574e-05, 'epoch': 1.27}\n",
      "{'loss': 0.4298, 'grad_norm': 0.2808266878128052, 'learning_rate': 3.208543867707855e-05, 'epoch': 1.27}\n",
      "{'loss': 0.6284, 'grad_norm': 0.2310105264186859, 'learning_rate': 3.197060174552136e-05, 'epoch': 1.28}\n",
      "{'loss': 0.518, 'grad_norm': 16.106706619262695, 'learning_rate': 3.1855764813964174e-05, 'epoch': 1.29}\n",
      "{'loss': 0.6612, 'grad_norm': 31.667146682739258, 'learning_rate': 3.174092788240698e-05, 'epoch': 1.29}\n",
      "{'loss': 0.5075, 'grad_norm': 3.6821014881134033, 'learning_rate': 3.1626090950849796e-05, 'epoch': 1.3}\n",
      "{'loss': 0.3016, 'grad_norm': 10.055251121520996, 'learning_rate': 3.151125401929261e-05, 'epoch': 1.3}\n",
      "{'loss': 0.1084, 'grad_norm': 0.3235322833061218, 'learning_rate': 3.139641708773542e-05, 'epoch': 1.31}\n",
      "{'loss': 0.4499, 'grad_norm': 0.4666650891304016, 'learning_rate': 3.128158015617823e-05, 'epoch': 1.32}\n",
      "{'loss': 0.9514, 'grad_norm': 26.35495948791504, 'learning_rate': 3.116674322462104e-05, 'epoch': 1.32}\n",
      "{'loss': 0.6614, 'grad_norm': 20.764034271240234, 'learning_rate': 3.105190629306385e-05, 'epoch': 1.33}\n",
      "{'loss': 0.4608, 'grad_norm': 1.121199369430542, 'learning_rate': 3.093706936150666e-05, 'epoch': 1.33}\n",
      "{'loss': 0.4702, 'grad_norm': 0.7713282108306885, 'learning_rate': 3.082223242994947e-05, 'epoch': 1.34}\n",
      "{'loss': 0.3926, 'grad_norm': 9.523310661315918, 'learning_rate': 3.0707395498392285e-05, 'epoch': 1.35}\n",
      "{'loss': 0.2595, 'grad_norm': 0.6820602416992188, 'learning_rate': 3.059255856683509e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3558, 'grad_norm': 0.2449544370174408, 'learning_rate': 3.0477721635277908e-05, 'epoch': 1.36}\n",
      "{'loss': 0.7201, 'grad_norm': 0.12223003804683685, 'learning_rate': 3.036288470372072e-05, 'epoch': 1.37}\n",
      "{'loss': 0.7598, 'grad_norm': 0.16215649247169495, 'learning_rate': 3.024804777216353e-05, 'epoch': 1.37}\n",
      "{'loss': 0.4505, 'grad_norm': 5.603200912475586, 'learning_rate': 3.013321084060634e-05, 'epoch': 1.38}\n",
      "{'loss': 0.3277, 'grad_norm': 10.920584678649902, 'learning_rate': 3.0018373909049152e-05, 'epoch': 1.38}\n",
      "{'loss': 0.4131, 'grad_norm': 0.391354501247406, 'learning_rate': 2.9903536977491963e-05, 'epoch': 1.39}\n",
      "{'loss': 0.3777, 'grad_norm': 5.109095573425293, 'learning_rate': 2.9788700045934775e-05, 'epoch': 1.4}\n",
      "{'loss': 0.4013, 'grad_norm': 0.5658596158027649, 'learning_rate': 2.9673863114377586e-05, 'epoch': 1.4}\n",
      "{'loss': 0.5712, 'grad_norm': 6.063354969024658, 'learning_rate': 2.9559026182820397e-05, 'epoch': 1.41}\n",
      "{'loss': 0.1422, 'grad_norm': 0.32646486163139343, 'learning_rate': 2.944418925126321e-05, 'epoch': 1.42}\n",
      "{'loss': 0.6933, 'grad_norm': 1.4813915491104126, 'learning_rate': 2.9329352319706023e-05, 'epoch': 1.42}\n",
      "{'loss': 0.4362, 'grad_norm': 0.3938976526260376, 'learning_rate': 2.9214515388148827e-05, 'epoch': 1.43}\n",
      "{'loss': 0.56, 'grad_norm': 0.7737960815429688, 'learning_rate': 2.9099678456591638e-05, 'epoch': 1.43}\n",
      "{'loss': 0.3688, 'grad_norm': 23.407352447509766, 'learning_rate': 2.898484152503445e-05, 'epoch': 1.44}\n",
      "{'loss': 0.131, 'grad_norm': 0.11422324180603027, 'learning_rate': 2.8870004593477264e-05, 'epoch': 1.45}\n",
      "{'loss': 0.3017, 'grad_norm': 0.13148115575313568, 'learning_rate': 2.8755167661920075e-05, 'epoch': 1.45}\n",
      "{'loss': 0.8704, 'grad_norm': 18.12883949279785, 'learning_rate': 2.8640330730362886e-05, 'epoch': 1.46}\n",
      "{'loss': 0.4041, 'grad_norm': 5.7285990715026855, 'learning_rate': 2.8525493798805697e-05, 'epoch': 1.46}\n",
      "{'loss': 0.4287, 'grad_norm': 7.891317367553711, 'learning_rate': 2.841065686724851e-05, 'epoch': 1.47}\n",
      "{'loss': 0.3464, 'grad_norm': 4.51162052154541, 'learning_rate': 2.829581993569132e-05, 'epoch': 1.48}\n",
      "{'loss': 0.5537, 'grad_norm': 4.382108211517334, 'learning_rate': 2.818098300413413e-05, 'epoch': 1.48}\n",
      "{'loss': 0.1554, 'grad_norm': 5.910136699676514, 'learning_rate': 2.8066146072576942e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2923, 'grad_norm': 5.9710469245910645, 'learning_rate': 2.7951309141019753e-05, 'epoch': 1.5}\n",
      "{'loss': 0.3697, 'grad_norm': 0.16474494338035583, 'learning_rate': 2.7836472209462568e-05, 'epoch': 1.5}\n",
      "{'loss': 0.5106, 'grad_norm': 5.303403377532959, 'learning_rate': 2.772163527790538e-05, 'epoch': 1.51}\n",
      "{'loss': 0.1222, 'grad_norm': 61.76877975463867, 'learning_rate': 2.760679834634819e-05, 'epoch': 1.51}\n",
      "{'loss': 0.3919, 'grad_norm': 6.455941677093506, 'learning_rate': 2.7491961414790994e-05, 'epoch': 1.52}\n",
      "{'loss': 0.214, 'grad_norm': 2.057896137237549, 'learning_rate': 2.7377124483233806e-05, 'epoch': 1.53}\n",
      "{'loss': 0.5954, 'grad_norm': 9.08785629272461, 'learning_rate': 2.726228755167662e-05, 'epoch': 1.53}\n",
      "{'loss': 0.3758, 'grad_norm': 17.28313446044922, 'learning_rate': 2.714745062011943e-05, 'epoch': 1.54}\n",
      "{'loss': 0.5627, 'grad_norm': 0.36098724603652954, 'learning_rate': 2.7032613688562243e-05, 'epoch': 1.55}\n",
      "{'loss': 0.7639, 'grad_norm': 10.615864753723145, 'learning_rate': 2.6917776757005054e-05, 'epoch': 1.55}\n",
      "{'loss': 0.4108, 'grad_norm': 22.379846572875977, 'learning_rate': 2.6802939825447865e-05, 'epoch': 1.56}\n",
      "{'loss': 0.4536, 'grad_norm': 9.685794830322266, 'learning_rate': 2.6688102893890676e-05, 'epoch': 1.56}\n",
      "{'loss': 0.4182, 'grad_norm': 0.2800213396549225, 'learning_rate': 2.6573265962333487e-05, 'epoch': 1.57}\n",
      "{'loss': 0.5381, 'grad_norm': 10.4905424118042, 'learning_rate': 2.64584290307763e-05, 'epoch': 1.58}\n",
      "{'loss': 0.2939, 'grad_norm': 4.017139911651611, 'learning_rate': 2.634359209921911e-05, 'epoch': 1.58}\n",
      "{'loss': 0.2683, 'grad_norm': 0.374610960483551, 'learning_rate': 2.6228755167661924e-05, 'epoch': 1.59}\n",
      "{'loss': 0.3621, 'grad_norm': 14.160160064697266, 'learning_rate': 2.6113918236104735e-05, 'epoch': 1.59}\n",
      "{'loss': 0.3656, 'grad_norm': 0.2875012159347534, 'learning_rate': 2.5999081304547546e-05, 'epoch': 1.6}\n",
      "{'loss': 0.3453, 'grad_norm': 4.319630146026611, 'learning_rate': 2.5884244372990358e-05, 'epoch': 1.61}\n",
      "{'loss': 0.1276, 'grad_norm': 1.0720436573028564, 'learning_rate': 2.576940744143317e-05, 'epoch': 1.61}\n",
      "{'loss': 0.6612, 'grad_norm': 12.447552680969238, 'learning_rate': 2.5654570509875973e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0888, 'grad_norm': 10.477314949035645, 'learning_rate': 2.5539733578318788e-05, 'epoch': 1.63}\n",
      "{'loss': 0.4518, 'grad_norm': 1.3059334754943848, 'learning_rate': 2.54248966467616e-05, 'epoch': 1.63}\n",
      "{'loss': 0.5313, 'grad_norm': 0.17775537073612213, 'learning_rate': 2.531005971520441e-05, 'epoch': 1.64}\n",
      "{'loss': 0.3719, 'grad_norm': 0.6977485418319702, 'learning_rate': 2.519522278364722e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0793, 'grad_norm': 0.08797736465930939, 'learning_rate': 2.5080385852090032e-05, 'epoch': 1.65}\n",
      "{'loss': 0.5299, 'grad_norm': 11.891118049621582, 'learning_rate': 2.4965548920532844e-05, 'epoch': 1.66}\n",
      "{'loss': 0.4992, 'grad_norm': 0.2714678943157196, 'learning_rate': 2.4850711988975655e-05, 'epoch': 1.66}\n",
      "{'loss': 0.4084, 'grad_norm': 56.72197723388672, 'learning_rate': 2.4735875057418466e-05, 'epoch': 1.67}\n",
      "{'loss': 0.7405, 'grad_norm': 1.4122495651245117, 'learning_rate': 2.462103812586128e-05, 'epoch': 1.67}\n",
      "{'loss': 0.3291, 'grad_norm': 0.3304242491722107, 'learning_rate': 2.450620119430409e-05, 'epoch': 1.68}\n",
      "{'loss': 0.4502, 'grad_norm': 63.669647216796875, 'learning_rate': 2.4391364262746903e-05, 'epoch': 1.69}\n",
      "{'loss': 0.5045, 'grad_norm': 16.82427406311035, 'learning_rate': 2.427652733118971e-05, 'epoch': 1.69}\n",
      "{'loss': 0.4112, 'grad_norm': 3.9590632915496826, 'learning_rate': 2.4161690399632522e-05, 'epoch': 1.7}\n",
      "{'loss': 0.746, 'grad_norm': 6.796035289764404, 'learning_rate': 2.4046853468075333e-05, 'epoch': 1.71}\n",
      "{'loss': 0.3022, 'grad_norm': 0.7022577524185181, 'learning_rate': 2.3932016536518144e-05, 'epoch': 1.71}\n",
      "{'loss': 0.5647, 'grad_norm': 0.46679896116256714, 'learning_rate': 2.381717960496096e-05, 'epoch': 1.72}\n",
      "{'loss': 0.2233, 'grad_norm': 19.209577560424805, 'learning_rate': 2.370234267340377e-05, 'epoch': 1.72}\n",
      "{'loss': 0.2812, 'grad_norm': 5.531628131866455, 'learning_rate': 2.358750574184658e-05, 'epoch': 1.73}\n",
      "{'loss': 0.3331, 'grad_norm': 0.13745616376399994, 'learning_rate': 2.3472668810289392e-05, 'epoch': 1.74}\n",
      "{'loss': 0.4108, 'grad_norm': 6.090848445892334, 'learning_rate': 2.33578318787322e-05, 'epoch': 1.74}\n",
      "{'loss': 0.3374, 'grad_norm': 1.8678460121154785, 'learning_rate': 2.324299494717501e-05, 'epoch': 1.75}\n",
      "{'loss': 0.8031, 'grad_norm': 6.950521469116211, 'learning_rate': 2.3128158015617822e-05, 'epoch': 1.76}\n",
      "{'loss': 0.564, 'grad_norm': 7.3113932609558105, 'learning_rate': 2.3013321084060633e-05, 'epoch': 1.76}\n",
      "{'loss': 0.4529, 'grad_norm': 17.043970108032227, 'learning_rate': 2.2898484152503448e-05, 'epoch': 1.77}\n",
      "{'loss': 0.4088, 'grad_norm': 5.955678462982178, 'learning_rate': 2.278364722094626e-05, 'epoch': 1.77}\n",
      "{'loss': 0.1729, 'grad_norm': 25.317306518554688, 'learning_rate': 2.266881028938907e-05, 'epoch': 1.78}\n",
      "{'loss': 0.5483, 'grad_norm': 7.25191068649292, 'learning_rate': 2.2553973357831878e-05, 'epoch': 1.79}\n",
      "{'loss': 0.2921, 'grad_norm': 0.33262309432029724, 'learning_rate': 2.243913642627469e-05, 'epoch': 1.79}\n",
      "{'loss': 0.4309, 'grad_norm': 3.1252241134643555, 'learning_rate': 2.23242994947175e-05, 'epoch': 1.8}\n",
      "{'loss': 0.5678, 'grad_norm': 18.014842987060547, 'learning_rate': 2.220946256316031e-05, 'epoch': 1.8}\n",
      "{'loss': 0.6075, 'grad_norm': 15.260980606079102, 'learning_rate': 2.2094625631603126e-05, 'epoch': 1.81}\n",
      "{'loss': 0.2109, 'grad_norm': 0.4305027723312378, 'learning_rate': 2.1979788700045937e-05, 'epoch': 1.82}\n",
      "{'loss': 0.4464, 'grad_norm': 0.25863713026046753, 'learning_rate': 2.186495176848875e-05, 'epoch': 1.82}\n",
      "{'loss': 0.3437, 'grad_norm': 0.390214204788208, 'learning_rate': 2.175011483693156e-05, 'epoch': 1.83}\n",
      "{'loss': 0.4462, 'grad_norm': 22.874164581298828, 'learning_rate': 2.1635277905374367e-05, 'epoch': 1.84}\n",
      "{'loss': 0.2212, 'grad_norm': 0.5677623748779297, 'learning_rate': 2.152044097381718e-05, 'epoch': 1.84}\n",
      "{'loss': 0.5487, 'grad_norm': 0.2772807776927948, 'learning_rate': 2.140560404225999e-05, 'epoch': 1.85}\n",
      "{'loss': 0.4115, 'grad_norm': 40.407413482666016, 'learning_rate': 2.1290767110702804e-05, 'epoch': 1.85}\n",
      "{'loss': 0.4416, 'grad_norm': 6.50905704498291, 'learning_rate': 2.1175930179145615e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0223, 'grad_norm': 0.28731808066368103, 'learning_rate': 2.1061093247588427e-05, 'epoch': 1.87}\n",
      "{'loss': 0.4103, 'grad_norm': 0.26575684547424316, 'learning_rate': 2.0946256316031238e-05, 'epoch': 1.87}\n",
      "{'loss': 0.3549, 'grad_norm': 0.1536468267440796, 'learning_rate': 2.083141938447405e-05, 'epoch': 1.88}\n",
      "{'loss': 0.4709, 'grad_norm': 7.985661029815674, 'learning_rate': 2.0716582452916857e-05, 'epoch': 1.89}\n",
      "{'loss': 0.3148, 'grad_norm': 0.2316727340221405, 'learning_rate': 2.0601745521359668e-05, 'epoch': 1.89}\n",
      "{'loss': 0.1869, 'grad_norm': 0.2964620590209961, 'learning_rate': 2.0486908589802482e-05, 'epoch': 1.9}\n",
      "{'loss': 0.1678, 'grad_norm': 0.36049801111221313, 'learning_rate': 2.0372071658245294e-05, 'epoch': 1.9}\n",
      "{'loss': 0.5907, 'grad_norm': 0.2115800976753235, 'learning_rate': 2.0257234726688105e-05, 'epoch': 1.91}\n",
      "{'loss': 0.4636, 'grad_norm': 19.62295913696289, 'learning_rate': 2.0142397795130916e-05, 'epoch': 1.92}\n",
      "{'loss': 0.4752, 'grad_norm': 11.6383638381958, 'learning_rate': 2.0027560863573727e-05, 'epoch': 1.92}\n",
      "{'loss': 0.6578, 'grad_norm': 0.20205293595790863, 'learning_rate': 1.9912723932016538e-05, 'epoch': 1.93}\n",
      "{'loss': 0.4511, 'grad_norm': 9.739289283752441, 'learning_rate': 1.9797887000459346e-05, 'epoch': 1.93}\n",
      "{'loss': 0.4342, 'grad_norm': 4.415208339691162, 'learning_rate': 1.968305006890216e-05, 'epoch': 1.94}\n",
      "{'loss': 0.4784, 'grad_norm': 6.174350261688232, 'learning_rate': 1.9568213137344972e-05, 'epoch': 1.95}\n",
      "{'loss': 0.2866, 'grad_norm': 0.21616041660308838, 'learning_rate': 1.9453376205787783e-05, 'epoch': 1.95}\n",
      "{'loss': 0.507, 'grad_norm': 5.206993103027344, 'learning_rate': 1.9338539274230594e-05, 'epoch': 1.96}\n",
      "{'loss': 0.2348, 'grad_norm': 19.455066680908203, 'learning_rate': 1.9223702342673405e-05, 'epoch': 1.97}\n",
      "{'loss': 0.3758, 'grad_norm': 0.436416894197464, 'learning_rate': 1.9108865411116216e-05, 'epoch': 1.97}\n",
      "{'loss': 0.246, 'grad_norm': 0.9393644332885742, 'learning_rate': 1.8994028479559028e-05, 'epoch': 1.98}\n",
      "{'loss': 0.7655, 'grad_norm': 0.21509067714214325, 'learning_rate': 1.887919154800184e-05, 'epoch': 1.98}\n",
      "{'loss': 0.4187, 'grad_norm': 9.874906539916992, 'learning_rate': 1.876435461644465e-05, 'epoch': 1.99}\n",
      "{'loss': 0.6129, 'grad_norm': 5.883584022521973, 'learning_rate': 1.864951768488746e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d1047a08d24428a9acb4909b21e307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5717092752456665, 'eval_runtime': 47.4322, 'eval_samples_per_second': 24.076, 'eval_steps_per_second': 6.03, 'epoch': 2.0}\n",
      "{'loss': 0.4754, 'grad_norm': 16.62102699279785, 'learning_rate': 1.8534680753330272e-05, 'epoch': 2.0}\n",
      "{'loss': 0.4183, 'grad_norm': 12.449641227722168, 'learning_rate': 1.8419843821773083e-05, 'epoch': 2.01}\n",
      "{'loss': 0.3122, 'grad_norm': 0.5442740321159363, 'learning_rate': 1.8305006890215895e-05, 'epoch': 2.01}\n",
      "{'loss': 0.264, 'grad_norm': 0.2950558662414551, 'learning_rate': 1.8190169958658706e-05, 'epoch': 2.02}\n",
      "{'loss': 0.305, 'grad_norm': 0.14227183163166046, 'learning_rate': 1.8075333027101517e-05, 'epoch': 2.03}\n",
      "{'loss': 0.3351, 'grad_norm': 0.2147645503282547, 'learning_rate': 1.7960496095544328e-05, 'epoch': 2.03}\n",
      "{'loss': 0.2875, 'grad_norm': 7.422791957855225, 'learning_rate': 1.784565916398714e-05, 'epoch': 2.04}\n",
      "{'loss': 0.3994, 'grad_norm': 6.313598155975342, 'learning_rate': 1.773082223242995e-05, 'epoch': 2.05}\n",
      "{'loss': 0.2476, 'grad_norm': 0.6658515930175781, 'learning_rate': 1.761598530087276e-05, 'epoch': 2.05}\n",
      "{'loss': 0.2377, 'grad_norm': 0.2270718812942505, 'learning_rate': 1.7501148369315573e-05, 'epoch': 2.06}\n",
      "{'loss': 0.3483, 'grad_norm': 0.4466327726840973, 'learning_rate': 1.7386311437758384e-05, 'epoch': 2.06}\n",
      "{'loss': 0.2505, 'grad_norm': 0.24800783395767212, 'learning_rate': 1.7271474506201195e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0641, 'grad_norm': 0.16553665697574615, 'learning_rate': 1.7156637574644006e-05, 'epoch': 2.08}\n",
      "{'loss': 0.267, 'grad_norm': 0.07389938831329346, 'learning_rate': 1.7041800643086817e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0607, 'grad_norm': 7.23259162902832, 'learning_rate': 1.692696371152963e-05, 'epoch': 2.09}\n",
      "{'loss': 0.2612, 'grad_norm': 0.767481803894043, 'learning_rate': 1.681212677997244e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0957, 'grad_norm': 0.07001807540655136, 'learning_rate': 1.669728984841525e-05, 'epoch': 2.1}\n",
      "{'loss': 0.2279, 'grad_norm': 11.310330390930176, 'learning_rate': 1.6582452916858062e-05, 'epoch': 2.11}\n",
      "{'loss': 0.2765, 'grad_norm': 12.981671333312988, 'learning_rate': 1.6467615985300873e-05, 'epoch': 2.11}\n",
      "{'loss': 0.3749, 'grad_norm': 0.1446937918663025, 'learning_rate': 1.6352779053743684e-05, 'epoch': 2.12}\n",
      "{'loss': 0.1119, 'grad_norm': 0.04585256427526474, 'learning_rate': 1.6237942122186496e-05, 'epoch': 2.13}\n",
      "{'loss': 0.1769, 'grad_norm': 0.05580643191933632, 'learning_rate': 1.6123105190629307e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0727, 'grad_norm': 1.1161606311798096, 'learning_rate': 1.6008268259072118e-05, 'epoch': 2.14}\n",
      "{'loss': 0.6088, 'grad_norm': 78.07893371582031, 'learning_rate': 1.589343132751493e-05, 'epoch': 2.14}\n",
      "{'loss': 0.3731, 'grad_norm': 0.09733692556619644, 'learning_rate': 1.577859439595774e-05, 'epoch': 2.15}\n",
      "{'loss': 0.1779, 'grad_norm': 8.648031234741211, 'learning_rate': 1.566375746440055e-05, 'epoch': 2.16}\n",
      "{'loss': 0.3756, 'grad_norm': 11.521993637084961, 'learning_rate': 1.5548920532843363e-05, 'epoch': 2.16}\n",
      "{'loss': 0.2477, 'grad_norm': 0.06306798756122589, 'learning_rate': 1.5434083601286177e-05, 'epoch': 2.17}\n",
      "{'loss': 0.4066, 'grad_norm': 23.807069778442383, 'learning_rate': 1.5319246669728985e-05, 'epoch': 2.18}\n",
      "{'loss': 0.1402, 'grad_norm': 0.18568848073482513, 'learning_rate': 1.5204409738171796e-05, 'epoch': 2.18}\n",
      "{'loss': 0.1398, 'grad_norm': 0.4964616894721985, 'learning_rate': 1.5089572806614607e-05, 'epoch': 2.19}\n",
      "{'loss': 0.2149, 'grad_norm': 59.43754196166992, 'learning_rate': 1.4974735875057418e-05, 'epoch': 2.19}\n",
      "{'loss': 0.3445, 'grad_norm': 0.09392905980348587, 'learning_rate': 1.4859898943500231e-05, 'epoch': 2.2}\n",
      "{'loss': 0.6001, 'grad_norm': 0.6898484826087952, 'learning_rate': 1.4745062011943042e-05, 'epoch': 2.21}\n",
      "{'loss': 0.2336, 'grad_norm': 0.26387232542037964, 'learning_rate': 1.4630225080385854e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0937, 'grad_norm': 0.1929003745317459, 'learning_rate': 1.4515388148828665e-05, 'epoch': 2.22}\n",
      "{'loss': 0.3296, 'grad_norm': 0.29319918155670166, 'learning_rate': 1.4400551217271474e-05, 'epoch': 2.22}\n",
      "{'loss': 0.2626, 'grad_norm': 0.08783356100320816, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.23}\n",
      "{'loss': 0.1232, 'grad_norm': 0.12807179987430573, 'learning_rate': 1.4170877354157097e-05, 'epoch': 2.24}\n",
      "{'loss': 0.3026, 'grad_norm': 9.401603698730469, 'learning_rate': 1.405604042259991e-05, 'epoch': 2.24}\n",
      "{'loss': 0.2145, 'grad_norm': 0.11417973786592484, 'learning_rate': 1.394120349104272e-05, 'epoch': 2.25}\n",
      "{'loss': 0.2045, 'grad_norm': 0.13075464963912964, 'learning_rate': 1.3826366559485532e-05, 'epoch': 2.26}\n",
      "{'loss': 0.1008, 'grad_norm': 0.2012820690870285, 'learning_rate': 1.3711529627928343e-05, 'epoch': 2.26}\n",
      "{'loss': 0.1089, 'grad_norm': 0.11388938128948212, 'learning_rate': 1.3596692696371154e-05, 'epoch': 2.27}\n",
      "{'loss': 0.1381, 'grad_norm': 0.0899234488606453, 'learning_rate': 1.3481855764813964e-05, 'epoch': 2.27}\n",
      "{'loss': 0.2056, 'grad_norm': 0.09959138929843903, 'learning_rate': 1.3367018833256775e-05, 'epoch': 2.28}\n",
      "{'loss': 0.2215, 'grad_norm': 0.2240617871284485, 'learning_rate': 1.3252181901699588e-05, 'epoch': 2.29}\n",
      "{'loss': 0.3829, 'grad_norm': 0.13530029356479645, 'learning_rate': 1.3137344970142399e-05, 'epoch': 2.29}\n",
      "{'loss': 0.4543, 'grad_norm': 0.10137815773487091, 'learning_rate': 1.302250803858521e-05, 'epoch': 2.3}\n",
      "{'loss': 0.1823, 'grad_norm': 0.08944803476333618, 'learning_rate': 1.2907671107028021e-05, 'epoch': 2.31}\n",
      "{'loss': 0.4716, 'grad_norm': 0.13910435140132904, 'learning_rate': 1.2792834175470832e-05, 'epoch': 2.31}\n",
      "{'loss': 0.4824, 'grad_norm': 0.09605466574430466, 'learning_rate': 1.2677997243913642e-05, 'epoch': 2.32}\n",
      "{'loss': 0.3738, 'grad_norm': 15.413703918457031, 'learning_rate': 1.2563160312356453e-05, 'epoch': 2.32}\n",
      "{'loss': 0.3163, 'grad_norm': 0.5714001059532166, 'learning_rate': 1.2448323380799266e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0596, 'grad_norm': 0.1949470192193985, 'learning_rate': 1.2333486449242077e-05, 'epoch': 2.34}\n",
      "{'loss': 0.162, 'grad_norm': 0.051817938685417175, 'learning_rate': 1.2218649517684888e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0057, 'grad_norm': 0.059127043932676315, 'learning_rate': 1.21038125861277e-05, 'epoch': 2.35}\n",
      "{'loss': 0.1155, 'grad_norm': 0.049339089542627335, 'learning_rate': 1.198897565457051e-05, 'epoch': 2.35}\n",
      "{'loss': 0.3426, 'grad_norm': 7.26286506652832, 'learning_rate': 1.1874138723013322e-05, 'epoch': 2.36}\n",
      "{'loss': 0.2905, 'grad_norm': 7.692250728607178, 'learning_rate': 1.1759301791456133e-05, 'epoch': 2.37}\n",
      "{'loss': 0.2277, 'grad_norm': 27.306686401367188, 'learning_rate': 1.1644464859898944e-05, 'epoch': 2.37}\n",
      "{'loss': 0.2842, 'grad_norm': 0.14229004085063934, 'learning_rate': 1.1529627928341755e-05, 'epoch': 2.38}\n",
      "{'loss': 0.2262, 'grad_norm': 0.6590325832366943, 'learning_rate': 1.1414790996784566e-05, 'epoch': 2.39}\n",
      "{'loss': 0.1265, 'grad_norm': 0.15462873876094818, 'learning_rate': 1.1299954065227377e-05, 'epoch': 2.39}\n",
      "{'loss': 0.2723, 'grad_norm': 0.06020191311836243, 'learning_rate': 1.1185117133670189e-05, 'epoch': 2.4}\n",
      "{'loss': 0.3577, 'grad_norm': 20.694490432739258, 'learning_rate': 1.1070280202113e-05, 'epoch': 2.4}\n",
      "{'loss': 0.3051, 'grad_norm': 0.13487599790096283, 'learning_rate': 1.0955443270555811e-05, 'epoch': 2.41}\n",
      "{'loss': 0.4114, 'grad_norm': 8.656668663024902, 'learning_rate': 1.0840606338998622e-05, 'epoch': 2.42}\n",
      "{'loss': 0.128, 'grad_norm': 0.10661561042070389, 'learning_rate': 1.0725769407441435e-05, 'epoch': 2.42}\n",
      "{'loss': 0.3051, 'grad_norm': 0.4016323387622833, 'learning_rate': 1.0610932475884244e-05, 'epoch': 2.43}\n",
      "{'loss': 0.387, 'grad_norm': 1.566047191619873, 'learning_rate': 1.0496095544327056e-05, 'epoch': 2.44}\n",
      "{'loss': 0.2887, 'grad_norm': 0.08710922300815582, 'learning_rate': 1.0381258612769867e-05, 'epoch': 2.44}\n",
      "{'loss': 0.452, 'grad_norm': 0.29092058539390564, 'learning_rate': 1.026642168121268e-05, 'epoch': 2.45}\n",
      "{'loss': 0.6774, 'grad_norm': 2.3399248123168945, 'learning_rate': 1.0151584749655489e-05, 'epoch': 2.45}\n",
      "{'loss': 0.1481, 'grad_norm': 0.347807914018631, 'learning_rate': 1.00367478180983e-05, 'epoch': 2.46}\n",
      "{'loss': 0.2329, 'grad_norm': 0.12349554896354675, 'learning_rate': 9.921910886541111e-06, 'epoch': 2.47}\n",
      "{'loss': 0.058, 'grad_norm': 0.45835962891578674, 'learning_rate': 9.807073954983924e-06, 'epoch': 2.47}\n",
      "{'loss': 0.2061, 'grad_norm': 0.1373303383588791, 'learning_rate': 9.692237023426734e-06, 'epoch': 2.48}\n",
      "{'loss': 0.544, 'grad_norm': 0.179594486951828, 'learning_rate': 9.577400091869545e-06, 'epoch': 2.48}\n",
      "{'loss': 0.3235, 'grad_norm': 17.900850296020508, 'learning_rate': 9.462563160312358e-06, 'epoch': 2.49}\n",
      "{'loss': 0.186, 'grad_norm': 0.06588207185268402, 'learning_rate': 9.347726228755169e-06, 'epoch': 2.5}\n",
      "{'loss': 0.479, 'grad_norm': 32.62202453613281, 'learning_rate': 9.232889297197978e-06, 'epoch': 2.5}\n",
      "{'loss': 0.3265, 'grad_norm': 0.2369118630886078, 'learning_rate': 9.11805236564079e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0681, 'grad_norm': 0.14427661895751953, 'learning_rate': 9.003215434083602e-06, 'epoch': 2.52}\n",
      "{'loss': 0.225, 'grad_norm': 0.04963701590895653, 'learning_rate': 8.888378502526414e-06, 'epoch': 2.52}\n",
      "{'loss': 0.3013, 'grad_norm': 10.935444831848145, 'learning_rate': 8.773541570969223e-06, 'epoch': 2.53}\n",
      "{'loss': 0.3671, 'grad_norm': 0.0700514167547226, 'learning_rate': 8.658704639412036e-06, 'epoch': 2.53}\n",
      "{'loss': 0.1855, 'grad_norm': 0.07477767765522003, 'learning_rate': 8.543867707854847e-06, 'epoch': 2.54}\n",
      "{'loss': 0.2144, 'grad_norm': 1.3977032899856567, 'learning_rate': 8.429030776297658e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0224, 'grad_norm': 0.1205623671412468, 'learning_rate': 8.314193844740468e-06, 'epoch': 2.55}\n",
      "{'loss': 0.7269, 'grad_norm': 10.126653671264648, 'learning_rate': 8.19935691318328e-06, 'epoch': 2.56}\n",
      "{'loss': 0.3403, 'grad_norm': 9.49654483795166, 'learning_rate': 8.084519981626092e-06, 'epoch': 2.56}\n",
      "{'loss': 0.2208, 'grad_norm': 0.04632877558469772, 'learning_rate': 7.969683050068903e-06, 'epoch': 2.57}\n",
      "{'loss': 0.2678, 'grad_norm': 0.06852994114160538, 'learning_rate': 7.854846118511714e-06, 'epoch': 2.58}\n",
      "{'loss': 0.4963, 'grad_norm': 1.5534578561782837, 'learning_rate': 7.740009186954525e-06, 'epoch': 2.58}\n",
      "{'loss': 0.2644, 'grad_norm': 0.09151554852724075, 'learning_rate': 7.6251722553973365e-06, 'epoch': 2.59}\n",
      "{'loss': 0.1009, 'grad_norm': 0.25652703642845154, 'learning_rate': 7.510335323840148e-06, 'epoch': 2.6}\n",
      "{'loss': 0.1461, 'grad_norm': 0.04874531552195549, 'learning_rate': 7.395498392282958e-06, 'epoch': 2.6}\n",
      "{'loss': 0.1243, 'grad_norm': 11.502863883972168, 'learning_rate': 7.28066146072577e-06, 'epoch': 2.61}\n",
      "{'loss': 0.4263, 'grad_norm': 9.6553955078125, 'learning_rate': 7.165824529168581e-06, 'epoch': 2.61}\n",
      "{'loss': 0.2028, 'grad_norm': 16.134248733520508, 'learning_rate': 7.0509875976113915e-06, 'epoch': 2.62}\n",
      "{'loss': 0.4204, 'grad_norm': 0.04841787740588188, 'learning_rate': 6.936150666054203e-06, 'epoch': 2.63}\n",
      "{'loss': 0.4484, 'grad_norm': 26.067949295043945, 'learning_rate': 6.821313734497015e-06, 'epoch': 2.63}\n",
      "{'loss': 0.248, 'grad_norm': 0.364892840385437, 'learning_rate': 6.706476802939826e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0832, 'grad_norm': 0.16356106102466583, 'learning_rate': 6.591639871382636e-06, 'epoch': 2.65}\n",
      "{'loss': 0.2253, 'grad_norm': 30.926427841186523, 'learning_rate': 6.476802939825448e-06, 'epoch': 2.65}\n",
      "{'loss': 0.4661, 'grad_norm': 9.563102722167969, 'learning_rate': 6.361966008268259e-06, 'epoch': 2.66}\n",
      "{'loss': 0.2219, 'grad_norm': 0.1511538177728653, 'learning_rate': 6.2471290767110705e-06, 'epoch': 2.66}\n",
      "{'loss': 0.1655, 'grad_norm': 9.89553165435791, 'learning_rate': 6.132292145153882e-06, 'epoch': 2.67}\n",
      "{'loss': 0.2066, 'grad_norm': 26.09915542602539, 'learning_rate': 6.017455213596693e-06, 'epoch': 2.68}\n",
      "{'loss': 0.4806, 'grad_norm': 8.484273910522461, 'learning_rate': 5.902618282039504e-06, 'epoch': 2.68}\n",
      "{'loss': 0.024, 'grad_norm': 1.667301058769226, 'learning_rate': 5.787781350482315e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0842, 'grad_norm': 0.199642613530159, 'learning_rate': 5.672944418925126e-06, 'epoch': 2.69}\n",
      "{'loss': 0.4762, 'grad_norm': 0.07483791559934616, 'learning_rate': 5.5581074873679375e-06, 'epoch': 2.7}\n",
      "{'loss': 0.5144, 'grad_norm': 10.668987274169922, 'learning_rate': 5.4432705558107495e-06, 'epoch': 2.71}\n",
      "{'loss': 0.4536, 'grad_norm': 0.30125805735588074, 'learning_rate': 5.32843362425356e-06, 'epoch': 2.71}\n",
      "{'loss': 0.1445, 'grad_norm': 7.773948669433594, 'learning_rate': 5.213596692696372e-06, 'epoch': 2.72}\n",
      "{'loss': 0.2281, 'grad_norm': 13.439931869506836, 'learning_rate': 5.098759761139182e-06, 'epoch': 2.73}\n",
      "{'loss': 0.3521, 'grad_norm': 0.14783066511154175, 'learning_rate': 4.983922829581994e-06, 'epoch': 2.73}\n",
      "{'loss': 0.1194, 'grad_norm': 0.08754266798496246, 'learning_rate': 4.8690858980248045e-06, 'epoch': 2.74}\n",
      "{'loss': 0.5075, 'grad_norm': 27.858970642089844, 'learning_rate': 4.7542489664676165e-06, 'epoch': 2.74}\n",
      "{'loss': 0.1718, 'grad_norm': 0.05044464394450188, 'learning_rate': 4.639412034910427e-06, 'epoch': 2.75}\n",
      "{'loss': 0.5917, 'grad_norm': 8.783071517944336, 'learning_rate': 4.524575103353239e-06, 'epoch': 2.76}\n",
      "{'loss': 0.2045, 'grad_norm': 12.227823257446289, 'learning_rate': 4.40973817179605e-06, 'epoch': 2.76}\n",
      "{'loss': 0.346, 'grad_norm': 11.59769344329834, 'learning_rate': 4.294901240238861e-06, 'epoch': 2.77}\n",
      "{'loss': 0.4635, 'grad_norm': 0.07627210021018982, 'learning_rate': 4.180064308681672e-06, 'epoch': 2.78}\n",
      "{'loss': 0.1365, 'grad_norm': 0.11950960755348206, 'learning_rate': 4.0652273771244835e-06, 'epoch': 2.78}\n",
      "{'loss': 0.3709, 'grad_norm': 1.6401190757751465, 'learning_rate': 3.950390445567295e-06, 'epoch': 2.79}\n",
      "{'loss': 0.2178, 'grad_norm': 4.338315963745117, 'learning_rate': 3.835553514010106e-06, 'epoch': 2.79}\n",
      "{'loss': 0.4271, 'grad_norm': 0.08874097466468811, 'learning_rate': 3.720716582452917e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0926, 'grad_norm': 0.07383511960506439, 'learning_rate': 3.6058796508957286e-06, 'epoch': 2.81}\n",
      "{'loss': 0.3858, 'grad_norm': 12.345280647277832, 'learning_rate': 3.4910427193385393e-06, 'epoch': 2.81}\n",
      "{'loss': 0.1567, 'grad_norm': 0.05462132394313812, 'learning_rate': 3.376205787781351e-06, 'epoch': 2.82}\n",
      "{'loss': 0.6629, 'grad_norm': 33.13235092163086, 'learning_rate': 3.2613688562241617e-06, 'epoch': 2.82}\n",
      "{'loss': 0.095, 'grad_norm': 28.953622817993164, 'learning_rate': 3.1465319246669733e-06, 'epoch': 2.83}\n",
      "{'loss': 0.1144, 'grad_norm': 0.10177364200353622, 'learning_rate': 3.031694993109784e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0268, 'grad_norm': 0.07553683966398239, 'learning_rate': 2.9168580615525956e-06, 'epoch': 2.84}\n",
      "{'loss': 0.2591, 'grad_norm': 0.2590855360031128, 'learning_rate': 2.8020211299954068e-06, 'epoch': 2.85}\n",
      "{'loss': 0.3422, 'grad_norm': 2.9058148860931396, 'learning_rate': 2.687184198438218e-06, 'epoch': 2.86}\n",
      "{'loss': 0.2786, 'grad_norm': 20.811355590820312, 'learning_rate': 2.572347266881029e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0742, 'grad_norm': 77.79620361328125, 'learning_rate': 2.4575103353238403e-06, 'epoch': 2.87}\n",
      "{'loss': 0.3085, 'grad_norm': 13.375287055969238, 'learning_rate': 2.3426734037666514e-06, 'epoch': 2.87}\n",
      "{'loss': 0.2562, 'grad_norm': 0.08135148882865906, 'learning_rate': 2.2278364722094626e-06, 'epoch': 2.88}\n",
      "{'loss': 0.3596, 'grad_norm': 29.068742752075195, 'learning_rate': 2.1129995406522738e-06, 'epoch': 2.89}\n",
      "{'loss': 0.1515, 'grad_norm': 0.12834393978118896, 'learning_rate': 1.998162609095085e-06, 'epoch': 2.89}\n",
      "{'loss': 0.1734, 'grad_norm': 0.13944606482982635, 'learning_rate': 1.8833256775378963e-06, 'epoch': 2.9}\n",
      "{'loss': 0.1853, 'grad_norm': 33.115997314453125, 'learning_rate': 1.7684887459807075e-06, 'epoch': 2.9}\n",
      "{'loss': 0.3096, 'grad_norm': 0.2235107570886612, 'learning_rate': 1.6536518144235186e-06, 'epoch': 2.91}\n",
      "{'loss': 0.5433, 'grad_norm': 0.1681610643863678, 'learning_rate': 1.53881488286633e-06, 'epoch': 2.92}\n",
      "{'loss': 0.4439, 'grad_norm': 46.89957809448242, 'learning_rate': 1.4239779513091412e-06, 'epoch': 2.92}\n",
      "{'loss': 0.2265, 'grad_norm': 0.13947337865829468, 'learning_rate': 1.3091410197519524e-06, 'epoch': 2.93}\n",
      "{'loss': 0.5474, 'grad_norm': 65.99994659423828, 'learning_rate': 1.1943040881947635e-06, 'epoch': 2.94}\n",
      "{'loss': 0.2221, 'grad_norm': 1.0585147142410278, 'learning_rate': 1.0794671566375747e-06, 'epoch': 2.94}\n",
      "{'loss': 0.4157, 'grad_norm': 0.23261772096157074, 'learning_rate': 9.64630225080386e-07, 'epoch': 2.95}\n",
      "{'loss': 0.1392, 'grad_norm': 0.09398271888494492, 'learning_rate': 8.497932935231971e-07, 'epoch': 2.95}\n",
      "{'loss': 0.1654, 'grad_norm': 0.07333020865917206, 'learning_rate': 7.349563619660083e-07, 'epoch': 2.96}\n",
      "{'loss': 0.1746, 'grad_norm': 40.97045135498047, 'learning_rate': 6.201194304088195e-07, 'epoch': 2.97}\n",
      "{'loss': 0.4721, 'grad_norm': 0.0423075295984745, 'learning_rate': 5.052824988516307e-07, 'epoch': 2.97}\n",
      "{'loss': 0.2515, 'grad_norm': 185.90029907226562, 'learning_rate': 3.904455672944419e-07, 'epoch': 2.98}\n",
      "{'loss': 0.5191, 'grad_norm': 0.2096586376428604, 'learning_rate': 2.756086357372531e-07, 'epoch': 2.99}\n",
      "{'loss': 0.0069, 'grad_norm': 0.03818385675549507, 'learning_rate': 1.6077170418006432e-07, 'epoch': 2.99}\n",
      "{'loss': 0.1957, 'grad_norm': 14.511608123779297, 'learning_rate': 4.593477262287552e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3f8670cc624089b1972a47b25e5daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7988894581794739, 'eval_runtime': 47.8857, 'eval_samples_per_second': 23.848, 'eval_steps_per_second': 5.973, 'epoch': 3.0}\n",
      "{'train_runtime': 5066.6684, 'train_samples_per_second': 3.832, 'train_steps_per_second': 0.958, 'train_loss': 0.41929942678750715, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d9731e8d0343b6b6d4028a6b5dda00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1831207299463cb4e23581e0f4e257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 87\u001b[0m\n\u001b[1;32m     80\u001b[0m distill_bert_test_y_pred_full \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(distill_bert_test_logits_full, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m distill_bert_test_submission_full \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: df_test_full\u001b[38;5;241m.\u001b[39mindex,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m: distill_bert_test_y_pred_full\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     85\u001b[0m })\n\u001b[0;32m---> 87\u001b[0m \u001b[43mdistill_bert_trainer_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisastertweets_distill_bert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m distill_bert_tokenizer_full\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisastertweets_distill_bert_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m distill_bert_trainer_full\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myanncauchepin/kaggle_disastertweets_distill_bert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "distill_bert_tokenizer_full = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distill_bert_model_full = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "distill_bert_encodings_full = distill_bert_tokenizer_full(list(df_train_full['text']), truncation=True, padding=True, max_length=256)\n",
    "distill_bert_labels_full = torch.tensor(list(df_train_full['target']))\n",
    "\n",
    "distill_bert_input_ids_train_full, distill_bert_input_ids_valid_full, \\\n",
    "distill_bert_attention_mask_train_full, distill_bert_attention_mask_valid_full, distill_bert_y_train_full, distill_bert_y_valid_full = train_test_split(\n",
    "    distill_bert_encodings_full['input_ids'], \n",
    "    distill_bert_encodings_full['attention_mask'], \n",
    "    distill_bert_labels_full, \n",
    "    test_size=0.15, \n",
    "    stratify=distill_bert_labels_full, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "distill_bert_train_encodings_full = {\n",
    "    'input_ids': torch.tensor(distill_bert_input_ids_train_full),\n",
    "    'attention_mask': torch.tensor(distill_bert_attention_mask_train_full)\n",
    "}\n",
    "\n",
    "distill_bert_valid_encodings_full = {\n",
    "    'input_ids': torch.tensor(distill_bert_input_ids_valid_full),\n",
    "    'attention_mask': torch.tensor(distill_bert_attention_mask_valid_full)\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True \n",
    ")\n",
    "\n",
    "distill_bert_train_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": distill_bert_train_encodings_full['input_ids'],\n",
    "    \"attention_mask\": distill_bert_train_encodings_full['attention_mask'],\n",
    "    \"labels\": distill_bert_y_train_full\n",
    "})\n",
    "\n",
    "distill_bert_valid_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": distill_bert_valid_encodings_full['input_ids'],\n",
    "    \"attention_mask\": distill_bert_valid_encodings_full['attention_mask'],\n",
    "    \"labels\": distill_bert_y_valid_full\n",
    "})\n",
    "\n",
    "distill_bert_trainer_full = Trainer(\n",
    "    model=distill_bert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=distill_bert_train_dataset_full,\n",
    "    eval_dataset=distill_bert_valid_dataset_full\n",
    ")\n",
    "\n",
    "distill_bert_trainer_full.train()\n",
    "\n",
    "distill_bert_predictions_full = distill_bert_trainer_full.predict(distill_bert_valid_dataset_full)\n",
    "distill_bert_logits_full = distill_bert_predictions_full.predictions\n",
    "distill_bert_y_pred_full = np.argmax(distill_bert_logits_full, axis=1)\n",
    "\n",
    "distill_bert_trainer_full_assessement = evaluate_classifier(distill_bert_y_valid_full.numpy(), distill_bert_y_pred_full)\n",
    "\n",
    "distill_bert_test_encodings_full = distill_bert_tokenizer_full(list(df_test_full['text']), truncation=True, padding=True, max_length=256)\n",
    "distill_bert_test_encodings_full = {key: torch.tensor(val) for key, val in distill_bert_test_encodings_full.items()}\n",
    "distill_bert_test_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": distill_bert_test_encodings_full['input_ids'],\n",
    "    \"attention_mask\": distill_bert_test_encodings_full['attention_mask']\n",
    "})\n",
    "\n",
    "distill_bert_test_predictions_full = distill_bert_trainer_full.predict(distill_bert_test_dataset_full)\n",
    "distill_bert_test_logits_full = distill_bert_test_predictions_full.predictions\n",
    "distill_bert_test_y_pred_full = np.argmax(distill_bert_test_logits_full, axis=1)\n",
    "\n",
    "distill_bert_test_submission_full = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': distill_bert_test_y_pred_full.flatten()\n",
    "})\n",
    "\n",
    "distill_bert_trainer_full.model.push_to_hub(\"yanncauchepin/kaggle_disastertweets_distill_bert_model\")\n",
    "distill_bert_tokenizer_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_distill_bert_tokenizer\")\n",
    "\n",
    "hf_distill_bert_test_submission_full = Dataset.from_pandas(distill_bert_test_submission_full)\n",
    "hf_distill_bert_test_submission_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_distill_bert_submission_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa9a1961cd94ae1b3faa265fe9957ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0309d4f3799a41dbac12008202af2b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f377f020b4a14ea6b180c5b555a34418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e665e5608aef40bc8f1e238a7ce45834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d960af94cdaa41d4b304c480ecef14c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a04b2cf41fb4418a3fba7e167e7a3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/transformers/training_args.py:1574: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3095c6abf9874375947cbfa0a883a2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7047, 'grad_norm': 4.763729572296143, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 0.6909, 'grad_norm': 2.3180086612701416, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 0.6959, 'grad_norm': 4.462715148925781, 'learning_rate': 3e-06, 'epoch': 0.02}\n",
      "{'loss': 0.693, 'grad_norm': 4.434909820556641, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6917, 'grad_norm': 3.107389211654663, 'learning_rate': 5e-06, 'epoch': 0.03}\n",
      "{'loss': 0.6905, 'grad_norm': 5.843577861785889, 'learning_rate': 6e-06, 'epoch': 0.04}\n",
      "{'loss': 0.6944, 'grad_norm': 6.763678073883057, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 0.696, 'grad_norm': 4.3050079345703125, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 0.6812, 'grad_norm': 10.309606552124023, 'learning_rate': 9e-06, 'epoch': 0.06}\n",
      "{'loss': 0.72, 'grad_norm': 4.82548713684082, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 0.6682, 'grad_norm': 1.924714207649231, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6584, 'grad_norm': 7.451861381530762, 'learning_rate': 1.2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6784, 'grad_norm': 8.313910484313965, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6823, 'grad_norm': 12.379124641418457, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5835, 'grad_norm': 13.259035110473633, 'learning_rate': 1.5e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5392, 'grad_norm': 7.364662170410156, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.1}\n",
      "{'loss': 0.4489, 'grad_norm': 29.351869583129883, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 0.6215, 'grad_norm': 12.04206371307373, 'learning_rate': 1.8e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2912, 'grad_norm': 9.139932632446289, 'learning_rate': 1.9e-05, 'epoch': 0.12}\n",
      "{'loss': 0.6888, 'grad_norm': 3.9426677227020264, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.544, 'grad_norm': 14.00932502746582, 'learning_rate': 2.1e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4966, 'grad_norm': 6.0895514488220215, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.7538, 'grad_norm': 26.578554153442383, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4935, 'grad_norm': 5.0815935134887695, 'learning_rate': 2.4e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5417, 'grad_norm': 15.093242645263672, 'learning_rate': 2.5e-05, 'epoch': 0.15}\n",
      "{'loss': 0.6269, 'grad_norm': 2.903424024581909, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6031, 'grad_norm': 2.045637369155884, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5581, 'grad_norm': 204.8267059326172, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7453, 'grad_norm': 33.16191482543945, 'learning_rate': 2.9e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4858, 'grad_norm': 8.152320861816406, 'learning_rate': 3e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7546, 'grad_norm': 1.6986420154571533, 'learning_rate': 3.1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6893, 'grad_norm': 35.01735305786133, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "robert_tokenizer_full = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "robert_model_full = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "robert_encodings_full = robert_tokenizer_full(list(df_train_full['text']), truncation=True, padding=True, max_length=256)\n",
    "robert_labels_full = torch.tensor(list(df_train_full['target']))\n",
    "\n",
    "robert_input_ids_train_full, robert_input_ids_valid_full, \\\n",
    "robert_attention_mask_train_full, robert_attention_mask_valid_full, robert_y_train_full, robert_y_valid_full = train_test_split(\n",
    "    robert_encodings_full['input_ids'], \n",
    "    robert_encodings_full['attention_mask'], \n",
    "    robert_labels_full, \n",
    "    test_size=0.15, \n",
    "    stratify=robert_labels_full, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "robert_train_encodings_full = {\n",
    "    'input_ids': torch.tensor(robert_input_ids_train_full),\n",
    "    'attention_mask': torch.tensor(robert_attention_mask_train_full)\n",
    "}\n",
    "\n",
    "robert_valid_encodings_full = {\n",
    "    'input_ids': torch.tensor(robert_input_ids_valid_full),\n",
    "    'attention_mask': torch.tensor(robert_attention_mask_valid_full)\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True \n",
    ")\n",
    "\n",
    "robert_train_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": robert_train_encodings_full['input_ids'],\n",
    "    \"attention_mask\": robert_train_encodings_full['attention_mask'],\n",
    "    \"labels\": robert_y_train_full\n",
    "})\n",
    "\n",
    "robert_valid_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": robert_valid_encodings_full['input_ids'],\n",
    "    \"attention_mask\": robert_valid_encodings_full['attention_mask'],\n",
    "    \"labels\": robert_y_valid_full\n",
    "})\n",
    "\n",
    "robert_trainer_full = Trainer(\n",
    "    model=robert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=robert_train_dataset_full,\n",
    "    eval_dataset=robert_valid_dataset_full\n",
    ")\n",
    "\n",
    "robert_trainer_full.train()\n",
    "\n",
    "robert_predictions_full = robert_trainer_full.predict(robert_valid_dataset_full)\n",
    "robert_logits_full = robert_predictions_full.predictions\n",
    "robert_y_pred_full = np.argmax(robert_logits_full, axis=1)\n",
    "\n",
    "robert_trainer_full_assessement = evaluate_classifier(robert_y_valid_full.numpy(), robert_y_pred_full)\n",
    "\n",
    "robert_test_encodings_full = robert_tokenizer_full(list(df_test_full['text']), truncation=True, padding=True, max_length=256)\n",
    "robert_test_encodings_full = {key: torch.tensor(val) for key, val in robert_test_encodings_full.items()}\n",
    "robert_test_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": robert_test_encodings_full['input_ids'],\n",
    "    \"attention_mask\": robert_test_encodings_full['attention_mask']\n",
    "})\n",
    "\n",
    "robert_test_predictions_full = robert_trainer_full.predict(robert_test_dataset_full)\n",
    "robert_test_logits_full = robert_test_predictions_full.predictions\n",
    "robert_test_y_pred_full = np.argmax(robert_test_logits_full, axis=1)\n",
    "\n",
    "robert_test_submission_full = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': robert_test_y_pred_full.flatten()\n",
    "})\n",
    "\n",
    "robert_trainer_full.model.push_to_hub(\"yanncauchepin/kaggle_disastertweets_robert_model\")\n",
    "robert_tokenizer_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_robert_tokenizer\")\n",
    "\n",
    "hf_robert_test_submission_full = Dataset.from_pandas(robert_test_submission_full)\n",
    "hf_robert_test_submission_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_robert_submission_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/transformers/training_args.py:1574: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac260be8d5ec4ce8a24ac493940d2454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7353, 'grad_norm': 29.566905975341797, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 0.7345, 'grad_norm': 30.92800521850586, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 0.776, 'grad_norm': 34.36389923095703, 'learning_rate': 3e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6873, 'grad_norm': 20.674179077148438, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 0.7031, 'grad_norm': 21.924238204956055, 'learning_rate': 5e-06, 'epoch': 0.03}\n",
      "{'loss': 0.643, 'grad_norm': 21.9400691986084, 'learning_rate': 6e-06, 'epoch': 0.04}\n",
      "{'loss': 0.6528, 'grad_norm': 122.38347625732422, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 0.63, 'grad_norm': 65.72347259521484, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 0.5723, 'grad_norm': 21.250648498535156, 'learning_rate': 9e-06, 'epoch': 0.06}\n",
      "{'loss': 0.7273, 'grad_norm': 66.90182495117188, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5421, 'grad_norm': 37.23288345336914, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6159, 'grad_norm': 51.154029846191406, 'learning_rate': 1.2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5109, 'grad_norm': 6.846164703369141, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.4978, 'grad_norm': 15.703771591186523, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5629, 'grad_norm': 59.38235092163086, 'learning_rate': 1.5e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5046, 'grad_norm': 21.07247543334961, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5399, 'grad_norm': 142.09808349609375, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5645, 'grad_norm': 37.24563980102539, 'learning_rate': 1.8e-05, 'epoch': 0.11}\n",
      "{'loss': 0.4117, 'grad_norm': 13.045910835266113, 'learning_rate': 1.9e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4224, 'grad_norm': 5.6498332023620605, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5855, 'grad_norm': 33.104652404785156, 'learning_rate': 2.1e-05, 'epoch': 0.13}\n",
      "{'loss': 0.6956, 'grad_norm': 224.84304809570312, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.8003, 'grad_norm': 99.22398376464844, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 0.6006, 'grad_norm': 22.904434204101562, 'learning_rate': 2.4e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5341, 'grad_norm': 94.76133728027344, 'learning_rate': 2.5e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5093, 'grad_norm': 26.240009307861328, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6182, 'grad_norm': 43.43098068237305, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4941, 'grad_norm': 93.78580474853516, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5795, 'grad_norm': 229.84524536132812, 'learning_rate': 2.9e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4746, 'grad_norm': 74.99047088623047, 'learning_rate': 3e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7369, 'grad_norm': 28.427967071533203, 'learning_rate': 3.1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6226, 'grad_norm': 24.43445587158203, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4432, 'grad_norm': 10.809443473815918, 'learning_rate': 3.3e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5564, 'grad_norm': 32.932193756103516, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.21}\n",
      "{'loss': 0.846, 'grad_norm': 72.29618072509766, 'learning_rate': 3.5e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7475, 'grad_norm': 100.27558135986328, 'learning_rate': 3.6e-05, 'epoch': 0.22}\n",
      "{'loss': 0.8459, 'grad_norm': 14.231947898864746, 'learning_rate': 3.7e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6077, 'grad_norm': 11.098379135131836, 'learning_rate': 3.8e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6199, 'grad_norm': 38.06221008300781, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.24}\n",
      "{'loss': 0.8257, 'grad_norm': 85.57180786132812, 'learning_rate': 4e-05, 'epoch': 0.25}\n",
      "{'loss': 0.6577, 'grad_norm': 26.226612091064453, 'learning_rate': 4.1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9351, 'grad_norm': 8.865840911865234, 'learning_rate': 4.2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.7164, 'grad_norm': 102.7816162109375, 'learning_rate': 4.3e-05, 'epoch': 0.27}\n",
      "{'loss': 0.7988, 'grad_norm': 70.26097869873047, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.27}\n",
      "{'loss': 0.7294, 'grad_norm': 14.215441703796387, 'learning_rate': 4.5e-05, 'epoch': 0.28}\n",
      "{'loss': 0.8983, 'grad_norm': 27.688011169433594, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.764, 'grad_norm': 14.256537437438965, 'learning_rate': 4.7e-05, 'epoch': 0.29}\n",
      "{'loss': 0.6706, 'grad_norm': 9.972084045410156, 'learning_rate': 4.8e-05, 'epoch': 0.3}\n",
      "{'loss': 0.7694, 'grad_norm': 5.1652655601501465, 'learning_rate': 4.9e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6778, 'grad_norm': 3.193838357925415, 'learning_rate': 5e-05, 'epoch': 0.31}\n",
      "{'loss': 0.7049, 'grad_norm': 18.668054580688477, 'learning_rate': 4.988516306844282e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7052, 'grad_norm': 13.057449340820312, 'learning_rate': 4.9770326136885625e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7028, 'grad_norm': 3.8641183376312256, 'learning_rate': 4.965548920532844e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6985, 'grad_norm': 2.8733456134796143, 'learning_rate': 4.954065227377125e-05, 'epoch': 0.33}\n",
      "{'loss': 0.68, 'grad_norm': 5.632270812988281, 'learning_rate': 4.942581534221406e-05, 'epoch': 0.34}\n",
      "{'loss': 0.6936, 'grad_norm': 4.188185691833496, 'learning_rate': 4.931097841065687e-05, 'epoch': 0.35}\n",
      "{'loss': 0.6206, 'grad_norm': 2.6025867462158203, 'learning_rate': 4.9196141479099684e-05, 'epoch': 0.35}\n",
      "{'loss': 0.7729, 'grad_norm': 4.270860195159912, 'learning_rate': 4.908130454754249e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7312, 'grad_norm': 3.623110771179199, 'learning_rate': 4.89664676159853e-05, 'epoch': 0.36}\n",
      "{'loss': 0.681, 'grad_norm': 4.773443698883057, 'learning_rate': 4.8851630684428114e-05, 'epoch': 0.37}\n",
      "{'loss': 0.7163, 'grad_norm': 6.929572105407715, 'learning_rate': 4.873679375287092e-05, 'epoch': 0.38}\n",
      "{'loss': 0.5912, 'grad_norm': 5.47383975982666, 'learning_rate': 4.8621956821313736e-05, 'epoch': 0.38}\n",
      "{'loss': 0.7372, 'grad_norm': 7.775313377380371, 'learning_rate': 4.8507119889756544e-05, 'epoch': 0.39}\n",
      "{'loss': 0.7188, 'grad_norm': 3.3681790828704834, 'learning_rate': 4.839228295819936e-05, 'epoch': 0.4}\n",
      "{'loss': 0.6227, 'grad_norm': 4.293003082275391, 'learning_rate': 4.827744602664217e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7238, 'grad_norm': 6.350200653076172, 'learning_rate': 4.816260909508498e-05, 'epoch': 0.41}\n",
      "{'loss': 0.7594, 'grad_norm': 26.048288345336914, 'learning_rate': 4.8047772163527796e-05, 'epoch': 0.41}\n",
      "{'loss': 0.9049, 'grad_norm': 13.598098754882812, 'learning_rate': 4.7932935231970603e-05, 'epoch': 0.42}\n",
      "{'loss': 0.617, 'grad_norm': 9.389867782592773, 'learning_rate': 4.781809830041342e-05, 'epoch': 0.43}\n",
      "{'loss': 0.7532, 'grad_norm': 8.066390037536621, 'learning_rate': 4.7703261368856226e-05, 'epoch': 0.43}\n",
      "{'loss': 0.7297, 'grad_norm': 11.331262588500977, 'learning_rate': 4.758842443729904e-05, 'epoch': 0.44}\n",
      "{'loss': 0.7361, 'grad_norm': 10.565041542053223, 'learning_rate': 4.747358750574185e-05, 'epoch': 0.44}\n",
      "{'loss': 0.7241, 'grad_norm': 5.54034948348999, 'learning_rate': 4.735875057418466e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6449, 'grad_norm': 11.367257118225098, 'learning_rate': 4.724391364262747e-05, 'epoch': 0.46}\n",
      "{'loss': 0.9169, 'grad_norm': 7.054190635681152, 'learning_rate': 4.712907671107028e-05, 'epoch': 0.46}\n",
      "{'loss': 0.7979, 'grad_norm': 3.9206721782684326, 'learning_rate': 4.701423977951309e-05, 'epoch': 0.47}\n",
      "{'loss': 0.7128, 'grad_norm': 10.464218139648438, 'learning_rate': 4.68994028479559e-05, 'epoch': 0.48}\n",
      "{'loss': 0.7038, 'grad_norm': 8.389322280883789, 'learning_rate': 4.6784565916398715e-05, 'epoch': 0.48}\n",
      "{'loss': 0.7429, 'grad_norm': 9.634214401245117, 'learning_rate': 4.666972898484153e-05, 'epoch': 0.49}\n",
      "{'loss': 0.7327, 'grad_norm': 5.768658638000488, 'learning_rate': 4.655489205328434e-05, 'epoch': 0.49}\n",
      "{'loss': 0.6466, 'grad_norm': 2.9134795665740967, 'learning_rate': 4.644005512172715e-05, 'epoch': 0.5}\n",
      "{'loss': 0.7755, 'grad_norm': 6.680934429168701, 'learning_rate': 4.632521819016996e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6814, 'grad_norm': 9.324677467346191, 'learning_rate': 4.6210381258612774e-05, 'epoch': 0.51}\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "\n",
    "albert_tokenizer_full = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "albert_model_full = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2)\n",
    "\n",
    "albert_encodings_full = albert_tokenizer_full(list(df_train_full['text']), truncation=True, padding=True, max_length=256)\n",
    "albert_labels_full = torch.tensor(list(df_train_full['target']))\n",
    "\n",
    "albert_input_ids_train_full, albert_input_ids_valid_full, albert_token_type_ids_train_full, albert_token_type_ids_valid_full, \\\n",
    "albert_attention_mask_train_full, albert_attention_mask_valid_full, albert_y_train_full, albert_y_valid_full = train_test_split(\n",
    "    albert_encodings_full['input_ids'], \n",
    "    albert_encodings_full['token_type_ids'], \n",
    "    albert_encodings_full['attention_mask'], \n",
    "    albert_labels_full, \n",
    "    test_size=0.15, \n",
    "    stratify=albert_labels_full, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "albert_train_encodings_full = {\n",
    "    'input_ids': torch.tensor(albert_input_ids_train_full),\n",
    "    'token_type_ids': torch.tensor(albert_token_type_ids_train_full),\n",
    "    'attention_mask': torch.tensor(albert_attention_mask_train_full)\n",
    "}\n",
    "\n",
    "albert_valid_encodings_full = {\n",
    "    'input_ids': torch.tensor(albert_input_ids_valid_full),\n",
    "    'token_type_ids': torch.tensor(albert_token_type_ids_valid_full),\n",
    "    'attention_mask': torch.tensor(albert_attention_mask_valid_full)\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True \n",
    ")\n",
    "\n",
    "albert_train_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": albert_train_encodings_full['input_ids'],\n",
    "    \"attention_mask\": albert_train_encodings_full['attention_mask'],\n",
    "    \"labels\": albert_y_train_full\n",
    "})\n",
    "\n",
    "albert_valid_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": albert_valid_encodings_full['input_ids'],\n",
    "    \"attention_mask\": albert_valid_encodings_full['attention_mask'],\n",
    "    \"labels\": albert_y_valid_full\n",
    "})\n",
    "\n",
    "albert_trainer_full = Trainer(\n",
    "    model=albert_model_full,\n",
    "    args=training_args,\n",
    "    train_dataset=albert_train_dataset_full,\n",
    "    eval_dataset=albert_valid_dataset_full\n",
    ")\n",
    "\n",
    "albert_trainer_full.train()\n",
    "\n",
    "albert_predictions_full = albert_trainer_full.predict(albert_valid_dataset_full)\n",
    "albert_logits_full = albert_predictions_full.predictions\n",
    "albert_y_pred_full = np.argmax(albert_logits_full, axis=1)\n",
    "\n",
    "albert_trainer_full_assessement = evaluate_classifier(albert_y_valid_full.numpy(), albert_y_pred_full)\n",
    "\n",
    "albert_test_encodings_full = albert_tokenizer_full(list(df_test_full['text']), truncation=True, padding=True, max_length=256)\n",
    "albert_test_encodings_full = {key: torch.tensor(val) for key, val in albert_test_encodings_full.items()}\n",
    "albert_test_dataset_full = Dataset.from_dict({\n",
    "    \"input_ids\": albert_test_encodings_full['input_ids'],\n",
    "    \"attention_mask\": albert_test_encodings_full['attention_mask']\n",
    "})\n",
    "\n",
    "albert_test_predictions_full = albert_trainer_full.predict(albert_test_dataset_full)\n",
    "albert_test_logits_full = albert_test_predictions_full.predictions\n",
    "albert_test_y_pred_full = np.argmax(albert_test_logits_full, axis=1)\n",
    "\n",
    "albert_test_submission_full = pd.DataFrame({\n",
    "    'id': df_test_full.index,\n",
    "    'target': albert_test_y_pred_full.flatten()\n",
    "})\n",
    "\n",
    "albert_trainer_full.model.push_to_hub(\"yanncauchepin/kaggle_disastertweets_albert_model\")\n",
    "albert_tokenizer_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_albert_tokenizer\")\n",
    "\n",
    "hf_albert_test_submission_full = Dataset.from_pandas(albert_test_submission_full)\n",
    "hf_albert_test_submission_full.push_to_hub(\"yanncauchepin/kaggle_disastertweets_albert_submission_df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
